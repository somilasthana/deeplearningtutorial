{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q and A Bert.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "j2l9iFgnW5ZN",
        "colab_type": "code",
        "outputId": "62066a49-0d70-42f8-ab3e-6f1f2d01ae29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hlyv0jpgXMTf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the inputs\n",
        "#original_tf_inplem_dir = \"./tensorflow_code/\"\n",
        "model_dir = \"uncased_L-12_H-768_A-12/\"\n",
        "\n",
        "vocab_file = model_dir + \"vocab.txt\"\n",
        "bert_config_file = model_dir + \"bert_config.json\"\n",
        "init_checkpoint = model_dir + \"bert_model.ckpt\"\n",
        "\n",
        "input_file = \"/content/drive/My Drive/Colab Notebooks/data/train-v2.0.json\"\n",
        "max_seq_length = 384\n",
        "outside_pos = max_seq_length + 10\n",
        "doc_stride = 128\n",
        "max_query_length = 64\n",
        "max_answer_length = 30\n",
        "output_dir = \"output_model\"\n",
        "learning_rate = 3e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gCJ4THuZUosm",
        "colab_type": "code",
        "outputId": "a8cfafbb-1998-4612-a19c-2c096819a697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-13 02:50:39--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.206.128, 2a00:1450:400c:c04::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.206.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip.2’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   157MB/s    in 2.5s    \n",
            "\n",
            "2019-03-13 02:50:41 (157 MB/s) - ‘uncased_L-12_H-768_A-12.zip.2’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "replace uncased_L-12_H-768_A-12/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a0x79oJxO-bs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Try using normal run squad script\n"
      ]
    },
    {
      "metadata": {
        "id": "lkwIGg6l_Gg6",
        "colab_type": "code",
        "outputId": "fd745985-7be0-4b01-c5dc-4551d320e139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iD1gbiEd_BSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bert import modeling\n",
        "from bert import tokenization\n",
        "from bert import run_squad\n",
        "from bert import optimization\n",
        "import random\n",
        "import os\n",
        "import collections\n",
        "import six\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kCrN9yll_Z3O",
        "colab_type": "code",
        "outputId": "ad09730f-0815-46b2-98d9-feb025cebeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "!cp /content/drive/My\\ Drive/Colab\\ Notebooks/data/output_model.zip /content\n",
        "!unzip output_model.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  output_model.zip\n",
            "   creating: output_model/\n",
            "  inflating: output_model/model.ckpt-500.meta  \n",
            "  inflating: output_model/model.ckpt-0.index  \n",
            "  inflating: output_model/graph.pbtxt  \n",
            "  inflating: output_model/model.ckpt-760.meta  \n",
            "  inflating: output_model/checkpoint  \n",
            "  inflating: output_model/model.ckpt-500.data-00000-of-00001  \n",
            "  inflating: output_model/events.out.tfevents.1551996110.a75c072e79ed  \n",
            "  inflating: output_model/model.ckpt-0.meta  \n",
            "  inflating: output_model/model.ckpt-760.index  \n",
            "  inflating: output_model/model.ckpt-500.index  \n",
            "  inflating: output_model/model.ckpt-760.data-00000-of-00001  \n",
            "  inflating: output_model/model.ckpt-0.data-00000-of-00001  \n",
            "  inflating: output_model/train.tf_record  \n",
            "  inflating: output_model/events.out.tfevents.1552188869.d956a7f92d9e  \n",
            "  inflating: output_model/model.ckpt-750.index  \n",
            "  inflating: output_model/model.ckpt-750.meta  \n",
            "  inflating: output_model/model.ckpt-750.data-00000-of-00001  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UbLbMLTtPBEX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eiXVtvbPYxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnashtyApLLA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wsFIa2Z7PmyG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    save_summary_steps = SAVE_SUMMARY_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcAIQh7bToYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_squad_examples(input_file, is_training,version_2_with_negative):\n",
        "  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
        "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
        "    input_data = json.load(reader)[\"data\"]\n",
        "\n",
        "  def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  examples = []\n",
        "  for entry in input_data:\n",
        "    for paragraph in entry[\"paragraphs\"]:\n",
        "      paragraph_text = paragraph[\"context\"]\n",
        "      doc_tokens = []\n",
        "      char_to_word_offset = []\n",
        "      prev_is_whitespace = True\n",
        "      for c in paragraph_text:\n",
        "        if is_whitespace(c):\n",
        "          prev_is_whitespace = True\n",
        "        else:\n",
        "          if prev_is_whitespace:\n",
        "            doc_tokens.append(c)\n",
        "          else:\n",
        "            doc_tokens[-1] += c\n",
        "          prev_is_whitespace = False\n",
        "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "\n",
        "      for qa in paragraph[\"qas\"]:\n",
        "        qas_id = qa[\"id\"]\n",
        "        question_text = qa[\"question\"]\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "        orig_answer_text = None\n",
        "        is_impossible = False\n",
        "        if is_training:\n",
        "\n",
        "          if version_2_with_negative:\n",
        "            is_impossible = qa[\"is_impossible\"]\n",
        "          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
        "            raise ValueError(\n",
        "                \"For training, each question should have exactly 1 answer.\")\n",
        "          if not is_impossible:\n",
        "            answer = qa[\"answers\"][0]\n",
        "            orig_answer_text = answer[\"text\"]\n",
        "            answer_offset = answer[\"answer_start\"]\n",
        "            answer_length = len(orig_answer_text)\n",
        "            start_position = char_to_word_offset[answer_offset]\n",
        "            end_position = char_to_word_offset[answer_offset + answer_length -\n",
        "                                               1]\n",
        "            # Only add answers where the text can be exactly recovered from the\n",
        "            # document. If this CAN'T happen it's likely due to weird Unicode\n",
        "            # stuff so we will just skip the example.\n",
        "            #\n",
        "            # Note that this means for training mode, every example is NOT\n",
        "            # guaranteed to be preserved.\n",
        "            actual_text = \" \".join(\n",
        "                doc_tokens[start_position:(end_position + 1)])\n",
        "            cleaned_answer_text = \" \".join(\n",
        "                tokenization.whitespace_tokenize(orig_answer_text))\n",
        "            if actual_text.find(cleaned_answer_text) == -1:\n",
        "              tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                 actual_text, cleaned_answer_text)\n",
        "              continue\n",
        "          else:\n",
        "            start_position = -1\n",
        "            end_position = -1\n",
        "            orig_answer_text = \"\"\n",
        "\n",
        "        example = run_squad.SquadExample(\n",
        "            qas_id=qas_id,\n",
        "            question_text=question_text,\n",
        "            doc_tokens=doc_tokens,\n",
        "            orig_answer_text=orig_answer_text,\n",
        "            start_position=start_position,\n",
        "            end_position=end_position,\n",
        "            is_impossible=is_impossible)\n",
        "        examples.append(example)\n",
        "\n",
        "  return examples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NuCQxR8rpWSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert examples to features for training and testing\n",
        "#train_file = \"/content/drive/My Drive/Colab Notebooks/data/train-v2.0.json\"\n",
        "train_file = \"/content/drive/My Drive/Colab Notebooks/data/monitoring (1).json\"\n",
        "#test_file = \"/content/drive/My Drive/Colab Notebooks/data/dev-v2.0.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ww0Ho73FPq_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_examples = read_squad_examples(input_file=train_file, is_training=True,version_2_with_negative=True)\n",
        "\n",
        "# Get random sample of it because of size\n",
        "#train_examples = [train_examples[random.randrange(len(train_examples))]\n",
        "              #for item in range(8000)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgLY-tCaQEIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Shuffle the training examples\n",
        "rng = random.Random(12345)\n",
        "rng.shuffle(train_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QtM9yuMQVXeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    unique_ids = features[\"unique_ids\"]\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (start_logits, end_logits) = create_model(\n",
        "        bert_config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      seq_length = modeling.get_shape_list(input_ids)[1]\n",
        "\n",
        "      def compute_loss(logits, positions):\n",
        "        one_hot_positions = tf.one_hot(\n",
        "            positions, depth=seq_length, dtype=tf.float32)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        loss = -tf.reduce_mean(\n",
        "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "        return loss\n",
        "\n",
        "      start_positions = features[\"start_positions\"]\n",
        "      end_positions = features[\"end_positions\"]\n",
        "\n",
        "      start_loss = compute_loss(start_logits, start_positions)\n",
        "      end_loss = compute_loss(end_logits, end_positions)\n",
        "\n",
        "      total_loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                loss=total_loss,\n",
        "                train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      predictions = {\n",
        "          \"unique_ids\": unique_ids,\n",
        "          \"start_logits\": start_logits,\n",
        "          \"end_logits\": end_logits,\n",
        "      }\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode, predictions=predictions)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UIDL--MVpyfd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!rm -r output_model\n",
        "!mkdir output_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y116fElHUVUW",
        "colab_type": "code",
        "outputId": "bc411e99-e49e-4a2d-cc9b-ee842362b2bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 22137
        }
      },
      "cell_type": "code",
      "source": [
        "train_writer = run_squad.FeatureWriter(\n",
        "        filename=os.path.join(output_dir, \"train.tf_record\"),\n",
        "        is_training=True)\n",
        "run_squad.convert_examples_to_features(\n",
        "    examples=train_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=True,\n",
        "    output_fn=train_writer.process_feature)\n",
        "train_writer.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000000\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:12 26:13 27:14 28:15 29:16 30:17 31:18 32:19 33:20 34:21 35:22 36:23 37:24 38:25 39:25 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:33 48:34 49:35 50:36 51:37 52:38 53:38 54:39 55:40 56:41 57:41 58:41 59:42 60:43 61:44 62:45 63:46 64:47 65:48 66:49 67:50 68:51 69:51 70:52 71:53 72:54 73:55 74:56 75:57 76:58 77:59 78:60 79:61 80:62 81:63 82:64 83:64 84:65 85:66 86:67 87:67 88:67 89:68 90:69 91:70 92:71 93:72 94:73 95:74 96:75 97:76 98:77 99:77 100:78 101:79 102:80 103:80 104:80 105:81 106:82 107:83 108:84 109:85 110:86 111:87 112:88 113:89 114:90 115:90 116:90 117:91 118:92 119:93 120:94 121:95 122:96 123:97 124:98 125:99 126:100 127:101 128:102 129:103 130:103 131:104 132:105 133:106 134:107 135:108 136:109 137:110 138:111 139:112 140:113 141:114 142:115 143:116 144:116 145:117 146:118 147:119 148:120 149:121 150:122 151:123 152:124 153:125 154:126 155:127 156:128 157:129 158:129 159:130 160:131 161:132 162:132 163:132 164:133 165:134 166:135 167:136 168:137 169:138 170:139 171:140 172:141 173:142 174:142 175:143 176:144 177:145 178:146 179:147 180:148 181:149 182:150 183:151 184:152 185:153 186:154 187:155 188:155 189:156 190:157 191:158 192:158 193:158 194:159 195:160 196:161 197:162 198:163 199:164 200:165 201:166 202:167 203:168 204:168 205:169 206:170 207:171 208:171 209:171 210:172 211:173 212:174 213:175 214:176 215:177 216:178 217:179 218:180 219:181 220:181 221:181 222:182 223:183 224:184 225:185 226:186 227:187 228:188 229:189 230:190 231:191 232:192 233:193 234:194 235:195 236:195 237:196 238:197 239:198 240:199 241:200 242:201 243:202 244:203 245:204 246:205 247:206 248:207 249:208 250:209 251:209 252:210 253:211 254:212 255:213 256:214 257:215 258:216 259:217 260:218 261:219 262:220 263:221 264:222 265:223 266:223 267:224 268:225 269:226 270:227 271:228 272:229 273:230 274:231 275:232 276:233 277:234 278:235 279:236 280:237 281:237 282:237 283:237 284:238 285:239 286:240 287:241 288:242 289:243 290:244 291:245 292:246 293:247 294:248 295:249 296:250 297:251 298:251 299:252 300:253 301:254 302:255 303:256 304:257 305:258 306:259 307:260 308:261 309:262 310:263 311:264 312:265 313:265 314:265 315:265 316:266 317:267 318:268 319:269 320:270 321:271 322:272 323:273 324:274 325:275 326:276 327:277 328:277 329:278 330:279 331:280 332:281 333:282 334:283 335:283 336:284 337:285 338:286 339:287 340:288 341:289 342:290 343:291 344:291 345:292 346:293 347:294 348:295 349:296 350:297 351:297 352:298 353:299 354:300 355:301 356:302 357:303 358:304 359:305 360:305 361:306 362:307 363:308 364:309 365:310 366:311 367:311 368:312 369:313 370:314 371:315 372:316 373:317 374:318 375:319 376:319 377:320 378:321 379:322 380:323 381:324 382:325\n",
            "INFO:tensorflow:token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000001\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:113 13:114 14:115 15:116 16:116 17:117 18:118 19:119 20:120 21:121 22:122 23:123 24:124 25:125 26:126 27:127 28:128 29:129 30:129 31:130 32:131 33:132 34:132 35:132 36:133 37:134 38:135 39:136 40:137 41:138 42:139 43:140 44:141 45:142 46:142 47:143 48:144 49:145 50:146 51:147 52:148 53:149 54:150 55:151 56:152 57:153 58:154 59:155 60:155 61:156 62:157 63:158 64:158 65:158 66:159 67:160 68:161 69:162 70:163 71:164 72:165 73:166 74:167 75:168 76:168 77:169 78:170 79:171 80:171 81:171 82:172 83:173 84:174 85:175 86:176 87:177 88:178 89:179 90:180 91:181 92:181 93:181 94:182 95:183 96:184 97:185 98:186 99:187 100:188 101:189 102:190 103:191 104:192 105:193 106:194 107:195 108:195 109:196 110:197 111:198 112:199 113:200 114:201 115:202 116:203 117:204 118:205 119:206 120:207 121:208 122:209 123:209 124:210 125:211 126:212 127:213 128:214 129:215 130:216 131:217 132:218 133:219 134:220 135:221 136:222 137:223 138:223 139:224 140:225 141:226 142:227 143:228 144:229 145:230 146:231 147:232 148:233 149:234 150:235 151:236 152:237 153:237 154:237 155:237 156:238 157:239 158:240 159:241 160:242 161:243 162:244 163:245 164:246 165:247 166:248 167:249 168:250 169:251 170:251 171:252 172:253 173:254 174:255 175:256 176:257 177:258 178:259 179:260 180:261 181:262 182:263 183:264 184:265 185:265 186:265 187:265 188:266 189:267 190:268 191:269 192:270 193:271 194:272 195:273 196:274 197:275 198:276 199:277 200:277 201:278 202:279 203:280 204:281 205:282 206:283 207:283 208:284 209:285 210:286 211:287 212:288 213:289 214:290 215:291 216:291 217:292 218:293 219:294 220:295 221:296 222:297 223:297 224:298 225:299 226:300 227:301 228:302 229:303 230:304 231:305 232:305 233:306 234:307 235:308 236:309 237:310 238:311 239:311 240:312 241:313 242:314 243:315 244:316 245:317 246:318 247:319 248:319 249:320 250:321 251:322 252:323 253:324 254:325 255:325 256:326 257:327 258:328 259:329 260:330 261:331 262:332 263:333 264:333 265:333 266:333 267:334 268:335 269:336 270:337 271:338 272:339 273:339 274:340 275:341 276:342 277:343 278:344 279:345 280:346 281:347 282:347 283:348 284:349 285:350 286:351 287:352 288:353 289:353 290:354 291:355 292:356 293:357 294:358 295:359 296:360 297:361 298:361 299:361 300:361 301:362 302:363 303:364 304:365 305:366 306:367 307:367 308:368 309:369 310:370 311:371 312:372 313:373 314:373 315:374 316:375 317:376 318:377 319:378 320:379 321:380 322:381 323:382 324:383 325:384 326:385 327:386 328:386 329:387 330:388 331:389 332:390 333:391 334:392 335:393 336:394 337:395 338:396 339:397 340:398 341:399 342:399 343:400 344:401 345:402 346:403 347:404 348:405 349:406 350:407 351:408 352:409 353:410 354:411 355:412 356:412 357:413 358:414 359:415 360:415 361:415 362:416 363:417 364:418 365:419 366:420 367:421 368:422 369:423 370:424 371:425 372:425 373:426 374:427 375:428 376:429 377:430 378:431 379:432 380:433 381:434 382:435\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000002\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:225 13:226 14:227 15:228 16:229 17:230 18:231 19:232 20:233 21:234 22:235 23:236 24:237 25:237 26:237 27:237 28:238 29:239 30:240 31:241 32:242 33:243 34:244 35:245 36:246 37:247 38:248 39:249 40:250 41:251 42:251 43:252 44:253 45:254 46:255 47:256 48:257 49:258 50:259 51:260 52:261 53:262 54:263 55:264 56:265 57:265 58:265 59:265 60:266 61:267 62:268 63:269 64:270 65:271 66:272 67:273 68:274 69:275 70:276 71:277 72:277 73:278 74:279 75:280 76:281 77:282 78:283 79:283 80:284 81:285 82:286 83:287 84:288 85:289 86:290 87:291 88:291 89:292 90:293 91:294 92:295 93:296 94:297 95:297 96:298 97:299 98:300 99:301 100:302 101:303 102:304 103:305 104:305 105:306 106:307 107:308 108:309 109:310 110:311 111:311 112:312 113:313 114:314 115:315 116:316 117:317 118:318 119:319 120:319 121:320 122:321 123:322 124:323 125:324 126:325 127:325 128:326 129:327 130:328 131:329 132:330 133:331 134:332 135:333 136:333 137:333 138:333 139:334 140:335 141:336 142:337 143:338 144:339 145:339 146:340 147:341 148:342 149:343 150:344 151:345 152:346 153:347 154:347 155:348 156:349 157:350 158:351 159:352 160:353 161:353 162:354 163:355 164:356 165:357 166:358 167:359 168:360 169:361 170:361 171:361 172:361 173:362 174:363 175:364 176:365 177:366 178:367 179:367 180:368 181:369 182:370 183:371 184:372 185:373 186:373 187:374 188:375 189:376 190:377 191:378 192:379 193:380 194:381 195:382 196:383 197:384 198:385 199:386 200:386 201:387 202:388 203:389 204:390 205:391 206:392 207:393 208:394 209:395 210:396 211:397 212:398 213:399 214:399 215:400 216:401 217:402 218:403 219:404 220:405 221:406 222:407 223:408 224:409 225:410 226:411 227:412 228:412 229:413 230:414 231:415 232:415 233:415 234:416 235:417 236:418 237:419 238:420 239:421 240:422 241:423 242:424 243:425 244:425 245:426 246:427 247:428 248:429 249:430 250:431 251:432 252:433 253:434 254:435 255:436 256:437 257:438 258:438 259:439 260:440 261:441 262:441 263:441 264:442 265:443 266:444 267:445 268:446 269:447 270:448 271:449 272:450 273:451 274:451 275:452 276:453 277:454 278:454 279:454 280:455 281:456 282:457 283:458 284:459 285:460 286:461 287:462 288:463 289:464 290:464 291:464 292:465 293:466 294:467 295:468 296:469 297:470 298:471 299:472 300:473 301:474 302:475 303:476 304:477 305:477 306:478 307:479 308:480 309:481 310:482 311:483 312:484 313:485 314:486 315:487 316:488 317:489 318:490 319:490 320:491 321:492 322:493 323:494 324:495 325:496 326:497 327:498 328:499 329:500 330:501 331:502 332:503 333:503 334:504 335:505 336:506 337:506 338:506 339:507 340:508 341:509 342:510 343:511 344:512 345:513 346:514 347:515 348:516 349:516 350:517 351:518 352:519 353:520 354:521 355:522 356:523 357:524 358:525 359:526 360:527 361:528 362:529 363:529 364:530 365:531 366:532 367:532 368:532 369:533 370:534 371:535 372:536 373:537 374:538 375:539 376:540 377:541 378:542 379:542 380:543 381:544 382:545\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000003\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:335 13:336 14:337 15:338 16:339 17:339 18:340 19:341 20:342 21:343 22:344 23:345 24:346 25:347 26:347 27:348 28:349 29:350 30:351 31:352 32:353 33:353 34:354 35:355 36:356 37:357 38:358 39:359 40:360 41:361 42:361 43:361 44:361 45:362 46:363 47:364 48:365 49:366 50:367 51:367 52:368 53:369 54:370 55:371 56:372 57:373 58:373 59:374 60:375 61:376 62:377 63:378 64:379 65:380 66:381 67:382 68:383 69:384 70:385 71:386 72:386 73:387 74:388 75:389 76:390 77:391 78:392 79:393 80:394 81:395 82:396 83:397 84:398 85:399 86:399 87:400 88:401 89:402 90:403 91:404 92:405 93:406 94:407 95:408 96:409 97:410 98:411 99:412 100:412 101:413 102:414 103:415 104:415 105:415 106:416 107:417 108:418 109:419 110:420 111:421 112:422 113:423 114:424 115:425 116:425 117:426 118:427 119:428 120:429 121:430 122:431 123:432 124:433 125:434 126:435 127:436 128:437 129:438 130:438 131:439 132:440 133:441 134:441 135:441 136:442 137:443 138:444 139:445 140:446 141:447 142:448 143:449 144:450 145:451 146:451 147:452 148:453 149:454 150:454 151:454 152:455 153:456 154:457 155:458 156:459 157:460 158:461 159:462 160:463 161:464 162:464 163:464 164:465 165:466 166:467 167:468 168:469 169:470 170:471 171:472 172:473 173:474 174:475 175:476 176:477 177:477 178:478 179:479 180:480 181:481 182:482 183:483 184:484 185:485 186:486 187:487 188:488 189:489 190:490 191:490 192:491 193:492 194:493 195:494 196:495 197:496 198:497 199:498 200:499 201:500 202:501 203:502 204:503 205:503 206:504 207:505 208:506 209:506 210:506 211:507 212:508 213:509 214:510 215:511 216:512 217:513 218:514 219:515 220:516 221:516 222:517 223:518 224:519 225:520 226:521 227:522 228:523 229:524 230:525 231:526 232:527 233:528 234:529 235:529 236:530 237:531 238:532 239:532 240:532 241:533 242:534 243:535 244:536 245:537 246:538 247:539 248:540 249:541 250:542 251:542 252:543 253:544 254:545 255:545 256:545 257:546 258:547 259:548 260:549 261:550 262:551 263:552 264:553 265:554 266:555 267:555 268:555 269:556 270:557 271:558 272:559 273:560 274:561 275:562 276:563 277:564 278:565 279:566 280:567 281:568 282:569 283:569 284:570 285:571 286:572 287:573 288:574 289:575 290:576 291:577 292:578 293:579 294:580 295:581 296:582 297:583 298:583 299:584 300:585 301:586 302:587 303:588 304:589 305:590 306:591 307:592 308:593 309:594 310:595 311:596 312:597 313:597 314:598 315:599 316:600 317:601 318:602 319:603 320:604 321:605 322:606 323:607 324:608 325:609 326:610 327:611 328:611 329:611 330:611 331:612 332:613 333:614 334:615 335:616 336:617 337:618 338:619 339:620 340:621 341:622 342:623 343:624 344:625 345:625 346:626 347:627 348:628 349:629 350:630 351:631 352:632 353:633 354:634 355:635 356:636 357:637 358:638 359:639 360:639 361:639 362:639 363:640 364:641 365:642 366:643 367:644 368:645 369:646 370:647 371:648 372:649 373:650 374:651 375:651 376:652 377:653 378:654 379:655 380:656 381:657 382:657\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000004\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:446 13:447 14:448 15:449 16:450 17:451 18:451 19:452 20:453 21:454 22:454 23:454 24:455 25:456 26:457 27:458 28:459 29:460 30:461 31:462 32:463 33:464 34:464 35:464 36:465 37:466 38:467 39:468 40:469 41:470 42:471 43:472 44:473 45:474 46:475 47:476 48:477 49:477 50:478 51:479 52:480 53:481 54:482 55:483 56:484 57:485 58:486 59:487 60:488 61:489 62:490 63:490 64:491 65:492 66:493 67:494 68:495 69:496 70:497 71:498 72:499 73:500 74:501 75:502 76:503 77:503 78:504 79:505 80:506 81:506 82:506 83:507 84:508 85:509 86:510 87:511 88:512 89:513 90:514 91:515 92:516 93:516 94:517 95:518 96:519 97:520 98:521 99:522 100:523 101:524 102:525 103:526 104:527 105:528 106:529 107:529 108:530 109:531 110:532 111:532 112:532 113:533 114:534 115:535 116:536 117:537 118:538 119:539 120:540 121:541 122:542 123:542 124:543 125:544 126:545 127:545 128:545 129:546 130:547 131:548 132:549 133:550 134:551 135:552 136:553 137:554 138:555 139:555 140:555 141:556 142:557 143:558 144:559 145:560 146:561 147:562 148:563 149:564 150:565 151:566 152:567 153:568 154:569 155:569 156:570 157:571 158:572 159:573 160:574 161:575 162:576 163:577 164:578 165:579 166:580 167:581 168:582 169:583 170:583 171:584 172:585 173:586 174:587 175:588 176:589 177:590 178:591 179:592 180:593 181:594 182:595 183:596 184:597 185:597 186:598 187:599 188:600 189:601 190:602 191:603 192:604 193:605 194:606 195:607 196:608 197:609 198:610 199:611 200:611 201:611 202:611 203:612 204:613 205:614 206:615 207:616 208:617 209:618 210:619 211:620 212:621 213:622 214:623 215:624 216:625 217:625 218:626 219:627 220:628 221:629 222:630 223:631 224:632 225:633 226:634 227:635 228:636 229:637 230:638 231:639 232:639 233:639 234:639 235:640 236:641 237:642 238:643 239:644 240:645 241:646 242:647 243:648 244:649 245:650 246:651 247:651 248:652 249:653 250:654 251:655 252:656 253:657 254:657 255:658 256:659 257:660 258:661 259:662 260:663 261:664 262:665 263:665 264:666 265:667 266:668 267:669 268:670 269:671 270:671 271:672 272:673 273:674 274:675 275:676 276:677 277:678 278:679 279:679 280:680 281:681 282:682 283:683 284:684 285:685 286:685 287:686 288:687 289:688 290:689 291:690 292:691 293:692 294:693 295:693 296:694 297:695 298:696 299:697 300:698 301:699 302:699 303:700 304:701 305:702 306:703 307:704 308:705 309:706 310:707 311:707 312:707 313:707 314:708 315:709 316:710 317:711 318:712 319:713 320:713 321:714 322:715 323:716 324:717 325:718 326:719 327:720 328:721 329:721 330:722 331:723 332:724 333:725 334:726 335:727 336:727 337:728 338:729 339:730 340:731 341:732 342:733 343:734 344:735 345:735 346:735 347:735 348:736 349:737 350:738 351:739 352:740 353:741 354:741 355:742 356:743 357:744 358:745 359:746 360:747 361:747\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True 361:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 330\n",
            "INFO:tensorflow:end_position: 347\n",
            "INFO:tensorflow:answer: between jul 2015 and jun 2017 , the total sales trend was this for household _ type .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000005\n",
            "INFO:tensorflow:example_index: 1\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by loyalty [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:19 31:20 32:21 33:22 34:23 35:24 36:25 37:25 38:26 39:27 40:28 41:29 42:30 43:31 44:32 45:33 46:34 47:35 48:36 49:37 50:38 51:38 52:39 53:40 54:41 55:41 56:41 57:42 58:43 59:44 60:45 61:46 62:47 63:48 64:49 65:50 66:51 67:51 68:52 69:53 70:54 71:55 72:56 73:57 74:58 75:59 76:60 77:61 78:62 79:63 80:64 81:64 82:65 83:66 84:67 85:67 86:67 87:68 88:69 89:70 90:71 91:72 92:73 93:74 94:75 95:76 96:77 97:77 98:78 99:79 100:80 101:80 102:80 103:81 104:82 105:83 106:84 107:85 108:86 109:87 110:88 111:89 112:90 113:90 114:90 115:91 116:92 117:93 118:94 119:95 120:96 121:97 122:98 123:99 124:100 125:101 126:102 127:103 128:103 129:104 130:105 131:106 132:107 133:108 134:109 135:110 136:111 137:112 138:113 139:114 140:115 141:116 142:116 143:117 144:118 145:119 146:120 147:121 148:122 149:123 150:124 151:125 152:126 153:127 154:128 155:129 156:129 157:130 158:131 159:132 160:132 161:132 162:133 163:134 164:135 165:136 166:137 167:138 168:139 169:140 170:141 171:142 172:142 173:143 174:144 175:145 176:146 177:147 178:148 179:149 180:150 181:151 182:152 183:153 184:154 185:155 186:155 187:156 188:157 189:158 190:158 191:158 192:159 193:160 194:161 195:162 196:163 197:164 198:165 199:166 200:167 201:168 202:168 203:169 204:170 205:171 206:171 207:171 208:172 209:173 210:174 211:175 212:176 213:177 214:178 215:179 216:180 217:181 218:181 219:181 220:182 221:183 222:184 223:185 224:186 225:187 226:188 227:189 228:190 229:191 230:192 231:193 232:194 233:195 234:195 235:196 236:197 237:198 238:199 239:200 240:201 241:202 242:203 243:204 244:205 245:206 246:207 247:208 248:209 249:209 250:210 251:211 252:212 253:213 254:214 255:215 256:216 257:217 258:218 259:219 260:220 261:221 262:222 263:223 264:223 265:224 266:225 267:226 268:227 269:228 270:229 271:230 272:231 273:232 274:233 275:234 276:235 277:236 278:237 279:237 280:237 281:237 282:238 283:239 284:240 285:241 286:242 287:243 288:244 289:245 290:246 291:247 292:248 293:249 294:250 295:251 296:251 297:252 298:253 299:254 300:255 301:256 302:257 303:258 304:259 305:260 306:261 307:262 308:263 309:264 310:265 311:265 312:265 313:265 314:266 315:267 316:268 317:269 318:270 319:271 320:272 321:273 322:274 323:275 324:276 325:277 326:277 327:278 328:279 329:280 330:281 331:282 332:283 333:283 334:284 335:285 336:286 337:287 338:288 339:289 340:290 341:291 342:291 343:292 344:293 345:294 346:295 347:296 348:297 349:297 350:298 351:299 352:300 353:301 354:302 355:303 356:304 357:305 358:305 359:306 360:307 361:308 362:309 363:310 364:311 365:311 366:312 367:313 368:314 369:315 370:316 371:317 372:318 373:319 374:319 375:320 376:321 377:322 378:323 379:324 380:325 381:325 382:326\n",
            "INFO:tensorflow:token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 9721 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 282\n",
            "INFO:tensorflow:end_position: 296\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000006\n",
            "INFO:tensorflow:example_index: 1\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by loyalty [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:113 11:114 12:115 13:116 14:116 15:117 16:118 17:119 18:120 19:121 20:122 21:123 22:124 23:125 24:126 25:127 26:128 27:129 28:129 29:130 30:131 31:132 32:132 33:132 34:133 35:134 36:135 37:136 38:137 39:138 40:139 41:140 42:141 43:142 44:142 45:143 46:144 47:145 48:146 49:147 50:148 51:149 52:150 53:151 54:152 55:153 56:154 57:155 58:155 59:156 60:157 61:158 62:158 63:158 64:159 65:160 66:161 67:162 68:163 69:164 70:165 71:166 72:167 73:168 74:168 75:169 76:170 77:171 78:171 79:171 80:172 81:173 82:174 83:175 84:176 85:177 86:178 87:179 88:180 89:181 90:181 91:181 92:182 93:183 94:184 95:185 96:186 97:187 98:188 99:189 100:190 101:191 102:192 103:193 104:194 105:195 106:195 107:196 108:197 109:198 110:199 111:200 112:201 113:202 114:203 115:204 116:205 117:206 118:207 119:208 120:209 121:209 122:210 123:211 124:212 125:213 126:214 127:215 128:216 129:217 130:218 131:219 132:220 133:221 134:222 135:223 136:223 137:224 138:225 139:226 140:227 141:228 142:229 143:230 144:231 145:232 146:233 147:234 148:235 149:236 150:237 151:237 152:237 153:237 154:238 155:239 156:240 157:241 158:242 159:243 160:244 161:245 162:246 163:247 164:248 165:249 166:250 167:251 168:251 169:252 170:253 171:254 172:255 173:256 174:257 175:258 176:259 177:260 178:261 179:262 180:263 181:264 182:265 183:265 184:265 185:265 186:266 187:267 188:268 189:269 190:270 191:271 192:272 193:273 194:274 195:275 196:276 197:277 198:277 199:278 200:279 201:280 202:281 203:282 204:283 205:283 206:284 207:285 208:286 209:287 210:288 211:289 212:290 213:291 214:291 215:292 216:293 217:294 218:295 219:296 220:297 221:297 222:298 223:299 224:300 225:301 226:302 227:303 228:304 229:305 230:305 231:306 232:307 233:308 234:309 235:310 236:311 237:311 238:312 239:313 240:314 241:315 242:316 243:317 244:318 245:319 246:319 247:320 248:321 249:322 250:323 251:324 252:325 253:325 254:326 255:327 256:328 257:329 258:330 259:331 260:332 261:333 262:333 263:333 264:333 265:334 266:335 267:336 268:337 269:338 270:339 271:339 272:340 273:341 274:342 275:343 276:344 277:345 278:346 279:347 280:347 281:348 282:349 283:350 284:351 285:352 286:353 287:353 288:354 289:355 290:356 291:357 292:358 293:359 294:360 295:361 296:361 297:361 298:361 299:362 300:363 301:364 302:365 303:366 304:367 305:367 306:368 307:369 308:370 309:371 310:372 311:373 312:373 313:374 314:375 315:376 316:377 317:378 318:379 319:380 320:381 321:382 322:383 323:384 324:385 325:386 326:386 327:387 328:388 329:389 330:390 331:391 332:392 333:393 334:394 335:395 336:396 337:397 338:398 339:399 340:399 341:400 342:401 343:402 344:403 345:404 346:405 347:406 348:407 349:408 350:409 351:410 352:411 353:412 354:412 355:413 356:414 357:415 358:415 359:415 360:416 361:417 362:418 363:419 364:420 365:421 366:422 367:423 368:424 369:425 370:425 371:426 372:427 373:428 374:429 375:430 376:431 377:432 378:433 379:434 380:435 381:436 382:437\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 9721 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 154\n",
            "INFO:tensorflow:end_position: 168\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000007\n",
            "INFO:tensorflow:example_index: 1\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by loyalty [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:225 11:226 12:227 13:228 14:229 15:230 16:231 17:232 18:233 19:234 20:235 21:236 22:237 23:237 24:237 25:237 26:238 27:239 28:240 29:241 30:242 31:243 32:244 33:245 34:246 35:247 36:248 37:249 38:250 39:251 40:251 41:252 42:253 43:254 44:255 45:256 46:257 47:258 48:259 49:260 50:261 51:262 52:263 53:264 54:265 55:265 56:265 57:265 58:266 59:267 60:268 61:269 62:270 63:271 64:272 65:273 66:274 67:275 68:276 69:277 70:277 71:278 72:279 73:280 74:281 75:282 76:283 77:283 78:284 79:285 80:286 81:287 82:288 83:289 84:290 85:291 86:291 87:292 88:293 89:294 90:295 91:296 92:297 93:297 94:298 95:299 96:300 97:301 98:302 99:303 100:304 101:305 102:305 103:306 104:307 105:308 106:309 107:310 108:311 109:311 110:312 111:313 112:314 113:315 114:316 115:317 116:318 117:319 118:319 119:320 120:321 121:322 122:323 123:324 124:325 125:325 126:326 127:327 128:328 129:329 130:330 131:331 132:332 133:333 134:333 135:333 136:333 137:334 138:335 139:336 140:337 141:338 142:339 143:339 144:340 145:341 146:342 147:343 148:344 149:345 150:346 151:347 152:347 153:348 154:349 155:350 156:351 157:352 158:353 159:353 160:354 161:355 162:356 163:357 164:358 165:359 166:360 167:361 168:361 169:361 170:361 171:362 172:363 173:364 174:365 175:366 176:367 177:367 178:368 179:369 180:370 181:371 182:372 183:373 184:373 185:374 186:375 187:376 188:377 189:378 190:379 191:380 192:381 193:382 194:383 195:384 196:385 197:386 198:386 199:387 200:388 201:389 202:390 203:391 204:392 205:393 206:394 207:395 208:396 209:397 210:398 211:399 212:399 213:400 214:401 215:402 216:403 217:404 218:405 219:406 220:407 221:408 222:409 223:410 224:411 225:412 226:412 227:413 228:414 229:415 230:415 231:415 232:416 233:417 234:418 235:419 236:420 237:421 238:422 239:423 240:424 241:425 242:425 243:426 244:427 245:428 246:429 247:430 248:431 249:432 250:433 251:434 252:435 253:436 254:437 255:438 256:438 257:439 258:440 259:441 260:441 261:441 262:442 263:443 264:444 265:445 266:446 267:447 268:448 269:449 270:450 271:451 272:451 273:452 274:453 275:454 276:454 277:454 278:455 279:456 280:457 281:458 282:459 283:460 284:461 285:462 286:463 287:464 288:464 289:464 290:465 291:466 292:467 293:468 294:469 295:470 296:471 297:472 298:473 299:474 300:475 301:476 302:477 303:477 304:478 305:479 306:480 307:481 308:482 309:483 310:484 311:485 312:486 313:487 314:488 315:489 316:490 317:490 318:491 319:492 320:493 321:494 322:495 323:496 324:497 325:498 326:499 327:500 328:501 329:502 330:503 331:503 332:504 333:505 334:506 335:506 336:506 337:507 338:508 339:509 340:510 341:511 342:512 343:513 344:514 345:515 346:516 347:516 348:517 349:518 350:519 351:520 352:521 353:522 354:523 355:524 356:525 357:526 358:527 359:528 360:529 361:529 362:530 363:531 364:532 365:532 366:532 367:533 368:534 369:535 370:536 371:537 372:538 373:539 374:540 375:541 376:542 377:542 378:543 379:544 380:545 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 9721 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 26\n",
            "INFO:tensorflow:end_position: 40\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000008\n",
            "INFO:tensorflow:example_index: 1\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by loyalty [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:335 11:336 12:337 13:338 14:339 15:339 16:340 17:341 18:342 19:343 20:344 21:345 22:346 23:347 24:347 25:348 26:349 27:350 28:351 29:352 30:353 31:353 32:354 33:355 34:356 35:357 36:358 37:359 38:360 39:361 40:361 41:361 42:361 43:362 44:363 45:364 46:365 47:366 48:367 49:367 50:368 51:369 52:370 53:371 54:372 55:373 56:373 57:374 58:375 59:376 60:377 61:378 62:379 63:380 64:381 65:382 66:383 67:384 68:385 69:386 70:386 71:387 72:388 73:389 74:390 75:391 76:392 77:393 78:394 79:395 80:396 81:397 82:398 83:399 84:399 85:400 86:401 87:402 88:403 89:404 90:405 91:406 92:407 93:408 94:409 95:410 96:411 97:412 98:412 99:413 100:414 101:415 102:415 103:415 104:416 105:417 106:418 107:419 108:420 109:421 110:422 111:423 112:424 113:425 114:425 115:426 116:427 117:428 118:429 119:430 120:431 121:432 122:433 123:434 124:435 125:436 126:437 127:438 128:438 129:439 130:440 131:441 132:441 133:441 134:442 135:443 136:444 137:445 138:446 139:447 140:448 141:449 142:450 143:451 144:451 145:452 146:453 147:454 148:454 149:454 150:455 151:456 152:457 153:458 154:459 155:460 156:461 157:462 158:463 159:464 160:464 161:464 162:465 163:466 164:467 165:468 166:469 167:470 168:471 169:472 170:473 171:474 172:475 173:476 174:477 175:477 176:478 177:479 178:480 179:481 180:482 181:483 182:484 183:485 184:486 185:487 186:488 187:489 188:490 189:490 190:491 191:492 192:493 193:494 194:495 195:496 196:497 197:498 198:499 199:500 200:501 201:502 202:503 203:503 204:504 205:505 206:506 207:506 208:506 209:507 210:508 211:509 212:510 213:511 214:512 215:513 216:514 217:515 218:516 219:516 220:517 221:518 222:519 223:520 224:521 225:522 226:523 227:524 228:525 229:526 230:527 231:528 232:529 233:529 234:530 235:531 236:532 237:532 238:532 239:533 240:534 241:535 242:536 243:537 244:538 245:539 246:540 247:541 248:542 249:542 250:543 251:544 252:545 253:545 254:545 255:546 256:547 257:548 258:549 259:550 260:551 261:552 262:553 263:554 264:555 265:555 266:555 267:556 268:557 269:558 270:559 271:560 272:561 273:562 274:563 275:564 276:565 277:566 278:567 279:568 280:569 281:569 282:570 283:571 284:572 285:573 286:574 287:575 288:576 289:577 290:578 291:579 292:580 293:581 294:582 295:583 296:583 297:584 298:585 299:586 300:587 301:588 302:589 303:590 304:591 305:592 306:593 307:594 308:595 309:596 310:597 311:597 312:598 313:599 314:600 315:601 316:602 317:603 318:604 319:605 320:606 321:607 322:608 323:609 324:610 325:611 326:611 327:611 328:611 329:612 330:613 331:614 332:615 333:616 334:617 335:618 336:619 337:620 338:621 339:622 340:623 341:624 342:625 343:625 344:626 345:627 346:628 347:629 348:630 349:631 350:632 351:633 352:634 353:635 354:636 355:637 356:638 357:639 358:639 359:639 360:639 361:640 362:641 363:642 364:643 365:644 366:645 367:646 368:647 369:648 370:649 371:650 372:651 373:651 374:652 375:653 376:654 377:655 378:656 379:657 380:657 381:658 382:659\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 9721 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000009\n",
            "INFO:tensorflow:example_index: 1\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by loyalty [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:446 11:447 12:448 13:449 14:450 15:451 16:451 17:452 18:453 19:454 20:454 21:454 22:455 23:456 24:457 25:458 26:459 27:460 28:461 29:462 30:463 31:464 32:464 33:464 34:465 35:466 36:467 37:468 38:469 39:470 40:471 41:472 42:473 43:474 44:475 45:476 46:477 47:477 48:478 49:479 50:480 51:481 52:482 53:483 54:484 55:485 56:486 57:487 58:488 59:489 60:490 61:490 62:491 63:492 64:493 65:494 66:495 67:496 68:497 69:498 70:499 71:500 72:501 73:502 74:503 75:503 76:504 77:505 78:506 79:506 80:506 81:507 82:508 83:509 84:510 85:511 86:512 87:513 88:514 89:515 90:516 91:516 92:517 93:518 94:519 95:520 96:521 97:522 98:523 99:524 100:525 101:526 102:527 103:528 104:529 105:529 106:530 107:531 108:532 109:532 110:532 111:533 112:534 113:535 114:536 115:537 116:538 117:539 118:540 119:541 120:542 121:542 122:543 123:544 124:545 125:545 126:545 127:546 128:547 129:548 130:549 131:550 132:551 133:552 134:553 135:554 136:555 137:555 138:555 139:556 140:557 141:558 142:559 143:560 144:561 145:562 146:563 147:564 148:565 149:566 150:567 151:568 152:569 153:569 154:570 155:571 156:572 157:573 158:574 159:575 160:576 161:577 162:578 163:579 164:580 165:581 166:582 167:583 168:583 169:584 170:585 171:586 172:587 173:588 174:589 175:590 176:591 177:592 178:593 179:594 180:595 181:596 182:597 183:597 184:598 185:599 186:600 187:601 188:602 189:603 190:604 191:605 192:606 193:607 194:608 195:609 196:610 197:611 198:611 199:611 200:611 201:612 202:613 203:614 204:615 205:616 206:617 207:618 208:619 209:620 210:621 211:622 212:623 213:624 214:625 215:625 216:626 217:627 218:628 219:629 220:630 221:631 222:632 223:633 224:634 225:635 226:636 227:637 228:638 229:639 230:639 231:639 232:639 233:640 234:641 235:642 236:643 237:644 238:645 239:646 240:647 241:648 242:649 243:650 244:651 245:651 246:652 247:653 248:654 249:655 250:656 251:657 252:657 253:658 254:659 255:660 256:661 257:662 258:663 259:664 260:665 261:665 262:666 263:667 264:668 265:669 266:670 267:671 268:671 269:672 270:673 271:674 272:675 273:676 274:677 275:678 276:679 277:679 278:680 279:681 280:682 281:683 282:684 283:685 284:685 285:686 286:687 287:688 288:689 289:690 290:691 291:692 292:693 293:693 294:694 295:695 296:696 297:697 298:698 299:699 300:699 301:700 302:701 303:702 304:703 305:704 306:705 307:706 308:707 309:707 310:707 311:707 312:708 313:709 314:710 315:711 316:712 317:713 318:713 319:714 320:715 321:716 322:717 323:718 324:719 325:720 326:721 327:721 328:722 329:723 330:724 331:725 332:726 333:727 334:727 335:728 336:729 337:730 338:731 339:732 340:733 341:734 342:735 343:735 344:735 345:735 346:736 347:737 348:738 349:739 350:740 351:741 352:741 353:742 354:743 355:744 356:745 357:746 358:747 359:747\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 9721 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000010\n",
            "INFO:tensorflow:example_index: 2\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by brand [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:19 31:20 32:21 33:22 34:23 35:24 36:25 37:25 38:26 39:27 40:28 41:29 42:30 43:31 44:32 45:33 46:34 47:35 48:36 49:37 50:38 51:38 52:39 53:40 54:41 55:41 56:41 57:42 58:43 59:44 60:45 61:46 62:47 63:48 64:49 65:50 66:51 67:51 68:52 69:53 70:54 71:55 72:56 73:57 74:58 75:59 76:60 77:61 78:62 79:63 80:64 81:64 82:65 83:66 84:67 85:67 86:67 87:68 88:69 89:70 90:71 91:72 92:73 93:74 94:75 95:76 96:77 97:77 98:78 99:79 100:80 101:80 102:80 103:81 104:82 105:83 106:84 107:85 108:86 109:87 110:88 111:89 112:90 113:90 114:90 115:91 116:92 117:93 118:94 119:95 120:96 121:97 122:98 123:99 124:100 125:101 126:102 127:103 128:103 129:104 130:105 131:106 132:107 133:108 134:109 135:110 136:111 137:112 138:113 139:114 140:115 141:116 142:116 143:117 144:118 145:119 146:120 147:121 148:122 149:123 150:124 151:125 152:126 153:127 154:128 155:129 156:129 157:130 158:131 159:132 160:132 161:132 162:133 163:134 164:135 165:136 166:137 167:138 168:139 169:140 170:141 171:142 172:142 173:143 174:144 175:145 176:146 177:147 178:148 179:149 180:150 181:151 182:152 183:153 184:154 185:155 186:155 187:156 188:157 189:158 190:158 191:158 192:159 193:160 194:161 195:162 196:163 197:164 198:165 199:166 200:167 201:168 202:168 203:169 204:170 205:171 206:171 207:171 208:172 209:173 210:174 211:175 212:176 213:177 214:178 215:179 216:180 217:181 218:181 219:181 220:182 221:183 222:184 223:185 224:186 225:187 226:188 227:189 228:190 229:191 230:192 231:193 232:194 233:195 234:195 235:196 236:197 237:198 238:199 239:200 240:201 241:202 242:203 243:204 244:205 245:206 246:207 247:208 248:209 249:209 250:210 251:211 252:212 253:213 254:214 255:215 256:216 257:217 258:218 259:219 260:220 261:221 262:222 263:223 264:223 265:224 266:225 267:226 268:227 269:228 270:229 271:230 272:231 273:232 274:233 275:234 276:235 277:236 278:237 279:237 280:237 281:237 282:238 283:239 284:240 285:241 286:242 287:243 288:244 289:245 290:246 291:247 292:248 293:249 294:250 295:251 296:251 297:252 298:253 299:254 300:255 301:256 302:257 303:258 304:259 305:260 306:261 307:262 308:263 309:264 310:265 311:265 312:265 313:265 314:266 315:267 316:268 317:269 318:270 319:271 320:272 321:273 322:274 323:275 324:276 325:277 326:277 327:278 328:279 329:280 330:281 331:282 332:283 333:283 334:284 335:285 336:286 337:287 338:288 339:289 340:290 341:291 342:291 343:292 344:293 345:294 346:295 347:296 348:297 349:297 350:298 351:299 352:300 353:301 354:302 355:303 356:304 357:305 358:305 359:306 360:307 361:308 362:309 363:310 364:311 365:311 366:312 367:313 368:314 369:315 370:316 371:317 372:318 373:319 374:319 375:320 376:321 377:322 378:323 379:324 380:325 381:325 382:326\n",
            "INFO:tensorflow:token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4435 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000011\n",
            "INFO:tensorflow:example_index: 2\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by brand [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:113 11:114 12:115 13:116 14:116 15:117 16:118 17:119 18:120 19:121 20:122 21:123 22:124 23:125 24:126 25:127 26:128 27:129 28:129 29:130 30:131 31:132 32:132 33:132 34:133 35:134 36:135 37:136 38:137 39:138 40:139 41:140 42:141 43:142 44:142 45:143 46:144 47:145 48:146 49:147 50:148 51:149 52:150 53:151 54:152 55:153 56:154 57:155 58:155 59:156 60:157 61:158 62:158 63:158 64:159 65:160 66:161 67:162 68:163 69:164 70:165 71:166 72:167 73:168 74:168 75:169 76:170 77:171 78:171 79:171 80:172 81:173 82:174 83:175 84:176 85:177 86:178 87:179 88:180 89:181 90:181 91:181 92:182 93:183 94:184 95:185 96:186 97:187 98:188 99:189 100:190 101:191 102:192 103:193 104:194 105:195 106:195 107:196 108:197 109:198 110:199 111:200 112:201 113:202 114:203 115:204 116:205 117:206 118:207 119:208 120:209 121:209 122:210 123:211 124:212 125:213 126:214 127:215 128:216 129:217 130:218 131:219 132:220 133:221 134:222 135:223 136:223 137:224 138:225 139:226 140:227 141:228 142:229 143:230 144:231 145:232 146:233 147:234 148:235 149:236 150:237 151:237 152:237 153:237 154:238 155:239 156:240 157:241 158:242 159:243 160:244 161:245 162:246 163:247 164:248 165:249 166:250 167:251 168:251 169:252 170:253 171:254 172:255 173:256 174:257 175:258 176:259 177:260 178:261 179:262 180:263 181:264 182:265 183:265 184:265 185:265 186:266 187:267 188:268 189:269 190:270 191:271 192:272 193:273 194:274 195:275 196:276 197:277 198:277 199:278 200:279 201:280 202:281 203:282 204:283 205:283 206:284 207:285 208:286 209:287 210:288 211:289 212:290 213:291 214:291 215:292 216:293 217:294 218:295 219:296 220:297 221:297 222:298 223:299 224:300 225:301 226:302 227:303 228:304 229:305 230:305 231:306 232:307 233:308 234:309 235:310 236:311 237:311 238:312 239:313 240:314 241:315 242:316 243:317 244:318 245:319 246:319 247:320 248:321 249:322 250:323 251:324 252:325 253:325 254:326 255:327 256:328 257:329 258:330 259:331 260:332 261:333 262:333 263:333 264:333 265:334 266:335 267:336 268:337 269:338 270:339 271:339 272:340 273:341 274:342 275:343 276:344 277:345 278:346 279:347 280:347 281:348 282:349 283:350 284:351 285:352 286:353 287:353 288:354 289:355 290:356 291:357 292:358 293:359 294:360 295:361 296:361 297:361 298:361 299:362 300:363 301:364 302:365 303:366 304:367 305:367 306:368 307:369 308:370 309:371 310:372 311:373 312:373 313:374 314:375 315:376 316:377 317:378 318:379 319:380 320:381 321:382 322:383 323:384 324:385 325:386 326:386 327:387 328:388 329:389 330:390 331:391 332:392 333:393 334:394 335:395 336:396 337:397 338:398 339:399 340:399 341:400 342:401 343:402 344:403 345:404 346:405 347:406 348:407 349:408 350:409 351:410 352:411 353:412 354:412 355:413 356:414 357:415 358:415 359:415 360:416 361:417 362:418 363:419 364:420 365:421 366:422 367:423 368:424 369:425 370:425 371:426 372:427 373:428 374:429 375:430 376:431 377:432 378:433 379:434 380:435 381:436 382:437\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4435 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000012\n",
            "INFO:tensorflow:example_index: 2\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by brand [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:225 11:226 12:227 13:228 14:229 15:230 16:231 17:232 18:233 19:234 20:235 21:236 22:237 23:237 24:237 25:237 26:238 27:239 28:240 29:241 30:242 31:243 32:244 33:245 34:246 35:247 36:248 37:249 38:250 39:251 40:251 41:252 42:253 43:254 44:255 45:256 46:257 47:258 48:259 49:260 50:261 51:262 52:263 53:264 54:265 55:265 56:265 57:265 58:266 59:267 60:268 61:269 62:270 63:271 64:272 65:273 66:274 67:275 68:276 69:277 70:277 71:278 72:279 73:280 74:281 75:282 76:283 77:283 78:284 79:285 80:286 81:287 82:288 83:289 84:290 85:291 86:291 87:292 88:293 89:294 90:295 91:296 92:297 93:297 94:298 95:299 96:300 97:301 98:302 99:303 100:304 101:305 102:305 103:306 104:307 105:308 106:309 107:310 108:311 109:311 110:312 111:313 112:314 113:315 114:316 115:317 116:318 117:319 118:319 119:320 120:321 121:322 122:323 123:324 124:325 125:325 126:326 127:327 128:328 129:329 130:330 131:331 132:332 133:333 134:333 135:333 136:333 137:334 138:335 139:336 140:337 141:338 142:339 143:339 144:340 145:341 146:342 147:343 148:344 149:345 150:346 151:347 152:347 153:348 154:349 155:350 156:351 157:352 158:353 159:353 160:354 161:355 162:356 163:357 164:358 165:359 166:360 167:361 168:361 169:361 170:361 171:362 172:363 173:364 174:365 175:366 176:367 177:367 178:368 179:369 180:370 181:371 182:372 183:373 184:373 185:374 186:375 187:376 188:377 189:378 190:379 191:380 192:381 193:382 194:383 195:384 196:385 197:386 198:386 199:387 200:388 201:389 202:390 203:391 204:392 205:393 206:394 207:395 208:396 209:397 210:398 211:399 212:399 213:400 214:401 215:402 216:403 217:404 218:405 219:406 220:407 221:408 222:409 223:410 224:411 225:412 226:412 227:413 228:414 229:415 230:415 231:415 232:416 233:417 234:418 235:419 236:420 237:421 238:422 239:423 240:424 241:425 242:425 243:426 244:427 245:428 246:429 247:430 248:431 249:432 250:433 251:434 252:435 253:436 254:437 255:438 256:438 257:439 258:440 259:441 260:441 261:441 262:442 263:443 264:444 265:445 266:446 267:447 268:448 269:449 270:450 271:451 272:451 273:452 274:453 275:454 276:454 277:454 278:455 279:456 280:457 281:458 282:459 283:460 284:461 285:462 286:463 287:464 288:464 289:464 290:465 291:466 292:467 293:468 294:469 295:470 296:471 297:472 298:473 299:474 300:475 301:476 302:477 303:477 304:478 305:479 306:480 307:481 308:482 309:483 310:484 311:485 312:486 313:487 314:488 315:489 316:490 317:490 318:491 319:492 320:493 321:494 322:495 323:496 324:497 325:498 326:499 327:500 328:501 329:502 330:503 331:503 332:504 333:505 334:506 335:506 336:506 337:507 338:508 339:509 340:510 341:511 342:512 343:513 344:514 345:515 346:516 347:516 348:517 349:518 350:519 351:520 352:521 353:522 354:523 355:524 356:525 357:526 358:527 359:528 360:529 361:529 362:530 363:531 364:532 365:532 366:532 367:533 368:534 369:535 370:536 371:537 372:538 373:539 374:540 375:541 376:542 377:542 378:543 379:544 380:545 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4435 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000013\n",
            "INFO:tensorflow:example_index: 2\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by brand [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:335 11:336 12:337 13:338 14:339 15:339 16:340 17:341 18:342 19:343 20:344 21:345 22:346 23:347 24:347 25:348 26:349 27:350 28:351 29:352 30:353 31:353 32:354 33:355 34:356 35:357 36:358 37:359 38:360 39:361 40:361 41:361 42:361 43:362 44:363 45:364 46:365 47:366 48:367 49:367 50:368 51:369 52:370 53:371 54:372 55:373 56:373 57:374 58:375 59:376 60:377 61:378 62:379 63:380 64:381 65:382 66:383 67:384 68:385 69:386 70:386 71:387 72:388 73:389 74:390 75:391 76:392 77:393 78:394 79:395 80:396 81:397 82:398 83:399 84:399 85:400 86:401 87:402 88:403 89:404 90:405 91:406 92:407 93:408 94:409 95:410 96:411 97:412 98:412 99:413 100:414 101:415 102:415 103:415 104:416 105:417 106:418 107:419 108:420 109:421 110:422 111:423 112:424 113:425 114:425 115:426 116:427 117:428 118:429 119:430 120:431 121:432 122:433 123:434 124:435 125:436 126:437 127:438 128:438 129:439 130:440 131:441 132:441 133:441 134:442 135:443 136:444 137:445 138:446 139:447 140:448 141:449 142:450 143:451 144:451 145:452 146:453 147:454 148:454 149:454 150:455 151:456 152:457 153:458 154:459 155:460 156:461 157:462 158:463 159:464 160:464 161:464 162:465 163:466 164:467 165:468 166:469 167:470 168:471 169:472 170:473 171:474 172:475 173:476 174:477 175:477 176:478 177:479 178:480 179:481 180:482 181:483 182:484 183:485 184:486 185:487 186:488 187:489 188:490 189:490 190:491 191:492 192:493 193:494 194:495 195:496 196:497 197:498 198:499 199:500 200:501 201:502 202:503 203:503 204:504 205:505 206:506 207:506 208:506 209:507 210:508 211:509 212:510 213:511 214:512 215:513 216:514 217:515 218:516 219:516 220:517 221:518 222:519 223:520 224:521 225:522 226:523 227:524 228:525 229:526 230:527 231:528 232:529 233:529 234:530 235:531 236:532 237:532 238:532 239:533 240:534 241:535 242:536 243:537 244:538 245:539 246:540 247:541 248:542 249:542 250:543 251:544 252:545 253:545 254:545 255:546 256:547 257:548 258:549 259:550 260:551 261:552 262:553 263:554 264:555 265:555 266:555 267:556 268:557 269:558 270:559 271:560 272:561 273:562 274:563 275:564 276:565 277:566 278:567 279:568 280:569 281:569 282:570 283:571 284:572 285:573 286:574 287:575 288:576 289:577 290:578 291:579 292:580 293:581 294:582 295:583 296:583 297:584 298:585 299:586 300:587 301:588 302:589 303:590 304:591 305:592 306:593 307:594 308:595 309:596 310:597 311:597 312:598 313:599 314:600 315:601 316:602 317:603 318:604 319:605 320:606 321:607 322:608 323:609 324:610 325:611 326:611 327:611 328:611 329:612 330:613 331:614 332:615 333:616 334:617 335:618 336:619 337:620 338:621 339:622 340:623 341:624 342:625 343:625 344:626 345:627 346:628 347:629 348:630 349:631 350:632 351:633 352:634 353:635 354:636 355:637 356:638 357:639 358:639 359:639 360:639 361:640 362:641 363:642 364:643 365:644 366:645 367:646 368:647 369:648 370:649 371:650 372:651 373:651 374:652 375:653 376:654 377:655 378:656 379:657 380:657 381:658 382:659\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4435 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000014\n",
            "INFO:tensorflow:example_index: 2\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by brand [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:446 11:447 12:448 13:449 14:450 15:451 16:451 17:452 18:453 19:454 20:454 21:454 22:455 23:456 24:457 25:458 26:459 27:460 28:461 29:462 30:463 31:464 32:464 33:464 34:465 35:466 36:467 37:468 38:469 39:470 40:471 41:472 42:473 43:474 44:475 45:476 46:477 47:477 48:478 49:479 50:480 51:481 52:482 53:483 54:484 55:485 56:486 57:487 58:488 59:489 60:490 61:490 62:491 63:492 64:493 65:494 66:495 67:496 68:497 69:498 70:499 71:500 72:501 73:502 74:503 75:503 76:504 77:505 78:506 79:506 80:506 81:507 82:508 83:509 84:510 85:511 86:512 87:513 88:514 89:515 90:516 91:516 92:517 93:518 94:519 95:520 96:521 97:522 98:523 99:524 100:525 101:526 102:527 103:528 104:529 105:529 106:530 107:531 108:532 109:532 110:532 111:533 112:534 113:535 114:536 115:537 116:538 117:539 118:540 119:541 120:542 121:542 122:543 123:544 124:545 125:545 126:545 127:546 128:547 129:548 130:549 131:550 132:551 133:552 134:553 135:554 136:555 137:555 138:555 139:556 140:557 141:558 142:559 143:560 144:561 145:562 146:563 147:564 148:565 149:566 150:567 151:568 152:569 153:569 154:570 155:571 156:572 157:573 158:574 159:575 160:576 161:577 162:578 163:579 164:580 165:581 166:582 167:583 168:583 169:584 170:585 171:586 172:587 173:588 174:589 175:590 176:591 177:592 178:593 179:594 180:595 181:596 182:597 183:597 184:598 185:599 186:600 187:601 188:602 189:603 190:604 191:605 192:606 193:607 194:608 195:609 196:610 197:611 198:611 199:611 200:611 201:612 202:613 203:614 204:615 205:616 206:617 207:618 208:619 209:620 210:621 211:622 212:623 213:624 214:625 215:625 216:626 217:627 218:628 219:629 220:630 221:631 222:632 223:633 224:634 225:635 226:636 227:637 228:638 229:639 230:639 231:639 232:639 233:640 234:641 235:642 236:643 237:644 238:645 239:646 240:647 241:648 242:649 243:650 244:651 245:651 246:652 247:653 248:654 249:655 250:656 251:657 252:657 253:658 254:659 255:660 256:661 257:662 258:663 259:664 260:665 261:665 262:666 263:667 264:668 265:669 266:670 267:671 268:671 269:672 270:673 271:674 272:675 273:676 274:677 275:678 276:679 277:679 278:680 279:681 280:682 281:683 282:684 283:685 284:685 285:686 286:687 287:688 288:689 289:690 290:691 291:692 292:693 293:693 294:694 295:695 296:696 297:697 298:698 299:699 300:699 301:700 302:701 303:702 304:703 305:704 306:705 307:706 308:707 309:707 310:707 311:707 312:708 313:709 314:710 315:711 316:712 317:713 318:713 319:714 320:715 321:716 322:717 323:718 324:719 325:720 326:721 327:721 328:722 329:723 330:724 331:725 332:726 333:727 334:727 335:728 336:729 337:730 338:731 339:732 340:733 341:734 342:735 343:735 344:735 345:735 346:736 347:737 348:738 349:739 350:740 351:741 352:741 353:742 354:743 355:744 356:745 357:746 358:747 359:747\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4435 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 262\n",
            "INFO:tensorflow:end_position: 277\n",
            "INFO:tensorflow:answer: between jul 2015 and jun 2017 , the total sales trend was this for brand .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000015\n",
            "INFO:tensorflow:example_index: 3\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by department [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:19 31:20 32:21 33:22 34:23 35:24 36:25 37:25 38:26 39:27 40:28 41:29 42:30 43:31 44:32 45:33 46:34 47:35 48:36 49:37 50:38 51:38 52:39 53:40 54:41 55:41 56:41 57:42 58:43 59:44 60:45 61:46 62:47 63:48 64:49 65:50 66:51 67:51 68:52 69:53 70:54 71:55 72:56 73:57 74:58 75:59 76:60 77:61 78:62 79:63 80:64 81:64 82:65 83:66 84:67 85:67 86:67 87:68 88:69 89:70 90:71 91:72 92:73 93:74 94:75 95:76 96:77 97:77 98:78 99:79 100:80 101:80 102:80 103:81 104:82 105:83 106:84 107:85 108:86 109:87 110:88 111:89 112:90 113:90 114:90 115:91 116:92 117:93 118:94 119:95 120:96 121:97 122:98 123:99 124:100 125:101 126:102 127:103 128:103 129:104 130:105 131:106 132:107 133:108 134:109 135:110 136:111 137:112 138:113 139:114 140:115 141:116 142:116 143:117 144:118 145:119 146:120 147:121 148:122 149:123 150:124 151:125 152:126 153:127 154:128 155:129 156:129 157:130 158:131 159:132 160:132 161:132 162:133 163:134 164:135 165:136 166:137 167:138 168:139 169:140 170:141 171:142 172:142 173:143 174:144 175:145 176:146 177:147 178:148 179:149 180:150 181:151 182:152 183:153 184:154 185:155 186:155 187:156 188:157 189:158 190:158 191:158 192:159 193:160 194:161 195:162 196:163 197:164 198:165 199:166 200:167 201:168 202:168 203:169 204:170 205:171 206:171 207:171 208:172 209:173 210:174 211:175 212:176 213:177 214:178 215:179 216:180 217:181 218:181 219:181 220:182 221:183 222:184 223:185 224:186 225:187 226:188 227:189 228:190 229:191 230:192 231:193 232:194 233:195 234:195 235:196 236:197 237:198 238:199 239:200 240:201 241:202 242:203 243:204 244:205 245:206 246:207 247:208 248:209 249:209 250:210 251:211 252:212 253:213 254:214 255:215 256:216 257:217 258:218 259:219 260:220 261:221 262:222 263:223 264:223 265:224 266:225 267:226 268:227 269:228 270:229 271:230 272:231 273:232 274:233 275:234 276:235 277:236 278:237 279:237 280:237 281:237 282:238 283:239 284:240 285:241 286:242 287:243 288:244 289:245 290:246 291:247 292:248 293:249 294:250 295:251 296:251 297:252 298:253 299:254 300:255 301:256 302:257 303:258 304:259 305:260 306:261 307:262 308:263 309:264 310:265 311:265 312:265 313:265 314:266 315:267 316:268 317:269 318:270 319:271 320:272 321:273 322:274 323:275 324:276 325:277 326:277 327:278 328:279 329:280 330:281 331:282 332:283 333:283 334:284 335:285 336:286 337:287 338:288 339:289 340:290 341:291 342:291 343:292 344:293 345:294 346:295 347:296 348:297 349:297 350:298 351:299 352:300 353:301 354:302 355:303 356:304 357:305 358:305 359:306 360:307 361:308 362:309 363:310 364:311 365:311 366:312 367:313 368:314 369:315 370:316 371:317 372:318 373:319 374:319 375:320 376:321 377:322 378:323 379:324 380:325 381:325 382:326\n",
            "INFO:tensorflow:token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 2533 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000016\n",
            "INFO:tensorflow:example_index: 3\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by department [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:113 11:114 12:115 13:116 14:116 15:117 16:118 17:119 18:120 19:121 20:122 21:123 22:124 23:125 24:126 25:127 26:128 27:129 28:129 29:130 30:131 31:132 32:132 33:132 34:133 35:134 36:135 37:136 38:137 39:138 40:139 41:140 42:141 43:142 44:142 45:143 46:144 47:145 48:146 49:147 50:148 51:149 52:150 53:151 54:152 55:153 56:154 57:155 58:155 59:156 60:157 61:158 62:158 63:158 64:159 65:160 66:161 67:162 68:163 69:164 70:165 71:166 72:167 73:168 74:168 75:169 76:170 77:171 78:171 79:171 80:172 81:173 82:174 83:175 84:176 85:177 86:178 87:179 88:180 89:181 90:181 91:181 92:182 93:183 94:184 95:185 96:186 97:187 98:188 99:189 100:190 101:191 102:192 103:193 104:194 105:195 106:195 107:196 108:197 109:198 110:199 111:200 112:201 113:202 114:203 115:204 116:205 117:206 118:207 119:208 120:209 121:209 122:210 123:211 124:212 125:213 126:214 127:215 128:216 129:217 130:218 131:219 132:220 133:221 134:222 135:223 136:223 137:224 138:225 139:226 140:227 141:228 142:229 143:230 144:231 145:232 146:233 147:234 148:235 149:236 150:237 151:237 152:237 153:237 154:238 155:239 156:240 157:241 158:242 159:243 160:244 161:245 162:246 163:247 164:248 165:249 166:250 167:251 168:251 169:252 170:253 171:254 172:255 173:256 174:257 175:258 176:259 177:260 178:261 179:262 180:263 181:264 182:265 183:265 184:265 185:265 186:266 187:267 188:268 189:269 190:270 191:271 192:272 193:273 194:274 195:275 196:276 197:277 198:277 199:278 200:279 201:280 202:281 203:282 204:283 205:283 206:284 207:285 208:286 209:287 210:288 211:289 212:290 213:291 214:291 215:292 216:293 217:294 218:295 219:296 220:297 221:297 222:298 223:299 224:300 225:301 226:302 227:303 228:304 229:305 230:305 231:306 232:307 233:308 234:309 235:310 236:311 237:311 238:312 239:313 240:314 241:315 242:316 243:317 244:318 245:319 246:319 247:320 248:321 249:322 250:323 251:324 252:325 253:325 254:326 255:327 256:328 257:329 258:330 259:331 260:332 261:333 262:333 263:333 264:333 265:334 266:335 267:336 268:337 269:338 270:339 271:339 272:340 273:341 274:342 275:343 276:344 277:345 278:346 279:347 280:347 281:348 282:349 283:350 284:351 285:352 286:353 287:353 288:354 289:355 290:356 291:357 292:358 293:359 294:360 295:361 296:361 297:361 298:361 299:362 300:363 301:364 302:365 303:366 304:367 305:367 306:368 307:369 308:370 309:371 310:372 311:373 312:373 313:374 314:375 315:376 316:377 317:378 318:379 319:380 320:381 321:382 322:383 323:384 324:385 325:386 326:386 327:387 328:388 329:389 330:390 331:391 332:392 333:393 334:394 335:395 336:396 337:397 338:398 339:399 340:399 341:400 342:401 343:402 344:403 345:404 346:405 347:406 348:407 349:408 350:409 351:410 352:411 353:412 354:412 355:413 356:414 357:415 358:415 359:415 360:416 361:417 362:418 363:419 364:420 365:421 366:422 367:423 368:424 369:425 370:425 371:426 372:427 373:428 374:429 375:430 376:431 377:432 378:433 379:434 380:435 381:436 382:437\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 2533 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000017\n",
            "INFO:tensorflow:example_index: 3\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by department [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:225 11:226 12:227 13:228 14:229 15:230 16:231 17:232 18:233 19:234 20:235 21:236 22:237 23:237 24:237 25:237 26:238 27:239 28:240 29:241 30:242 31:243 32:244 33:245 34:246 35:247 36:248 37:249 38:250 39:251 40:251 41:252 42:253 43:254 44:255 45:256 46:257 47:258 48:259 49:260 50:261 51:262 52:263 53:264 54:265 55:265 56:265 57:265 58:266 59:267 60:268 61:269 62:270 63:271 64:272 65:273 66:274 67:275 68:276 69:277 70:277 71:278 72:279 73:280 74:281 75:282 76:283 77:283 78:284 79:285 80:286 81:287 82:288 83:289 84:290 85:291 86:291 87:292 88:293 89:294 90:295 91:296 92:297 93:297 94:298 95:299 96:300 97:301 98:302 99:303 100:304 101:305 102:305 103:306 104:307 105:308 106:309 107:310 108:311 109:311 110:312 111:313 112:314 113:315 114:316 115:317 116:318 117:319 118:319 119:320 120:321 121:322 122:323 123:324 124:325 125:325 126:326 127:327 128:328 129:329 130:330 131:331 132:332 133:333 134:333 135:333 136:333 137:334 138:335 139:336 140:337 141:338 142:339 143:339 144:340 145:341 146:342 147:343 148:344 149:345 150:346 151:347 152:347 153:348 154:349 155:350 156:351 157:352 158:353 159:353 160:354 161:355 162:356 163:357 164:358 165:359 166:360 167:361 168:361 169:361 170:361 171:362 172:363 173:364 174:365 175:366 176:367 177:367 178:368 179:369 180:370 181:371 182:372 183:373 184:373 185:374 186:375 187:376 188:377 189:378 190:379 191:380 192:381 193:382 194:383 195:384 196:385 197:386 198:386 199:387 200:388 201:389 202:390 203:391 204:392 205:393 206:394 207:395 208:396 209:397 210:398 211:399 212:399 213:400 214:401 215:402 216:403 217:404 218:405 219:406 220:407 221:408 222:409 223:410 224:411 225:412 226:412 227:413 228:414 229:415 230:415 231:415 232:416 233:417 234:418 235:419 236:420 237:421 238:422 239:423 240:424 241:425 242:425 243:426 244:427 245:428 246:429 247:430 248:431 249:432 250:433 251:434 252:435 253:436 254:437 255:438 256:438 257:439 258:440 259:441 260:441 261:441 262:442 263:443 264:444 265:445 266:446 267:447 268:448 269:449 270:450 271:451 272:451 273:452 274:453 275:454 276:454 277:454 278:455 279:456 280:457 281:458 282:459 283:460 284:461 285:462 286:463 287:464 288:464 289:464 290:465 291:466 292:467 293:468 294:469 295:470 296:471 297:472 298:473 299:474 300:475 301:476 302:477 303:477 304:478 305:479 306:480 307:481 308:482 309:483 310:484 311:485 312:486 313:487 314:488 315:489 316:490 317:490 318:491 319:492 320:493 321:494 322:495 323:496 324:497 325:498 326:499 327:500 328:501 329:502 330:503 331:503 332:504 333:505 334:506 335:506 336:506 337:507 338:508 339:509 340:510 341:511 342:512 343:513 344:514 345:515 346:516 347:516 348:517 349:518 350:519 351:520 352:521 353:522 354:523 355:524 356:525 357:526 358:527 359:528 360:529 361:529 362:530 363:531 364:532 365:532 366:532 367:533 368:534 369:535 370:536 371:537 372:538 373:539 374:540 375:541 376:542 377:542 378:543 379:544 380:545 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 2533 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000018\n",
            "INFO:tensorflow:example_index: 3\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by department [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:335 11:336 12:337 13:338 14:339 15:339 16:340 17:341 18:342 19:343 20:344 21:345 22:346 23:347 24:347 25:348 26:349 27:350 28:351 29:352 30:353 31:353 32:354 33:355 34:356 35:357 36:358 37:359 38:360 39:361 40:361 41:361 42:361 43:362 44:363 45:364 46:365 47:366 48:367 49:367 50:368 51:369 52:370 53:371 54:372 55:373 56:373 57:374 58:375 59:376 60:377 61:378 62:379 63:380 64:381 65:382 66:383 67:384 68:385 69:386 70:386 71:387 72:388 73:389 74:390 75:391 76:392 77:393 78:394 79:395 80:396 81:397 82:398 83:399 84:399 85:400 86:401 87:402 88:403 89:404 90:405 91:406 92:407 93:408 94:409 95:410 96:411 97:412 98:412 99:413 100:414 101:415 102:415 103:415 104:416 105:417 106:418 107:419 108:420 109:421 110:422 111:423 112:424 113:425 114:425 115:426 116:427 117:428 118:429 119:430 120:431 121:432 122:433 123:434 124:435 125:436 126:437 127:438 128:438 129:439 130:440 131:441 132:441 133:441 134:442 135:443 136:444 137:445 138:446 139:447 140:448 141:449 142:450 143:451 144:451 145:452 146:453 147:454 148:454 149:454 150:455 151:456 152:457 153:458 154:459 155:460 156:461 157:462 158:463 159:464 160:464 161:464 162:465 163:466 164:467 165:468 166:469 167:470 168:471 169:472 170:473 171:474 172:475 173:476 174:477 175:477 176:478 177:479 178:480 179:481 180:482 181:483 182:484 183:485 184:486 185:487 186:488 187:489 188:490 189:490 190:491 191:492 192:493 193:494 194:495 195:496 196:497 197:498 198:499 199:500 200:501 201:502 202:503 203:503 204:504 205:505 206:506 207:506 208:506 209:507 210:508 211:509 212:510 213:511 214:512 215:513 216:514 217:515 218:516 219:516 220:517 221:518 222:519 223:520 224:521 225:522 226:523 227:524 228:525 229:526 230:527 231:528 232:529 233:529 234:530 235:531 236:532 237:532 238:532 239:533 240:534 241:535 242:536 243:537 244:538 245:539 246:540 247:541 248:542 249:542 250:543 251:544 252:545 253:545 254:545 255:546 256:547 257:548 258:549 259:550 260:551 261:552 262:553 263:554 264:555 265:555 266:555 267:556 268:557 269:558 270:559 271:560 272:561 273:562 274:563 275:564 276:565 277:566 278:567 279:568 280:569 281:569 282:570 283:571 284:572 285:573 286:574 287:575 288:576 289:577 290:578 291:579 292:580 293:581 294:582 295:583 296:583 297:584 298:585 299:586 300:587 301:588 302:589 303:590 304:591 305:592 306:593 307:594 308:595 309:596 310:597 311:597 312:598 313:599 314:600 315:601 316:602 317:603 318:604 319:605 320:606 321:607 322:608 323:609 324:610 325:611 326:611 327:611 328:611 329:612 330:613 331:614 332:615 333:616 334:617 335:618 336:619 337:620 338:621 339:622 340:623 341:624 342:625 343:625 344:626 345:627 346:628 347:629 348:630 349:631 350:632 351:633 352:634 353:635 354:636 355:637 356:638 357:639 358:639 359:639 360:639 361:640 362:641 363:642 364:643 365:644 366:645 367:646 368:647 369:648 370:649 371:650 372:651 373:651 374:652 375:653 376:654 377:655 378:656 379:657 380:657 381:658 382:659\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 2533 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 267\n",
            "INFO:tensorflow:end_position: 281\n",
            "INFO:tensorflow:answer: the breakdown of sales between jul 2015 and jun 2017 was this for department .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000019\n",
            "INFO:tensorflow:example_index: 3\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by department [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:446 11:447 12:448 13:449 14:450 15:451 16:451 17:452 18:453 19:454 20:454 21:454 22:455 23:456 24:457 25:458 26:459 27:460 28:461 29:462 30:463 31:464 32:464 33:464 34:465 35:466 36:467 37:468 38:469 39:470 40:471 41:472 42:473 43:474 44:475 45:476 46:477 47:477 48:478 49:479 50:480 51:481 52:482 53:483 54:484 55:485 56:486 57:487 58:488 59:489 60:490 61:490 62:491 63:492 64:493 65:494 66:495 67:496 68:497 69:498 70:499 71:500 72:501 73:502 74:503 75:503 76:504 77:505 78:506 79:506 80:506 81:507 82:508 83:509 84:510 85:511 86:512 87:513 88:514 89:515 90:516 91:516 92:517 93:518 94:519 95:520 96:521 97:522 98:523 99:524 100:525 101:526 102:527 103:528 104:529 105:529 106:530 107:531 108:532 109:532 110:532 111:533 112:534 113:535 114:536 115:537 116:538 117:539 118:540 119:541 120:542 121:542 122:543 123:544 124:545 125:545 126:545 127:546 128:547 129:548 130:549 131:550 132:551 133:552 134:553 135:554 136:555 137:555 138:555 139:556 140:557 141:558 142:559 143:560 144:561 145:562 146:563 147:564 148:565 149:566 150:567 151:568 152:569 153:569 154:570 155:571 156:572 157:573 158:574 159:575 160:576 161:577 162:578 163:579 164:580 165:581 166:582 167:583 168:583 169:584 170:585 171:586 172:587 173:588 174:589 175:590 176:591 177:592 178:593 179:594 180:595 181:596 182:597 183:597 184:598 185:599 186:600 187:601 188:602 189:603 190:604 191:605 192:606 193:607 194:608 195:609 196:610 197:611 198:611 199:611 200:611 201:612 202:613 203:614 204:615 205:616 206:617 207:618 208:619 209:620 210:621 211:622 212:623 213:624 214:625 215:625 216:626 217:627 218:628 219:629 220:630 221:631 222:632 223:633 224:634 225:635 226:636 227:637 228:638 229:639 230:639 231:639 232:639 233:640 234:641 235:642 236:643 237:644 238:645 239:646 240:647 241:648 242:649 243:650 244:651 245:651 246:652 247:653 248:654 249:655 250:656 251:657 252:657 253:658 254:659 255:660 256:661 257:662 258:663 259:664 260:665 261:665 262:666 263:667 264:668 265:669 266:670 267:671 268:671 269:672 270:673 271:674 272:675 273:676 274:677 275:678 276:679 277:679 278:680 279:681 280:682 281:683 282:684 283:685 284:685 285:686 286:687 287:688 288:689 289:690 290:691 291:692 292:693 293:693 294:694 295:695 296:696 297:697 298:698 299:699 300:699 301:700 302:701 303:702 304:703 305:704 306:705 307:706 308:707 309:707 310:707 311:707 312:708 313:709 314:710 315:711 316:712 317:713 318:713 319:714 320:715 321:716 322:717 323:718 324:719 325:720 326:721 327:721 328:722 329:723 330:724 331:725 332:726 333:727 334:727 335:728 336:729 337:730 338:731 339:732 340:733 341:734 342:735 343:735 344:735 345:735 346:736 347:737 348:738 349:739 350:740 351:741 352:741 353:742 354:743 355:744 356:745 357:746 358:747 359:747\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 2533 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 139\n",
            "INFO:tensorflow:end_position: 153\n",
            "INFO:tensorflow:answer: the breakdown of sales between jul 2015 and jun 2017 was this for department .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000020\n",
            "INFO:tensorflow:example_index: 4\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom brand by quantity [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:19 30:20 31:21 32:22 33:23 34:24 35:25 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:33 45:34 46:35 47:36 48:37 49:38 50:38 51:39 52:40 53:41 54:41 55:41 56:42 57:43 58:44 59:45 60:46 61:47 62:48 63:49 64:50 65:51 66:51 67:52 68:53 69:54 70:55 71:56 72:57 73:58 74:59 75:60 76:61 77:62 78:63 79:64 80:64 81:65 82:66 83:67 84:67 85:67 86:68 87:69 88:70 89:71 90:72 91:73 92:74 93:75 94:76 95:77 96:77 97:78 98:79 99:80 100:80 101:80 102:81 103:82 104:83 105:84 106:85 107:86 108:87 109:88 110:89 111:90 112:90 113:90 114:91 115:92 116:93 117:94 118:95 119:96 120:97 121:98 122:99 123:100 124:101 125:102 126:103 127:103 128:104 129:105 130:106 131:107 132:108 133:109 134:110 135:111 136:112 137:113 138:114 139:115 140:116 141:116 142:117 143:118 144:119 145:120 146:121 147:122 148:123 149:124 150:125 151:126 152:127 153:128 154:129 155:129 156:130 157:131 158:132 159:132 160:132 161:133 162:134 163:135 164:136 165:137 166:138 167:139 168:140 169:141 170:142 171:142 172:143 173:144 174:145 175:146 176:147 177:148 178:149 179:150 180:151 181:152 182:153 183:154 184:155 185:155 186:156 187:157 188:158 189:158 190:158 191:159 192:160 193:161 194:162 195:163 196:164 197:165 198:166 199:167 200:168 201:168 202:169 203:170 204:171 205:171 206:171 207:172 208:173 209:174 210:175 211:176 212:177 213:178 214:179 215:180 216:181 217:181 218:181 219:182 220:183 221:184 222:185 223:186 224:187 225:188 226:189 227:190 228:191 229:192 230:193 231:194 232:195 233:195 234:196 235:197 236:198 237:199 238:200 239:201 240:202 241:203 242:204 243:205 244:206 245:207 246:208 247:209 248:209 249:210 250:211 251:212 252:213 253:214 254:215 255:216 256:217 257:218 258:219 259:220 260:221 261:222 262:223 263:223 264:224 265:225 266:226 267:227 268:228 269:229 270:230 271:231 272:232 273:233 274:234 275:235 276:236 277:237 278:237 279:237 280:237 281:238 282:239 283:240 284:241 285:242 286:243 287:244 288:245 289:246 290:247 291:248 292:249 293:250 294:251 295:251 296:252 297:253 298:254 299:255 300:256 301:257 302:258 303:259 304:260 305:261 306:262 307:263 308:264 309:265 310:265 311:265 312:265 313:266 314:267 315:268 316:269 317:270 318:271 319:272 320:273 321:274 322:275 323:276 324:277 325:277 326:278 327:279 328:280 329:281 330:282 331:283 332:283 333:284 334:285 335:286 336:287 337:288 338:289 339:290 340:291 341:291 342:292 343:293 344:294 345:295 346:296 347:297 348:297 349:298 350:299 351:300 352:301 353:302 354:303 355:304 356:305 357:305 358:306 359:307 360:308 361:309 362:310 363:311 364:311 365:312 366:313 367:314 368:315 369:316 370:317 371:318 372:319 373:319 374:320 375:321 376:322 377:323 378:324 379:325 380:325 381:326 382:327\n",
            "INFO:tensorflow:token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4435 2011 11712 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 128\n",
            "INFO:tensorflow:end_position: 141\n",
            "INFO:tensorflow:answer: the bottom brand for quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000021\n",
            "INFO:tensorflow:example_index: 4\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom brand by quantity [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:113 10:114 11:115 12:116 13:116 14:117 15:118 16:119 17:120 18:121 19:122 20:123 21:124 22:125 23:126 24:127 25:128 26:129 27:129 28:130 29:131 30:132 31:132 32:132 33:133 34:134 35:135 36:136 37:137 38:138 39:139 40:140 41:141 42:142 43:142 44:143 45:144 46:145 47:146 48:147 49:148 50:149 51:150 52:151 53:152 54:153 55:154 56:155 57:155 58:156 59:157 60:158 61:158 62:158 63:159 64:160 65:161 66:162 67:163 68:164 69:165 70:166 71:167 72:168 73:168 74:169 75:170 76:171 77:171 78:171 79:172 80:173 81:174 82:175 83:176 84:177 85:178 86:179 87:180 88:181 89:181 90:181 91:182 92:183 93:184 94:185 95:186 96:187 97:188 98:189 99:190 100:191 101:192 102:193 103:194 104:195 105:195 106:196 107:197 108:198 109:199 110:200 111:201 112:202 113:203 114:204 115:205 116:206 117:207 118:208 119:209 120:209 121:210 122:211 123:212 124:213 125:214 126:215 127:216 128:217 129:218 130:219 131:220 132:221 133:222 134:223 135:223 136:224 137:225 138:226 139:227 140:228 141:229 142:230 143:231 144:232 145:233 146:234 147:235 148:236 149:237 150:237 151:237 152:237 153:238 154:239 155:240 156:241 157:242 158:243 159:244 160:245 161:246 162:247 163:248 164:249 165:250 166:251 167:251 168:252 169:253 170:254 171:255 172:256 173:257 174:258 175:259 176:260 177:261 178:262 179:263 180:264 181:265 182:265 183:265 184:265 185:266 186:267 187:268 188:269 189:270 190:271 191:272 192:273 193:274 194:275 195:276 196:277 197:277 198:278 199:279 200:280 201:281 202:282 203:283 204:283 205:284 206:285 207:286 208:287 209:288 210:289 211:290 212:291 213:291 214:292 215:293 216:294 217:295 218:296 219:297 220:297 221:298 222:299 223:300 224:301 225:302 226:303 227:304 228:305 229:305 230:306 231:307 232:308 233:309 234:310 235:311 236:311 237:312 238:313 239:314 240:315 241:316 242:317 243:318 244:319 245:319 246:320 247:321 248:322 249:323 250:324 251:325 252:325 253:326 254:327 255:328 256:329 257:330 258:331 259:332 260:333 261:333 262:333 263:333 264:334 265:335 266:336 267:337 268:338 269:339 270:339 271:340 272:341 273:342 274:343 275:344 276:345 277:346 278:347 279:347 280:348 281:349 282:350 283:351 284:352 285:353 286:353 287:354 288:355 289:356 290:357 291:358 292:359 293:360 294:361 295:361 296:361 297:361 298:362 299:363 300:364 301:365 302:366 303:367 304:367 305:368 306:369 307:370 308:371 309:372 310:373 311:373 312:374 313:375 314:376 315:377 316:378 317:379 318:380 319:381 320:382 321:383 322:384 323:385 324:386 325:386 326:387 327:388 328:389 329:390 330:391 331:392 332:393 333:394 334:395 335:396 336:397 337:398 338:399 339:399 340:400 341:401 342:402 343:403 344:404 345:405 346:406 347:407 348:408 349:409 350:410 351:411 352:412 353:412 354:413 355:414 356:415 357:415 358:415 359:416 360:417 361:418 362:419 363:420 364:421 365:422 366:423 367:424 368:425 369:425 370:426 371:427 372:428 373:429 374:430 375:431 376:432 377:433 378:434 379:435 380:436 381:437 382:438\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4435 2011 11712 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000022\n",
            "INFO:tensorflow:example_index: 4\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom brand by quantity [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:225 10:226 11:227 12:228 13:229 14:230 15:231 16:232 17:233 18:234 19:235 20:236 21:237 22:237 23:237 24:237 25:238 26:239 27:240 28:241 29:242 30:243 31:244 32:245 33:246 34:247 35:248 36:249 37:250 38:251 39:251 40:252 41:253 42:254 43:255 44:256 45:257 46:258 47:259 48:260 49:261 50:262 51:263 52:264 53:265 54:265 55:265 56:265 57:266 58:267 59:268 60:269 61:270 62:271 63:272 64:273 65:274 66:275 67:276 68:277 69:277 70:278 71:279 72:280 73:281 74:282 75:283 76:283 77:284 78:285 79:286 80:287 81:288 82:289 83:290 84:291 85:291 86:292 87:293 88:294 89:295 90:296 91:297 92:297 93:298 94:299 95:300 96:301 97:302 98:303 99:304 100:305 101:305 102:306 103:307 104:308 105:309 106:310 107:311 108:311 109:312 110:313 111:314 112:315 113:316 114:317 115:318 116:319 117:319 118:320 119:321 120:322 121:323 122:324 123:325 124:325 125:326 126:327 127:328 128:329 129:330 130:331 131:332 132:333 133:333 134:333 135:333 136:334 137:335 138:336 139:337 140:338 141:339 142:339 143:340 144:341 145:342 146:343 147:344 148:345 149:346 150:347 151:347 152:348 153:349 154:350 155:351 156:352 157:353 158:353 159:354 160:355 161:356 162:357 163:358 164:359 165:360 166:361 167:361 168:361 169:361 170:362 171:363 172:364 173:365 174:366 175:367 176:367 177:368 178:369 179:370 180:371 181:372 182:373 183:373 184:374 185:375 186:376 187:377 188:378 189:379 190:380 191:381 192:382 193:383 194:384 195:385 196:386 197:386 198:387 199:388 200:389 201:390 202:391 203:392 204:393 205:394 206:395 207:396 208:397 209:398 210:399 211:399 212:400 213:401 214:402 215:403 216:404 217:405 218:406 219:407 220:408 221:409 222:410 223:411 224:412 225:412 226:413 227:414 228:415 229:415 230:415 231:416 232:417 233:418 234:419 235:420 236:421 237:422 238:423 239:424 240:425 241:425 242:426 243:427 244:428 245:429 246:430 247:431 248:432 249:433 250:434 251:435 252:436 253:437 254:438 255:438 256:439 257:440 258:441 259:441 260:441 261:442 262:443 263:444 264:445 265:446 266:447 267:448 268:449 269:450 270:451 271:451 272:452 273:453 274:454 275:454 276:454 277:455 278:456 279:457 280:458 281:459 282:460 283:461 284:462 285:463 286:464 287:464 288:464 289:465 290:466 291:467 292:468 293:469 294:470 295:471 296:472 297:473 298:474 299:475 300:476 301:477 302:477 303:478 304:479 305:480 306:481 307:482 308:483 309:484 310:485 311:486 312:487 313:488 314:489 315:490 316:490 317:491 318:492 319:493 320:494 321:495 322:496 323:497 324:498 325:499 326:500 327:501 328:502 329:503 330:503 331:504 332:505 333:506 334:506 335:506 336:507 337:508 338:509 339:510 340:511 341:512 342:513 343:514 344:515 345:516 346:516 347:517 348:518 349:519 350:520 351:521 352:522 353:523 354:524 355:525 356:526 357:527 358:528 359:529 360:529 361:530 362:531 363:532 364:532 365:532 366:533 367:534 368:535 369:536 370:537 371:538 372:539 373:540 374:541 375:542 376:542 377:543 378:544 379:545 380:545 381:545 382:546\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4435 2011 11712 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000023\n",
            "INFO:tensorflow:example_index: 4\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom brand by quantity [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:335 10:336 11:337 12:338 13:339 14:339 15:340 16:341 17:342 18:343 19:344 20:345 21:346 22:347 23:347 24:348 25:349 26:350 27:351 28:352 29:353 30:353 31:354 32:355 33:356 34:357 35:358 36:359 37:360 38:361 39:361 40:361 41:361 42:362 43:363 44:364 45:365 46:366 47:367 48:367 49:368 50:369 51:370 52:371 53:372 54:373 55:373 56:374 57:375 58:376 59:377 60:378 61:379 62:380 63:381 64:382 65:383 66:384 67:385 68:386 69:386 70:387 71:388 72:389 73:390 74:391 75:392 76:393 77:394 78:395 79:396 80:397 81:398 82:399 83:399 84:400 85:401 86:402 87:403 88:404 89:405 90:406 91:407 92:408 93:409 94:410 95:411 96:412 97:412 98:413 99:414 100:415 101:415 102:415 103:416 104:417 105:418 106:419 107:420 108:421 109:422 110:423 111:424 112:425 113:425 114:426 115:427 116:428 117:429 118:430 119:431 120:432 121:433 122:434 123:435 124:436 125:437 126:438 127:438 128:439 129:440 130:441 131:441 132:441 133:442 134:443 135:444 136:445 137:446 138:447 139:448 140:449 141:450 142:451 143:451 144:452 145:453 146:454 147:454 148:454 149:455 150:456 151:457 152:458 153:459 154:460 155:461 156:462 157:463 158:464 159:464 160:464 161:465 162:466 163:467 164:468 165:469 166:470 167:471 168:472 169:473 170:474 171:475 172:476 173:477 174:477 175:478 176:479 177:480 178:481 179:482 180:483 181:484 182:485 183:486 184:487 185:488 186:489 187:490 188:490 189:491 190:492 191:493 192:494 193:495 194:496 195:497 196:498 197:499 198:500 199:501 200:502 201:503 202:503 203:504 204:505 205:506 206:506 207:506 208:507 209:508 210:509 211:510 212:511 213:512 214:513 215:514 216:515 217:516 218:516 219:517 220:518 221:519 222:520 223:521 224:522 225:523 226:524 227:525 228:526 229:527 230:528 231:529 232:529 233:530 234:531 235:532 236:532 237:532 238:533 239:534 240:535 241:536 242:537 243:538 244:539 245:540 246:541 247:542 248:542 249:543 250:544 251:545 252:545 253:545 254:546 255:547 256:548 257:549 258:550 259:551 260:552 261:553 262:554 263:555 264:555 265:555 266:556 267:557 268:558 269:559 270:560 271:561 272:562 273:563 274:564 275:565 276:566 277:567 278:568 279:569 280:569 281:570 282:571 283:572 284:573 285:574 286:575 287:576 288:577 289:578 290:579 291:580 292:581 293:582 294:583 295:583 296:584 297:585 298:586 299:587 300:588 301:589 302:590 303:591 304:592 305:593 306:594 307:595 308:596 309:597 310:597 311:598 312:599 313:600 314:601 315:602 316:603 317:604 318:605 319:606 320:607 321:608 322:609 323:610 324:611 325:611 326:611 327:611 328:612 329:613 330:614 331:615 332:616 333:617 334:618 335:619 336:620 337:621 338:622 339:623 340:624 341:625 342:625 343:626 344:627 345:628 346:629 347:630 348:631 349:632 350:633 351:634 352:635 353:636 354:637 355:638 356:639 357:639 358:639 359:639 360:640 361:641 362:642 363:643 364:644 365:645 366:646 367:647 368:648 369:649 370:650 371:651 372:651 373:652 374:653 375:654 376:655 377:656 378:657 379:657 380:658 381:659 382:660\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4435 2011 11712 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000024\n",
            "INFO:tensorflow:example_index: 4\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom brand by quantity [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:446 10:447 11:448 12:449 13:450 14:451 15:451 16:452 17:453 18:454 19:454 20:454 21:455 22:456 23:457 24:458 25:459 26:460 27:461 28:462 29:463 30:464 31:464 32:464 33:465 34:466 35:467 36:468 37:469 38:470 39:471 40:472 41:473 42:474 43:475 44:476 45:477 46:477 47:478 48:479 49:480 50:481 51:482 52:483 53:484 54:485 55:486 56:487 57:488 58:489 59:490 60:490 61:491 62:492 63:493 64:494 65:495 66:496 67:497 68:498 69:499 70:500 71:501 72:502 73:503 74:503 75:504 76:505 77:506 78:506 79:506 80:507 81:508 82:509 83:510 84:511 85:512 86:513 87:514 88:515 89:516 90:516 91:517 92:518 93:519 94:520 95:521 96:522 97:523 98:524 99:525 100:526 101:527 102:528 103:529 104:529 105:530 106:531 107:532 108:532 109:532 110:533 111:534 112:535 113:536 114:537 115:538 116:539 117:540 118:541 119:542 120:542 121:543 122:544 123:545 124:545 125:545 126:546 127:547 128:548 129:549 130:550 131:551 132:552 133:553 134:554 135:555 136:555 137:555 138:556 139:557 140:558 141:559 142:560 143:561 144:562 145:563 146:564 147:565 148:566 149:567 150:568 151:569 152:569 153:570 154:571 155:572 156:573 157:574 158:575 159:576 160:577 161:578 162:579 163:580 164:581 165:582 166:583 167:583 168:584 169:585 170:586 171:587 172:588 173:589 174:590 175:591 176:592 177:593 178:594 179:595 180:596 181:597 182:597 183:598 184:599 185:600 186:601 187:602 188:603 189:604 190:605 191:606 192:607 193:608 194:609 195:610 196:611 197:611 198:611 199:611 200:612 201:613 202:614 203:615 204:616 205:617 206:618 207:619 208:620 209:621 210:622 211:623 212:624 213:625 214:625 215:626 216:627 217:628 218:629 219:630 220:631 221:632 222:633 223:634 224:635 225:636 226:637 227:638 228:639 229:639 230:639 231:639 232:640 233:641 234:642 235:643 236:644 237:645 238:646 239:647 240:648 241:649 242:650 243:651 244:651 245:652 246:653 247:654 248:655 249:656 250:657 251:657 252:658 253:659 254:660 255:661 256:662 257:663 258:664 259:665 260:665 261:666 262:667 263:668 264:669 265:670 266:671 267:671 268:672 269:673 270:674 271:675 272:676 273:677 274:678 275:679 276:679 277:680 278:681 279:682 280:683 281:684 282:685 283:685 284:686 285:687 286:688 287:689 288:690 289:691 290:692 291:693 292:693 293:694 294:695 295:696 296:697 297:698 298:699 299:699 300:700 301:701 302:702 303:703 304:704 305:705 306:706 307:707 308:707 309:707 310:707 311:708 312:709 313:710 314:711 315:712 316:713 317:713 318:714 319:715 320:716 321:717 322:718 323:719 324:720 325:721 326:721 327:722 328:723 329:724 330:725 331:726 332:727 333:727 334:728 335:729 336:730 337:731 338:732 339:733 340:734 341:735 342:735 343:735 344:735 345:736 346:737 347:738 348:739 349:740 350:741 351:741 352:742 353:743 354:744 355:745 356:746 357:747 358:747\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4435 2011 11712 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000025\n",
            "INFO:tensorflow:example_index: 5\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the top age _ band by quantity [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:19 32:20 33:21 34:22 35:23 36:24 37:25 38:25 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:36 50:37 51:38 52:38 53:39 54:40 55:41 56:41 57:41 58:42 59:43 60:44 61:45 62:46 63:47 64:48 65:49 66:50 67:51 68:51 69:52 70:53 71:54 72:55 73:56 74:57 75:58 76:59 77:60 78:61 79:62 80:63 81:64 82:64 83:65 84:66 85:67 86:67 87:67 88:68 89:69 90:70 91:71 92:72 93:73 94:74 95:75 96:76 97:77 98:77 99:78 100:79 101:80 102:80 103:80 104:81 105:82 106:83 107:84 108:85 109:86 110:87 111:88 112:89 113:90 114:90 115:90 116:91 117:92 118:93 119:94 120:95 121:96 122:97 123:98 124:99 125:100 126:101 127:102 128:103 129:103 130:104 131:105 132:106 133:107 134:108 135:109 136:110 137:111 138:112 139:113 140:114 141:115 142:116 143:116 144:117 145:118 146:119 147:120 148:121 149:122 150:123 151:124 152:125 153:126 154:127 155:128 156:129 157:129 158:130 159:131 160:132 161:132 162:132 163:133 164:134 165:135 166:136 167:137 168:138 169:139 170:140 171:141 172:142 173:142 174:143 175:144 176:145 177:146 178:147 179:148 180:149 181:150 182:151 183:152 184:153 185:154 186:155 187:155 188:156 189:157 190:158 191:158 192:158 193:159 194:160 195:161 196:162 197:163 198:164 199:165 200:166 201:167 202:168 203:168 204:169 205:170 206:171 207:171 208:171 209:172 210:173 211:174 212:175 213:176 214:177 215:178 216:179 217:180 218:181 219:181 220:181 221:182 222:183 223:184 224:185 225:186 226:187 227:188 228:189 229:190 230:191 231:192 232:193 233:194 234:195 235:195 236:196 237:197 238:198 239:199 240:200 241:201 242:202 243:203 244:204 245:205 246:206 247:207 248:208 249:209 250:209 251:210 252:211 253:212 254:213 255:214 256:215 257:216 258:217 259:218 260:219 261:220 262:221 263:222 264:223 265:223 266:224 267:225 268:226 269:227 270:228 271:229 272:230 273:231 274:232 275:233 276:234 277:235 278:236 279:237 280:237 281:237 282:237 283:238 284:239 285:240 286:241 287:242 288:243 289:244 290:245 291:246 292:247 293:248 294:249 295:250 296:251 297:251 298:252 299:253 300:254 301:255 302:256 303:257 304:258 305:259 306:260 307:261 308:262 309:263 310:264 311:265 312:265 313:265 314:265 315:266 316:267 317:268 318:269 319:270 320:271 321:272 322:273 323:274 324:275 325:276 326:277 327:277 328:278 329:279 330:280 331:281 332:282 333:283 334:283 335:284 336:285 337:286 338:287 339:288 340:289 341:290 342:291 343:291 344:292 345:293 346:294 347:295 348:296 349:297 350:297 351:298 352:299 353:300 354:301 355:302 356:303 357:304 358:305 359:305 360:306 361:307 362:308 363:309 364:310 365:311 366:311 367:312 368:313 369:314 370:315 371:316 372:317 373:318 374:319 375:319 376:320 377:321 378:322 379:323 380:324 381:325 382:325\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 2287 1035 2316 2011 11712 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 53\n",
            "INFO:tensorflow:end_position: 68\n",
            "INFO:tensorflow:answer: the top age _ band for quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000026\n",
            "INFO:tensorflow:example_index: 5\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the top age _ band by quantity [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:113 12:114 13:115 14:116 15:116 16:117 17:118 18:119 19:120 20:121 21:122 22:123 23:124 24:125 25:126 26:127 27:128 28:129 29:129 30:130 31:131 32:132 33:132 34:132 35:133 36:134 37:135 38:136 39:137 40:138 41:139 42:140 43:141 44:142 45:142 46:143 47:144 48:145 49:146 50:147 51:148 52:149 53:150 54:151 55:152 56:153 57:154 58:155 59:155 60:156 61:157 62:158 63:158 64:158 65:159 66:160 67:161 68:162 69:163 70:164 71:165 72:166 73:167 74:168 75:168 76:169 77:170 78:171 79:171 80:171 81:172 82:173 83:174 84:175 85:176 86:177 87:178 88:179 89:180 90:181 91:181 92:181 93:182 94:183 95:184 96:185 97:186 98:187 99:188 100:189 101:190 102:191 103:192 104:193 105:194 106:195 107:195 108:196 109:197 110:198 111:199 112:200 113:201 114:202 115:203 116:204 117:205 118:206 119:207 120:208 121:209 122:209 123:210 124:211 125:212 126:213 127:214 128:215 129:216 130:217 131:218 132:219 133:220 134:221 135:222 136:223 137:223 138:224 139:225 140:226 141:227 142:228 143:229 144:230 145:231 146:232 147:233 148:234 149:235 150:236 151:237 152:237 153:237 154:237 155:238 156:239 157:240 158:241 159:242 160:243 161:244 162:245 163:246 164:247 165:248 166:249 167:250 168:251 169:251 170:252 171:253 172:254 173:255 174:256 175:257 176:258 177:259 178:260 179:261 180:262 181:263 182:264 183:265 184:265 185:265 186:265 187:266 188:267 189:268 190:269 191:270 192:271 193:272 194:273 195:274 196:275 197:276 198:277 199:277 200:278 201:279 202:280 203:281 204:282 205:283 206:283 207:284 208:285 209:286 210:287 211:288 212:289 213:290 214:291 215:291 216:292 217:293 218:294 219:295 220:296 221:297 222:297 223:298 224:299 225:300 226:301 227:302 228:303 229:304 230:305 231:305 232:306 233:307 234:308 235:309 236:310 237:311 238:311 239:312 240:313 241:314 242:315 243:316 244:317 245:318 246:319 247:319 248:320 249:321 250:322 251:323 252:324 253:325 254:325 255:326 256:327 257:328 258:329 259:330 260:331 261:332 262:333 263:333 264:333 265:333 266:334 267:335 268:336 269:337 270:338 271:339 272:339 273:340 274:341 275:342 276:343 277:344 278:345 279:346 280:347 281:347 282:348 283:349 284:350 285:351 286:352 287:353 288:353 289:354 290:355 291:356 292:357 293:358 294:359 295:360 296:361 297:361 298:361 299:361 300:362 301:363 302:364 303:365 304:366 305:367 306:367 307:368 308:369 309:370 310:371 311:372 312:373 313:373 314:374 315:375 316:376 317:377 318:378 319:379 320:380 321:381 322:382 323:383 324:384 325:385 326:386 327:386 328:387 329:388 330:389 331:390 332:391 333:392 334:393 335:394 336:395 337:396 338:397 339:398 340:399 341:399 342:400 343:401 344:402 345:403 346:404 347:405 348:406 349:407 350:408 351:409 352:410 353:411 354:412 355:412 356:413 357:414 358:415 359:415 360:415 361:416 362:417 363:418 364:419 365:420 366:421 367:422 368:423 369:424 370:425 371:425 372:426 373:427 374:428 375:429 376:430 377:431 378:432 379:433 380:434 381:435 382:436\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 2287 1035 2316 2011 11712 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000027\n",
            "INFO:tensorflow:example_index: 5\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the top age _ band by quantity [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:225 12:226 13:227 14:228 15:229 16:230 17:231 18:232 19:233 20:234 21:235 22:236 23:237 24:237 25:237 26:237 27:238 28:239 29:240 30:241 31:242 32:243 33:244 34:245 35:246 36:247 37:248 38:249 39:250 40:251 41:251 42:252 43:253 44:254 45:255 46:256 47:257 48:258 49:259 50:260 51:261 52:262 53:263 54:264 55:265 56:265 57:265 58:265 59:266 60:267 61:268 62:269 63:270 64:271 65:272 66:273 67:274 68:275 69:276 70:277 71:277 72:278 73:279 74:280 75:281 76:282 77:283 78:283 79:284 80:285 81:286 82:287 83:288 84:289 85:290 86:291 87:291 88:292 89:293 90:294 91:295 92:296 93:297 94:297 95:298 96:299 97:300 98:301 99:302 100:303 101:304 102:305 103:305 104:306 105:307 106:308 107:309 108:310 109:311 110:311 111:312 112:313 113:314 114:315 115:316 116:317 117:318 118:319 119:319 120:320 121:321 122:322 123:323 124:324 125:325 126:325 127:326 128:327 129:328 130:329 131:330 132:331 133:332 134:333 135:333 136:333 137:333 138:334 139:335 140:336 141:337 142:338 143:339 144:339 145:340 146:341 147:342 148:343 149:344 150:345 151:346 152:347 153:347 154:348 155:349 156:350 157:351 158:352 159:353 160:353 161:354 162:355 163:356 164:357 165:358 166:359 167:360 168:361 169:361 170:361 171:361 172:362 173:363 174:364 175:365 176:366 177:367 178:367 179:368 180:369 181:370 182:371 183:372 184:373 185:373 186:374 187:375 188:376 189:377 190:378 191:379 192:380 193:381 194:382 195:383 196:384 197:385 198:386 199:386 200:387 201:388 202:389 203:390 204:391 205:392 206:393 207:394 208:395 209:396 210:397 211:398 212:399 213:399 214:400 215:401 216:402 217:403 218:404 219:405 220:406 221:407 222:408 223:409 224:410 225:411 226:412 227:412 228:413 229:414 230:415 231:415 232:415 233:416 234:417 235:418 236:419 237:420 238:421 239:422 240:423 241:424 242:425 243:425 244:426 245:427 246:428 247:429 248:430 249:431 250:432 251:433 252:434 253:435 254:436 255:437 256:438 257:438 258:439 259:440 260:441 261:441 262:441 263:442 264:443 265:444 266:445 267:446 268:447 269:448 270:449 271:450 272:451 273:451 274:452 275:453 276:454 277:454 278:454 279:455 280:456 281:457 282:458 283:459 284:460 285:461 286:462 287:463 288:464 289:464 290:464 291:465 292:466 293:467 294:468 295:469 296:470 297:471 298:472 299:473 300:474 301:475 302:476 303:477 304:477 305:478 306:479 307:480 308:481 309:482 310:483 311:484 312:485 313:486 314:487 315:488 316:489 317:490 318:490 319:491 320:492 321:493 322:494 323:495 324:496 325:497 326:498 327:499 328:500 329:501 330:502 331:503 332:503 333:504 334:505 335:506 336:506 337:506 338:507 339:508 340:509 341:510 342:511 343:512 344:513 345:514 346:515 347:516 348:516 349:517 350:518 351:519 352:520 353:521 354:522 355:523 356:524 357:525 358:526 359:527 360:528 361:529 362:529 363:530 364:531 365:532 366:532 367:532 368:533 369:534 370:535 371:536 372:537 373:538 374:539 375:540 376:541 377:542 378:542 379:543 380:544 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 2287 1035 2316 2011 11712 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000028\n",
            "INFO:tensorflow:example_index: 5\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the top age _ band by quantity [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:335 12:336 13:337 14:338 15:339 16:339 17:340 18:341 19:342 20:343 21:344 22:345 23:346 24:347 25:347 26:348 27:349 28:350 29:351 30:352 31:353 32:353 33:354 34:355 35:356 36:357 37:358 38:359 39:360 40:361 41:361 42:361 43:361 44:362 45:363 46:364 47:365 48:366 49:367 50:367 51:368 52:369 53:370 54:371 55:372 56:373 57:373 58:374 59:375 60:376 61:377 62:378 63:379 64:380 65:381 66:382 67:383 68:384 69:385 70:386 71:386 72:387 73:388 74:389 75:390 76:391 77:392 78:393 79:394 80:395 81:396 82:397 83:398 84:399 85:399 86:400 87:401 88:402 89:403 90:404 91:405 92:406 93:407 94:408 95:409 96:410 97:411 98:412 99:412 100:413 101:414 102:415 103:415 104:415 105:416 106:417 107:418 108:419 109:420 110:421 111:422 112:423 113:424 114:425 115:425 116:426 117:427 118:428 119:429 120:430 121:431 122:432 123:433 124:434 125:435 126:436 127:437 128:438 129:438 130:439 131:440 132:441 133:441 134:441 135:442 136:443 137:444 138:445 139:446 140:447 141:448 142:449 143:450 144:451 145:451 146:452 147:453 148:454 149:454 150:454 151:455 152:456 153:457 154:458 155:459 156:460 157:461 158:462 159:463 160:464 161:464 162:464 163:465 164:466 165:467 166:468 167:469 168:470 169:471 170:472 171:473 172:474 173:475 174:476 175:477 176:477 177:478 178:479 179:480 180:481 181:482 182:483 183:484 184:485 185:486 186:487 187:488 188:489 189:490 190:490 191:491 192:492 193:493 194:494 195:495 196:496 197:497 198:498 199:499 200:500 201:501 202:502 203:503 204:503 205:504 206:505 207:506 208:506 209:506 210:507 211:508 212:509 213:510 214:511 215:512 216:513 217:514 218:515 219:516 220:516 221:517 222:518 223:519 224:520 225:521 226:522 227:523 228:524 229:525 230:526 231:527 232:528 233:529 234:529 235:530 236:531 237:532 238:532 239:532 240:533 241:534 242:535 243:536 244:537 245:538 246:539 247:540 248:541 249:542 250:542 251:543 252:544 253:545 254:545 255:545 256:546 257:547 258:548 259:549 260:550 261:551 262:552 263:553 264:554 265:555 266:555 267:555 268:556 269:557 270:558 271:559 272:560 273:561 274:562 275:563 276:564 277:565 278:566 279:567 280:568 281:569 282:569 283:570 284:571 285:572 286:573 287:574 288:575 289:576 290:577 291:578 292:579 293:580 294:581 295:582 296:583 297:583 298:584 299:585 300:586 301:587 302:588 303:589 304:590 305:591 306:592 307:593 308:594 309:595 310:596 311:597 312:597 313:598 314:599 315:600 316:601 317:602 318:603 319:604 320:605 321:606 322:607 323:608 324:609 325:610 326:611 327:611 328:611 329:611 330:612 331:613 332:614 333:615 334:616 335:617 336:618 337:619 338:620 339:621 340:622 341:623 342:624 343:625 344:625 345:626 346:627 347:628 348:629 349:630 350:631 351:632 352:633 353:634 354:635 355:636 356:637 357:638 358:639 359:639 360:639 361:639 362:640 363:641 364:642 365:643 366:644 367:645 368:646 369:647 370:648 371:649 372:650 373:651 374:651 375:652 376:653 377:654 378:655 379:656 380:657 381:657 382:658\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 2287 1035 2316 2011 11712 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000029\n",
            "INFO:tensorflow:example_index: 5\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the top age _ band by quantity [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:446 12:447 13:448 14:449 15:450 16:451 17:451 18:452 19:453 20:454 21:454 22:454 23:455 24:456 25:457 26:458 27:459 28:460 29:461 30:462 31:463 32:464 33:464 34:464 35:465 36:466 37:467 38:468 39:469 40:470 41:471 42:472 43:473 44:474 45:475 46:476 47:477 48:477 49:478 50:479 51:480 52:481 53:482 54:483 55:484 56:485 57:486 58:487 59:488 60:489 61:490 62:490 63:491 64:492 65:493 66:494 67:495 68:496 69:497 70:498 71:499 72:500 73:501 74:502 75:503 76:503 77:504 78:505 79:506 80:506 81:506 82:507 83:508 84:509 85:510 86:511 87:512 88:513 89:514 90:515 91:516 92:516 93:517 94:518 95:519 96:520 97:521 98:522 99:523 100:524 101:525 102:526 103:527 104:528 105:529 106:529 107:530 108:531 109:532 110:532 111:532 112:533 113:534 114:535 115:536 116:537 117:538 118:539 119:540 120:541 121:542 122:542 123:543 124:544 125:545 126:545 127:545 128:546 129:547 130:548 131:549 132:550 133:551 134:552 135:553 136:554 137:555 138:555 139:555 140:556 141:557 142:558 143:559 144:560 145:561 146:562 147:563 148:564 149:565 150:566 151:567 152:568 153:569 154:569 155:570 156:571 157:572 158:573 159:574 160:575 161:576 162:577 163:578 164:579 165:580 166:581 167:582 168:583 169:583 170:584 171:585 172:586 173:587 174:588 175:589 176:590 177:591 178:592 179:593 180:594 181:595 182:596 183:597 184:597 185:598 186:599 187:600 188:601 189:602 190:603 191:604 192:605 193:606 194:607 195:608 196:609 197:610 198:611 199:611 200:611 201:611 202:612 203:613 204:614 205:615 206:616 207:617 208:618 209:619 210:620 211:621 212:622 213:623 214:624 215:625 216:625 217:626 218:627 219:628 220:629 221:630 222:631 223:632 224:633 225:634 226:635 227:636 228:637 229:638 230:639 231:639 232:639 233:639 234:640 235:641 236:642 237:643 238:644 239:645 240:646 241:647 242:648 243:649 244:650 245:651 246:651 247:652 248:653 249:654 250:655 251:656 252:657 253:657 254:658 255:659 256:660 257:661 258:662 259:663 260:664 261:665 262:665 263:666 264:667 265:668 266:669 267:670 268:671 269:671 270:672 271:673 272:674 273:675 274:676 275:677 276:678 277:679 278:679 279:680 280:681 281:682 282:683 283:684 284:685 285:685 286:686 287:687 288:688 289:689 290:690 291:691 292:692 293:693 294:693 295:694 296:695 297:696 298:697 299:698 300:699 301:699 302:700 303:701 304:702 305:703 306:704 307:705 308:706 309:707 310:707 311:707 312:707 313:708 314:709 315:710 316:711 317:712 318:713 319:713 320:714 321:715 322:716 323:717 324:718 325:719 326:720 327:721 328:721 329:722 330:723 331:724 332:725 333:726 334:727 335:727 336:728 337:729 338:730 339:731 340:732 341:733 342:734 343:735 344:735 345:735 346:735 347:736 348:737 349:738 350:739 351:740 352:741 353:741 354:742 355:743 356:744 357:745 358:746 359:747 360:747\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 2287 1035 2316 2011 11712 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000030\n",
            "INFO:tensorflow:example_index: 6\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by brand [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:19 31:20 32:21 33:22 34:23 35:24 36:25 37:25 38:26 39:27 40:28 41:29 42:30 43:31 44:32 45:33 46:34 47:35 48:36 49:37 50:38 51:38 52:39 53:40 54:41 55:41 56:41 57:42 58:43 59:44 60:45 61:46 62:47 63:48 64:49 65:50 66:51 67:51 68:52 69:53 70:54 71:55 72:56 73:57 74:58 75:59 76:60 77:61 78:62 79:63 80:64 81:64 82:65 83:66 84:67 85:67 86:67 87:68 88:69 89:70 90:71 91:72 92:73 93:74 94:75 95:76 96:77 97:77 98:78 99:79 100:80 101:80 102:80 103:81 104:82 105:83 106:84 107:85 108:86 109:87 110:88 111:89 112:90 113:90 114:90 115:91 116:92 117:93 118:94 119:95 120:96 121:97 122:98 123:99 124:100 125:101 126:102 127:103 128:103 129:104 130:105 131:106 132:107 133:108 134:109 135:110 136:111 137:112 138:113 139:114 140:115 141:116 142:116 143:117 144:118 145:119 146:120 147:121 148:122 149:123 150:124 151:125 152:126 153:127 154:128 155:129 156:129 157:130 158:131 159:132 160:132 161:132 162:133 163:134 164:135 165:136 166:137 167:138 168:139 169:140 170:141 171:142 172:142 173:143 174:144 175:145 176:146 177:147 178:148 179:149 180:150 181:151 182:152 183:153 184:154 185:155 186:155 187:156 188:157 189:158 190:158 191:158 192:159 193:160 194:161 195:162 196:163 197:164 198:165 199:166 200:167 201:168 202:168 203:169 204:170 205:171 206:171 207:171 208:172 209:173 210:174 211:175 212:176 213:177 214:178 215:179 216:180 217:181 218:181 219:181 220:182 221:183 222:184 223:185 224:186 225:187 226:188 227:189 228:190 229:191 230:192 231:193 232:194 233:195 234:195 235:196 236:197 237:198 238:199 239:200 240:201 241:202 242:203 243:204 244:205 245:206 246:207 247:208 248:209 249:209 250:210 251:211 252:212 253:213 254:214 255:215 256:216 257:217 258:218 259:219 260:220 261:221 262:222 263:223 264:223 265:224 266:225 267:226 268:227 269:228 270:229 271:230 272:231 273:232 274:233 275:234 276:235 277:236 278:237 279:237 280:237 281:237 282:238 283:239 284:240 285:241 286:242 287:243 288:244 289:245 290:246 291:247 292:248 293:249 294:250 295:251 296:251 297:252 298:253 299:254 300:255 301:256 302:257 303:258 304:259 305:260 306:261 307:262 308:263 309:264 310:265 311:265 312:265 313:265 314:266 315:267 316:268 317:269 318:270 319:271 320:272 321:273 322:274 323:275 324:276 325:277 326:277 327:278 328:279 329:280 330:281 331:282 332:283 333:283 334:284 335:285 336:286 337:287 338:288 339:289 340:290 341:291 342:291 343:292 344:293 345:294 346:295 347:296 348:297 349:297 350:298 351:299 352:300 353:301 354:302 355:303 356:304 357:305 358:305 359:306 360:307 361:308 362:309 363:310 364:311 365:311 366:312 367:313 368:314 369:315 370:316 371:317 372:318 373:319 374:319 375:320 376:321 377:322 378:323 379:324 380:325 381:325 382:326\n",
            "INFO:tensorflow:token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 4435 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 235\n",
            "INFO:tensorflow:end_position: 249\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this for brand .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000031\n",
            "INFO:tensorflow:example_index: 6\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by brand [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:113 11:114 12:115 13:116 14:116 15:117 16:118 17:119 18:120 19:121 20:122 21:123 22:124 23:125 24:126 25:127 26:128 27:129 28:129 29:130 30:131 31:132 32:132 33:132 34:133 35:134 36:135 37:136 38:137 39:138 40:139 41:140 42:141 43:142 44:142 45:143 46:144 47:145 48:146 49:147 50:148 51:149 52:150 53:151 54:152 55:153 56:154 57:155 58:155 59:156 60:157 61:158 62:158 63:158 64:159 65:160 66:161 67:162 68:163 69:164 70:165 71:166 72:167 73:168 74:168 75:169 76:170 77:171 78:171 79:171 80:172 81:173 82:174 83:175 84:176 85:177 86:178 87:179 88:180 89:181 90:181 91:181 92:182 93:183 94:184 95:185 96:186 97:187 98:188 99:189 100:190 101:191 102:192 103:193 104:194 105:195 106:195 107:196 108:197 109:198 110:199 111:200 112:201 113:202 114:203 115:204 116:205 117:206 118:207 119:208 120:209 121:209 122:210 123:211 124:212 125:213 126:214 127:215 128:216 129:217 130:218 131:219 132:220 133:221 134:222 135:223 136:223 137:224 138:225 139:226 140:227 141:228 142:229 143:230 144:231 145:232 146:233 147:234 148:235 149:236 150:237 151:237 152:237 153:237 154:238 155:239 156:240 157:241 158:242 159:243 160:244 161:245 162:246 163:247 164:248 165:249 166:250 167:251 168:251 169:252 170:253 171:254 172:255 173:256 174:257 175:258 176:259 177:260 178:261 179:262 180:263 181:264 182:265 183:265 184:265 185:265 186:266 187:267 188:268 189:269 190:270 191:271 192:272 193:273 194:274 195:275 196:276 197:277 198:277 199:278 200:279 201:280 202:281 203:282 204:283 205:283 206:284 207:285 208:286 209:287 210:288 211:289 212:290 213:291 214:291 215:292 216:293 217:294 218:295 219:296 220:297 221:297 222:298 223:299 224:300 225:301 226:302 227:303 228:304 229:305 230:305 231:306 232:307 233:308 234:309 235:310 236:311 237:311 238:312 239:313 240:314 241:315 242:316 243:317 244:318 245:319 246:319 247:320 248:321 249:322 250:323 251:324 252:325 253:325 254:326 255:327 256:328 257:329 258:330 259:331 260:332 261:333 262:333 263:333 264:333 265:334 266:335 267:336 268:337 269:338 270:339 271:339 272:340 273:341 274:342 275:343 276:344 277:345 278:346 279:347 280:347 281:348 282:349 283:350 284:351 285:352 286:353 287:353 288:354 289:355 290:356 291:357 292:358 293:359 294:360 295:361 296:361 297:361 298:361 299:362 300:363 301:364 302:365 303:366 304:367 305:367 306:368 307:369 308:370 309:371 310:372 311:373 312:373 313:374 314:375 315:376 316:377 317:378 318:379 319:380 320:381 321:382 322:383 323:384 324:385 325:386 326:386 327:387 328:388 329:389 330:390 331:391 332:392 333:393 334:394 335:395 336:396 337:397 338:398 339:399 340:399 341:400 342:401 343:402 344:403 345:404 346:405 347:406 348:407 349:408 350:409 351:410 352:411 353:412 354:412 355:413 356:414 357:415 358:415 359:415 360:416 361:417 362:418 363:419 364:420 365:421 366:422 367:423 368:424 369:425 370:425 371:426 372:427 373:428 374:429 375:430 376:431 377:432 378:433 379:434 380:435 381:436 382:437\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 4435 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 107\n",
            "INFO:tensorflow:end_position: 121\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this for brand .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000032\n",
            "INFO:tensorflow:example_index: 6\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by brand [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:225 11:226 12:227 13:228 14:229 15:230 16:231 17:232 18:233 19:234 20:235 21:236 22:237 23:237 24:237 25:237 26:238 27:239 28:240 29:241 30:242 31:243 32:244 33:245 34:246 35:247 36:248 37:249 38:250 39:251 40:251 41:252 42:253 43:254 44:255 45:256 46:257 47:258 48:259 49:260 50:261 51:262 52:263 53:264 54:265 55:265 56:265 57:265 58:266 59:267 60:268 61:269 62:270 63:271 64:272 65:273 66:274 67:275 68:276 69:277 70:277 71:278 72:279 73:280 74:281 75:282 76:283 77:283 78:284 79:285 80:286 81:287 82:288 83:289 84:290 85:291 86:291 87:292 88:293 89:294 90:295 91:296 92:297 93:297 94:298 95:299 96:300 97:301 98:302 99:303 100:304 101:305 102:305 103:306 104:307 105:308 106:309 107:310 108:311 109:311 110:312 111:313 112:314 113:315 114:316 115:317 116:318 117:319 118:319 119:320 120:321 121:322 122:323 123:324 124:325 125:325 126:326 127:327 128:328 129:329 130:330 131:331 132:332 133:333 134:333 135:333 136:333 137:334 138:335 139:336 140:337 141:338 142:339 143:339 144:340 145:341 146:342 147:343 148:344 149:345 150:346 151:347 152:347 153:348 154:349 155:350 156:351 157:352 158:353 159:353 160:354 161:355 162:356 163:357 164:358 165:359 166:360 167:361 168:361 169:361 170:361 171:362 172:363 173:364 174:365 175:366 176:367 177:367 178:368 179:369 180:370 181:371 182:372 183:373 184:373 185:374 186:375 187:376 188:377 189:378 190:379 191:380 192:381 193:382 194:383 195:384 196:385 197:386 198:386 199:387 200:388 201:389 202:390 203:391 204:392 205:393 206:394 207:395 208:396 209:397 210:398 211:399 212:399 213:400 214:401 215:402 216:403 217:404 218:405 219:406 220:407 221:408 222:409 223:410 224:411 225:412 226:412 227:413 228:414 229:415 230:415 231:415 232:416 233:417 234:418 235:419 236:420 237:421 238:422 239:423 240:424 241:425 242:425 243:426 244:427 245:428 246:429 247:430 248:431 249:432 250:433 251:434 252:435 253:436 254:437 255:438 256:438 257:439 258:440 259:441 260:441 261:441 262:442 263:443 264:444 265:445 266:446 267:447 268:448 269:449 270:450 271:451 272:451 273:452 274:453 275:454 276:454 277:454 278:455 279:456 280:457 281:458 282:459 283:460 284:461 285:462 286:463 287:464 288:464 289:464 290:465 291:466 292:467 293:468 294:469 295:470 296:471 297:472 298:473 299:474 300:475 301:476 302:477 303:477 304:478 305:479 306:480 307:481 308:482 309:483 310:484 311:485 312:486 313:487 314:488 315:489 316:490 317:490 318:491 319:492 320:493 321:494 322:495 323:496 324:497 325:498 326:499 327:500 328:501 329:502 330:503 331:503 332:504 333:505 334:506 335:506 336:506 337:507 338:508 339:509 340:510 341:511 342:512 343:513 344:514 345:515 346:516 347:516 348:517 349:518 350:519 351:520 352:521 353:522 354:523 355:524 356:525 357:526 358:527 359:528 360:529 361:529 362:530 363:531 364:532 365:532 366:532 367:533 368:534 369:535 370:536 371:537 372:538 373:539 374:540 375:541 376:542 377:542 378:543 379:544 380:545 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 4435 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000033\n",
            "INFO:tensorflow:example_index: 6\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by brand [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:335 11:336 12:337 13:338 14:339 15:339 16:340 17:341 18:342 19:343 20:344 21:345 22:346 23:347 24:347 25:348 26:349 27:350 28:351 29:352 30:353 31:353 32:354 33:355 34:356 35:357 36:358 37:359 38:360 39:361 40:361 41:361 42:361 43:362 44:363 45:364 46:365 47:366 48:367 49:367 50:368 51:369 52:370 53:371 54:372 55:373 56:373 57:374 58:375 59:376 60:377 61:378 62:379 63:380 64:381 65:382 66:383 67:384 68:385 69:386 70:386 71:387 72:388 73:389 74:390 75:391 76:392 77:393 78:394 79:395 80:396 81:397 82:398 83:399 84:399 85:400 86:401 87:402 88:403 89:404 90:405 91:406 92:407 93:408 94:409 95:410 96:411 97:412 98:412 99:413 100:414 101:415 102:415 103:415 104:416 105:417 106:418 107:419 108:420 109:421 110:422 111:423 112:424 113:425 114:425 115:426 116:427 117:428 118:429 119:430 120:431 121:432 122:433 123:434 124:435 125:436 126:437 127:438 128:438 129:439 130:440 131:441 132:441 133:441 134:442 135:443 136:444 137:445 138:446 139:447 140:448 141:449 142:450 143:451 144:451 145:452 146:453 147:454 148:454 149:454 150:455 151:456 152:457 153:458 154:459 155:460 156:461 157:462 158:463 159:464 160:464 161:464 162:465 163:466 164:467 165:468 166:469 167:470 168:471 169:472 170:473 171:474 172:475 173:476 174:477 175:477 176:478 177:479 178:480 179:481 180:482 181:483 182:484 183:485 184:486 185:487 186:488 187:489 188:490 189:490 190:491 191:492 192:493 193:494 194:495 195:496 196:497 197:498 198:499 199:500 200:501 201:502 202:503 203:503 204:504 205:505 206:506 207:506 208:506 209:507 210:508 211:509 212:510 213:511 214:512 215:513 216:514 217:515 218:516 219:516 220:517 221:518 222:519 223:520 224:521 225:522 226:523 227:524 228:525 229:526 230:527 231:528 232:529 233:529 234:530 235:531 236:532 237:532 238:532 239:533 240:534 241:535 242:536 243:537 244:538 245:539 246:540 247:541 248:542 249:542 250:543 251:544 252:545 253:545 254:545 255:546 256:547 257:548 258:549 259:550 260:551 261:552 262:553 263:554 264:555 265:555 266:555 267:556 268:557 269:558 270:559 271:560 272:561 273:562 274:563 275:564 276:565 277:566 278:567 279:568 280:569 281:569 282:570 283:571 284:572 285:573 286:574 287:575 288:576 289:577 290:578 291:579 292:580 293:581 294:582 295:583 296:583 297:584 298:585 299:586 300:587 301:588 302:589 303:590 304:591 305:592 306:593 307:594 308:595 309:596 310:597 311:597 312:598 313:599 314:600 315:601 316:602 317:603 318:604 319:605 320:606 321:607 322:608 323:609 324:610 325:611 326:611 327:611 328:611 329:612 330:613 331:614 332:615 333:616 334:617 335:618 336:619 337:620 338:621 339:622 340:623 341:624 342:625 343:625 344:626 345:627 346:628 347:629 348:630 349:631 350:632 351:633 352:634 353:635 354:636 355:637 356:638 357:639 358:639 359:639 360:639 361:640 362:641 363:642 364:643 365:644 366:645 367:646 368:647 369:648 370:649 371:650 372:651 373:651 374:652 375:653 376:654 377:655 378:656 379:657 380:657 381:658 382:659\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 4435 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000034\n",
            "INFO:tensorflow:example_index: 6\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity by brand [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:446 11:447 12:448 13:449 14:450 15:451 16:451 17:452 18:453 19:454 20:454 21:454 22:455 23:456 24:457 25:458 26:459 27:460 28:461 29:462 30:463 31:464 32:464 33:464 34:465 35:466 36:467 37:468 38:469 39:470 40:471 41:472 42:473 43:474 44:475 45:476 46:477 47:477 48:478 49:479 50:480 51:481 52:482 53:483 54:484 55:485 56:486 57:487 58:488 59:489 60:490 61:490 62:491 63:492 64:493 65:494 66:495 67:496 68:497 69:498 70:499 71:500 72:501 73:502 74:503 75:503 76:504 77:505 78:506 79:506 80:506 81:507 82:508 83:509 84:510 85:511 86:512 87:513 88:514 89:515 90:516 91:516 92:517 93:518 94:519 95:520 96:521 97:522 98:523 99:524 100:525 101:526 102:527 103:528 104:529 105:529 106:530 107:531 108:532 109:532 110:532 111:533 112:534 113:535 114:536 115:537 116:538 117:539 118:540 119:541 120:542 121:542 122:543 123:544 124:545 125:545 126:545 127:546 128:547 129:548 130:549 131:550 132:551 133:552 134:553 135:554 136:555 137:555 138:555 139:556 140:557 141:558 142:559 143:560 144:561 145:562 146:563 147:564 148:565 149:566 150:567 151:568 152:569 153:569 154:570 155:571 156:572 157:573 158:574 159:575 160:576 161:577 162:578 163:579 164:580 165:581 166:582 167:583 168:583 169:584 170:585 171:586 172:587 173:588 174:589 175:590 176:591 177:592 178:593 179:594 180:595 181:596 182:597 183:597 184:598 185:599 186:600 187:601 188:602 189:603 190:604 191:605 192:606 193:607 194:608 195:609 196:610 197:611 198:611 199:611 200:611 201:612 202:613 203:614 204:615 205:616 206:617 207:618 208:619 209:620 210:621 211:622 212:623 213:624 214:625 215:625 216:626 217:627 218:628 219:629 220:630 221:631 222:632 223:633 224:634 225:635 226:636 227:637 228:638 229:639 230:639 231:639 232:639 233:640 234:641 235:642 236:643 237:644 238:645 239:646 240:647 241:648 242:649 243:650 244:651 245:651 246:652 247:653 248:654 249:655 250:656 251:657 252:657 253:658 254:659 255:660 256:661 257:662 258:663 259:664 260:665 261:665 262:666 263:667 264:668 265:669 266:670 267:671 268:671 269:672 270:673 271:674 272:675 273:676 274:677 275:678 276:679 277:679 278:680 279:681 280:682 281:683 282:684 283:685 284:685 285:686 286:687 287:688 288:689 289:690 290:691 291:692 292:693 293:693 294:694 295:695 296:696 297:697 298:698 299:699 300:699 301:700 302:701 303:702 304:703 305:704 306:705 307:706 308:707 309:707 310:707 311:707 312:708 313:709 314:710 315:711 316:712 317:713 318:713 319:714 320:715 321:716 322:717 323:718 324:719 325:720 326:721 327:721 328:722 329:723 330:724 331:725 332:726 333:727 334:727 335:728 336:729 337:730 338:731 339:732 340:733 341:734 342:735 343:735 344:735 345:735 346:736 347:737 348:738 349:739 350:740 351:741 352:741 353:742 354:743 355:744 356:745 357:746 358:747 359:747\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 2011 4435 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000035\n",
            "INFO:tensorflow:example_index: 7\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by sales [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:19 30:20 31:21 32:22 33:23 34:24 35:25 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:33 45:34 46:35 47:36 48:37 49:38 50:38 51:39 52:40 53:41 54:41 55:41 56:42 57:43 58:44 59:45 60:46 61:47 62:48 63:49 64:50 65:51 66:51 67:52 68:53 69:54 70:55 71:56 72:57 73:58 74:59 75:60 76:61 77:62 78:63 79:64 80:64 81:65 82:66 83:67 84:67 85:67 86:68 87:69 88:70 89:71 90:72 91:73 92:74 93:75 94:76 95:77 96:77 97:78 98:79 99:80 100:80 101:80 102:81 103:82 104:83 105:84 106:85 107:86 108:87 109:88 110:89 111:90 112:90 113:90 114:91 115:92 116:93 117:94 118:95 119:96 120:97 121:98 122:99 123:100 124:101 125:102 126:103 127:103 128:104 129:105 130:106 131:107 132:108 133:109 134:110 135:111 136:112 137:113 138:114 139:115 140:116 141:116 142:117 143:118 144:119 145:120 146:121 147:122 148:123 149:124 150:125 151:126 152:127 153:128 154:129 155:129 156:130 157:131 158:132 159:132 160:132 161:133 162:134 163:135 164:136 165:137 166:138 167:139 168:140 169:141 170:142 171:142 172:143 173:144 174:145 175:146 176:147 177:148 178:149 179:150 180:151 181:152 182:153 183:154 184:155 185:155 186:156 187:157 188:158 189:158 190:158 191:159 192:160 193:161 194:162 195:163 196:164 197:165 198:166 199:167 200:168 201:168 202:169 203:170 204:171 205:171 206:171 207:172 208:173 209:174 210:175 211:176 212:177 213:178 214:179 215:180 216:181 217:181 218:181 219:182 220:183 221:184 222:185 223:186 224:187 225:188 226:189 227:190 228:191 229:192 230:193 231:194 232:195 233:195 234:196 235:197 236:198 237:199 238:200 239:201 240:202 241:203 242:204 243:205 244:206 245:207 246:208 247:209 248:209 249:210 250:211 251:212 252:213 253:214 254:215 255:216 256:217 257:218 258:219 259:220 260:221 261:222 262:223 263:223 264:224 265:225 266:226 267:227 268:228 269:229 270:230 271:231 272:232 273:233 274:234 275:235 276:236 277:237 278:237 279:237 280:237 281:238 282:239 283:240 284:241 285:242 286:243 287:244 288:245 289:246 290:247 291:248 292:249 293:250 294:251 295:251 296:252 297:253 298:254 299:255 300:256 301:257 302:258 303:259 304:260 305:261 306:262 307:263 308:264 309:265 310:265 311:265 312:265 313:266 314:267 315:268 316:269 317:270 318:271 319:272 320:273 321:274 322:275 323:276 324:277 325:277 326:278 327:279 328:280 329:281 330:282 331:283 332:283 333:284 334:285 335:286 336:287 337:288 338:289 339:290 340:291 341:291 342:292 343:293 344:294 345:295 346:296 347:297 348:297 349:298 350:299 351:300 352:301 353:302 354:303 355:304 356:305 357:305 358:306 359:307 360:308 361:309 362:310 363:311 364:311 365:312 366:313 367:314 368:315 369:316 370:317 371:318 372:319 373:319 374:320 375:321 376:322 377:323 378:324 379:325 380:325 381:326 382:327\n",
            "INFO:tensorflow:token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 4341 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000036\n",
            "INFO:tensorflow:example_index: 7\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by sales [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:113 10:114 11:115 12:116 13:116 14:117 15:118 16:119 17:120 18:121 19:122 20:123 21:124 22:125 23:126 24:127 25:128 26:129 27:129 28:130 29:131 30:132 31:132 32:132 33:133 34:134 35:135 36:136 37:137 38:138 39:139 40:140 41:141 42:142 43:142 44:143 45:144 46:145 47:146 48:147 49:148 50:149 51:150 52:151 53:152 54:153 55:154 56:155 57:155 58:156 59:157 60:158 61:158 62:158 63:159 64:160 65:161 66:162 67:163 68:164 69:165 70:166 71:167 72:168 73:168 74:169 75:170 76:171 77:171 78:171 79:172 80:173 81:174 82:175 83:176 84:177 85:178 86:179 87:180 88:181 89:181 90:181 91:182 92:183 93:184 94:185 95:186 96:187 97:188 98:189 99:190 100:191 101:192 102:193 103:194 104:195 105:195 106:196 107:197 108:198 109:199 110:200 111:201 112:202 113:203 114:204 115:205 116:206 117:207 118:208 119:209 120:209 121:210 122:211 123:212 124:213 125:214 126:215 127:216 128:217 129:218 130:219 131:220 132:221 133:222 134:223 135:223 136:224 137:225 138:226 139:227 140:228 141:229 142:230 143:231 144:232 145:233 146:234 147:235 148:236 149:237 150:237 151:237 152:237 153:238 154:239 155:240 156:241 157:242 158:243 159:244 160:245 161:246 162:247 163:248 164:249 165:250 166:251 167:251 168:252 169:253 170:254 171:255 172:256 173:257 174:258 175:259 176:260 177:261 178:262 179:263 180:264 181:265 182:265 183:265 184:265 185:266 186:267 187:268 188:269 189:270 190:271 191:272 192:273 193:274 194:275 195:276 196:277 197:277 198:278 199:279 200:280 201:281 202:282 203:283 204:283 205:284 206:285 207:286 208:287 209:288 210:289 211:290 212:291 213:291 214:292 215:293 216:294 217:295 218:296 219:297 220:297 221:298 222:299 223:300 224:301 225:302 226:303 227:304 228:305 229:305 230:306 231:307 232:308 233:309 234:310 235:311 236:311 237:312 238:313 239:314 240:315 241:316 242:317 243:318 244:319 245:319 246:320 247:321 248:322 249:323 250:324 251:325 252:325 253:326 254:327 255:328 256:329 257:330 258:331 259:332 260:333 261:333 262:333 263:333 264:334 265:335 266:336 267:337 268:338 269:339 270:339 271:340 272:341 273:342 274:343 275:344 276:345 277:346 278:347 279:347 280:348 281:349 282:350 283:351 284:352 285:353 286:353 287:354 288:355 289:356 290:357 291:358 292:359 293:360 294:361 295:361 296:361 297:361 298:362 299:363 300:364 301:365 302:366 303:367 304:367 305:368 306:369 307:370 308:371 309:372 310:373 311:373 312:374 313:375 314:376 315:377 316:378 317:379 318:380 319:381 320:382 321:383 322:384 323:385 324:386 325:386 326:387 327:388 328:389 329:390 330:391 331:392 332:393 333:394 334:395 335:396 336:397 337:398 338:399 339:399 340:400 341:401 342:402 343:403 344:404 345:405 346:406 347:407 348:408 349:409 350:410 351:411 352:412 353:412 354:413 355:414 356:415 357:415 358:415 359:416 360:417 361:418 362:419 363:420 364:421 365:422 366:423 367:424 368:425 369:425 370:426 371:427 372:428 373:429 374:430 375:431 376:432 377:433 378:434 379:435 380:436 381:437 382:438\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 4341 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000037\n",
            "INFO:tensorflow:example_index: 7\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by sales [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:225 10:226 11:227 12:228 13:229 14:230 15:231 16:232 17:233 18:234 19:235 20:236 21:237 22:237 23:237 24:237 25:238 26:239 27:240 28:241 29:242 30:243 31:244 32:245 33:246 34:247 35:248 36:249 37:250 38:251 39:251 40:252 41:253 42:254 43:255 44:256 45:257 46:258 47:259 48:260 49:261 50:262 51:263 52:264 53:265 54:265 55:265 56:265 57:266 58:267 59:268 60:269 61:270 62:271 63:272 64:273 65:274 66:275 67:276 68:277 69:277 70:278 71:279 72:280 73:281 74:282 75:283 76:283 77:284 78:285 79:286 80:287 81:288 82:289 83:290 84:291 85:291 86:292 87:293 88:294 89:295 90:296 91:297 92:297 93:298 94:299 95:300 96:301 97:302 98:303 99:304 100:305 101:305 102:306 103:307 104:308 105:309 106:310 107:311 108:311 109:312 110:313 111:314 112:315 113:316 114:317 115:318 116:319 117:319 118:320 119:321 120:322 121:323 122:324 123:325 124:325 125:326 126:327 127:328 128:329 129:330 130:331 131:332 132:333 133:333 134:333 135:333 136:334 137:335 138:336 139:337 140:338 141:339 142:339 143:340 144:341 145:342 146:343 147:344 148:345 149:346 150:347 151:347 152:348 153:349 154:350 155:351 156:352 157:353 158:353 159:354 160:355 161:356 162:357 163:358 164:359 165:360 166:361 167:361 168:361 169:361 170:362 171:363 172:364 173:365 174:366 175:367 176:367 177:368 178:369 179:370 180:371 181:372 182:373 183:373 184:374 185:375 186:376 187:377 188:378 189:379 190:380 191:381 192:382 193:383 194:384 195:385 196:386 197:386 198:387 199:388 200:389 201:390 202:391 203:392 204:393 205:394 206:395 207:396 208:397 209:398 210:399 211:399 212:400 213:401 214:402 215:403 216:404 217:405 218:406 219:407 220:408 221:409 222:410 223:411 224:412 225:412 226:413 227:414 228:415 229:415 230:415 231:416 232:417 233:418 234:419 235:420 236:421 237:422 238:423 239:424 240:425 241:425 242:426 243:427 244:428 245:429 246:430 247:431 248:432 249:433 250:434 251:435 252:436 253:437 254:438 255:438 256:439 257:440 258:441 259:441 260:441 261:442 262:443 263:444 264:445 265:446 266:447 267:448 268:449 269:450 270:451 271:451 272:452 273:453 274:454 275:454 276:454 277:455 278:456 279:457 280:458 281:459 282:460 283:461 284:462 285:463 286:464 287:464 288:464 289:465 290:466 291:467 292:468 293:469 294:470 295:471 296:472 297:473 298:474 299:475 300:476 301:477 302:477 303:478 304:479 305:480 306:481 307:482 308:483 309:484 310:485 311:486 312:487 313:488 314:489 315:490 316:490 317:491 318:492 319:493 320:494 321:495 322:496 323:497 324:498 325:499 326:500 327:501 328:502 329:503 330:503 331:504 332:505 333:506 334:506 335:506 336:507 337:508 338:509 339:510 340:511 341:512 342:513 343:514 344:515 345:516 346:516 347:517 348:518 349:519 350:520 351:521 352:522 353:523 354:524 355:525 356:526 357:527 358:528 359:529 360:529 361:530 362:531 363:532 364:532 365:532 366:533 367:534 368:535 369:536 370:537 371:538 372:539 373:540 374:541 375:542 376:542 377:543 378:544 379:545 380:545 381:545 382:546\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 4341 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 317\n",
            "INFO:tensorflow:end_position: 330\n",
            "INFO:tensorflow:answer: the bottom commodity for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000038\n",
            "INFO:tensorflow:example_index: 7\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by sales [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:335 10:336 11:337 12:338 13:339 14:339 15:340 16:341 17:342 18:343 19:344 20:345 21:346 22:347 23:347 24:348 25:349 26:350 27:351 28:352 29:353 30:353 31:354 32:355 33:356 34:357 35:358 36:359 37:360 38:361 39:361 40:361 41:361 42:362 43:363 44:364 45:365 46:366 47:367 48:367 49:368 50:369 51:370 52:371 53:372 54:373 55:373 56:374 57:375 58:376 59:377 60:378 61:379 62:380 63:381 64:382 65:383 66:384 67:385 68:386 69:386 70:387 71:388 72:389 73:390 74:391 75:392 76:393 77:394 78:395 79:396 80:397 81:398 82:399 83:399 84:400 85:401 86:402 87:403 88:404 89:405 90:406 91:407 92:408 93:409 94:410 95:411 96:412 97:412 98:413 99:414 100:415 101:415 102:415 103:416 104:417 105:418 106:419 107:420 108:421 109:422 110:423 111:424 112:425 113:425 114:426 115:427 116:428 117:429 118:430 119:431 120:432 121:433 122:434 123:435 124:436 125:437 126:438 127:438 128:439 129:440 130:441 131:441 132:441 133:442 134:443 135:444 136:445 137:446 138:447 139:448 140:449 141:450 142:451 143:451 144:452 145:453 146:454 147:454 148:454 149:455 150:456 151:457 152:458 153:459 154:460 155:461 156:462 157:463 158:464 159:464 160:464 161:465 162:466 163:467 164:468 165:469 166:470 167:471 168:472 169:473 170:474 171:475 172:476 173:477 174:477 175:478 176:479 177:480 178:481 179:482 180:483 181:484 182:485 183:486 184:487 185:488 186:489 187:490 188:490 189:491 190:492 191:493 192:494 193:495 194:496 195:497 196:498 197:499 198:500 199:501 200:502 201:503 202:503 203:504 204:505 205:506 206:506 207:506 208:507 209:508 210:509 211:510 212:511 213:512 214:513 215:514 216:515 217:516 218:516 219:517 220:518 221:519 222:520 223:521 224:522 225:523 226:524 227:525 228:526 229:527 230:528 231:529 232:529 233:530 234:531 235:532 236:532 237:532 238:533 239:534 240:535 241:536 242:537 243:538 244:539 245:540 246:541 247:542 248:542 249:543 250:544 251:545 252:545 253:545 254:546 255:547 256:548 257:549 258:550 259:551 260:552 261:553 262:554 263:555 264:555 265:555 266:556 267:557 268:558 269:559 270:560 271:561 272:562 273:563 274:564 275:565 276:566 277:567 278:568 279:569 280:569 281:570 282:571 283:572 284:573 285:574 286:575 287:576 288:577 289:578 290:579 291:580 292:581 293:582 294:583 295:583 296:584 297:585 298:586 299:587 300:588 301:589 302:590 303:591 304:592 305:593 306:594 307:595 308:596 309:597 310:597 311:598 312:599 313:600 314:601 315:602 316:603 317:604 318:605 319:606 320:607 321:608 322:609 323:610 324:611 325:611 326:611 327:611 328:612 329:613 330:614 331:615 332:616 333:617 334:618 335:619 336:620 337:621 338:622 339:623 340:624 341:625 342:625 343:626 344:627 345:628 346:629 347:630 348:631 349:632 350:633 351:634 352:635 353:636 354:637 355:638 356:639 357:639 358:639 359:639 360:640 361:641 362:642 363:643 364:644 365:645 366:646 367:647 368:648 369:649 370:650 371:651 372:651 373:652 374:653 375:654 376:655 377:656 378:657 379:657 380:658 381:659 382:660\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 4341 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 189\n",
            "INFO:tensorflow:end_position: 202\n",
            "INFO:tensorflow:answer: the bottom commodity for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000039\n",
            "INFO:tensorflow:example_index: 7\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by sales [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:446 10:447 11:448 12:449 13:450 14:451 15:451 16:452 17:453 18:454 19:454 20:454 21:455 22:456 23:457 24:458 25:459 26:460 27:461 28:462 29:463 30:464 31:464 32:464 33:465 34:466 35:467 36:468 37:469 38:470 39:471 40:472 41:473 42:474 43:475 44:476 45:477 46:477 47:478 48:479 49:480 50:481 51:482 52:483 53:484 54:485 55:486 56:487 57:488 58:489 59:490 60:490 61:491 62:492 63:493 64:494 65:495 66:496 67:497 68:498 69:499 70:500 71:501 72:502 73:503 74:503 75:504 76:505 77:506 78:506 79:506 80:507 81:508 82:509 83:510 84:511 85:512 86:513 87:514 88:515 89:516 90:516 91:517 92:518 93:519 94:520 95:521 96:522 97:523 98:524 99:525 100:526 101:527 102:528 103:529 104:529 105:530 106:531 107:532 108:532 109:532 110:533 111:534 112:535 113:536 114:537 115:538 116:539 117:540 118:541 119:542 120:542 121:543 122:544 123:545 124:545 125:545 126:546 127:547 128:548 129:549 130:550 131:551 132:552 133:553 134:554 135:555 136:555 137:555 138:556 139:557 140:558 141:559 142:560 143:561 144:562 145:563 146:564 147:565 148:566 149:567 150:568 151:569 152:569 153:570 154:571 155:572 156:573 157:574 158:575 159:576 160:577 161:578 162:579 163:580 164:581 165:582 166:583 167:583 168:584 169:585 170:586 171:587 172:588 173:589 174:590 175:591 176:592 177:593 178:594 179:595 180:596 181:597 182:597 183:598 184:599 185:600 186:601 187:602 188:603 189:604 190:605 191:606 192:607 193:608 194:609 195:610 196:611 197:611 198:611 199:611 200:612 201:613 202:614 203:615 204:616 205:617 206:618 207:619 208:620 209:621 210:622 211:623 212:624 213:625 214:625 215:626 216:627 217:628 218:629 219:630 220:631 221:632 222:633 223:634 224:635 225:636 226:637 227:638 228:639 229:639 230:639 231:639 232:640 233:641 234:642 235:643 236:644 237:645 238:646 239:647 240:648 241:649 242:650 243:651 244:651 245:652 246:653 247:654 248:655 249:656 250:657 251:657 252:658 253:659 254:660 255:661 256:662 257:663 258:664 259:665 260:665 261:666 262:667 263:668 264:669 265:670 266:671 267:671 268:672 269:673 270:674 271:675 272:676 273:677 274:678 275:679 276:679 277:680 278:681 279:682 280:683 281:684 282:685 283:685 284:686 285:687 286:688 287:689 288:690 289:691 290:692 291:693 292:693 293:694 294:695 295:696 296:697 297:698 298:699 299:699 300:700 301:701 302:702 303:703 304:704 305:705 306:706 307:707 308:707 309:707 310:707 311:708 312:709 313:710 314:711 315:712 316:713 317:713 318:714 319:715 320:716 321:717 322:718 323:719 324:720 325:721 326:721 327:722 328:723 329:724 330:725 331:726 332:727 333:727 334:728 335:729 336:730 337:731 338:732 339:733 340:734 341:735 342:735 343:735 344:735 345:736 346:737 347:738 348:739 349:740 350:741 351:741 352:742 353:743 354:744 355:745 356:746 357:747 358:747\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 4341 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 61\n",
            "INFO:tensorflow:end_position: 74\n",
            "INFO:tensorflow:answer: the bottom commodity for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000040\n",
            "INFO:tensorflow:example_index: 8\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of quantity by department [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:19 31:20 32:21 33:22 34:23 35:24 36:25 37:25 38:26 39:27 40:28 41:29 42:30 43:31 44:32 45:33 46:34 47:35 48:36 49:37 50:38 51:38 52:39 53:40 54:41 55:41 56:41 57:42 58:43 59:44 60:45 61:46 62:47 63:48 64:49 65:50 66:51 67:51 68:52 69:53 70:54 71:55 72:56 73:57 74:58 75:59 76:60 77:61 78:62 79:63 80:64 81:64 82:65 83:66 84:67 85:67 86:67 87:68 88:69 89:70 90:71 91:72 92:73 93:74 94:75 95:76 96:77 97:77 98:78 99:79 100:80 101:80 102:80 103:81 104:82 105:83 106:84 107:85 108:86 109:87 110:88 111:89 112:90 113:90 114:90 115:91 116:92 117:93 118:94 119:95 120:96 121:97 122:98 123:99 124:100 125:101 126:102 127:103 128:103 129:104 130:105 131:106 132:107 133:108 134:109 135:110 136:111 137:112 138:113 139:114 140:115 141:116 142:116 143:117 144:118 145:119 146:120 147:121 148:122 149:123 150:124 151:125 152:126 153:127 154:128 155:129 156:129 157:130 158:131 159:132 160:132 161:132 162:133 163:134 164:135 165:136 166:137 167:138 168:139 169:140 170:141 171:142 172:142 173:143 174:144 175:145 176:146 177:147 178:148 179:149 180:150 181:151 182:152 183:153 184:154 185:155 186:155 187:156 188:157 189:158 190:158 191:158 192:159 193:160 194:161 195:162 196:163 197:164 198:165 199:166 200:167 201:168 202:168 203:169 204:170 205:171 206:171 207:171 208:172 209:173 210:174 211:175 212:176 213:177 214:178 215:179 216:180 217:181 218:181 219:181 220:182 221:183 222:184 223:185 224:186 225:187 226:188 227:189 228:190 229:191 230:192 231:193 232:194 233:195 234:195 235:196 236:197 237:198 238:199 239:200 240:201 241:202 242:203 243:204 244:205 245:206 246:207 247:208 248:209 249:209 250:210 251:211 252:212 253:213 254:214 255:215 256:216 257:217 258:218 259:219 260:220 261:221 262:222 263:223 264:223 265:224 266:225 267:226 268:227 269:228 270:229 271:230 272:231 273:232 274:233 275:234 276:235 277:236 278:237 279:237 280:237 281:237 282:238 283:239 284:240 285:241 286:242 287:243 288:244 289:245 290:246 291:247 292:248 293:249 294:250 295:251 296:251 297:252 298:253 299:254 300:255 301:256 302:257 303:258 304:259 305:260 306:261 307:262 308:263 309:264 310:265 311:265 312:265 313:265 314:266 315:267 316:268 317:269 318:270 319:271 320:272 321:273 322:274 323:275 324:276 325:277 326:277 327:278 328:279 329:280 330:281 331:282 332:283 333:283 334:284 335:285 336:286 337:287 338:288 339:289 340:290 341:291 342:291 343:292 344:293 345:294 346:295 347:296 348:297 349:297 350:298 351:299 352:300 353:301 354:302 355:303 356:304 357:305 358:305 359:306 360:307 361:308 362:309 363:310 364:311 365:311 366:312 367:313 368:314 369:315 370:316 371:317 372:318 373:319 374:319 375:320 376:321 377:322 378:323 379:324 380:325 381:325 382:326\n",
            "INFO:tensorflow:token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 11712 2011 2533 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 327\n",
            "INFO:tensorflow:end_position: 342\n",
            "INFO:tensorflow:answer: between jul 2015 and jun 2017 , the total quantity trend was this for department .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000041\n",
            "INFO:tensorflow:example_index: 8\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of quantity by department [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:113 11:114 12:115 13:116 14:116 15:117 16:118 17:119 18:120 19:121 20:122 21:123 22:124 23:125 24:126 25:127 26:128 27:129 28:129 29:130 30:131 31:132 32:132 33:132 34:133 35:134 36:135 37:136 38:137 39:138 40:139 41:140 42:141 43:142 44:142 45:143 46:144 47:145 48:146 49:147 50:148 51:149 52:150 53:151 54:152 55:153 56:154 57:155 58:155 59:156 60:157 61:158 62:158 63:158 64:159 65:160 66:161 67:162 68:163 69:164 70:165 71:166 72:167 73:168 74:168 75:169 76:170 77:171 78:171 79:171 80:172 81:173 82:174 83:175 84:176 85:177 86:178 87:179 88:180 89:181 90:181 91:181 92:182 93:183 94:184 95:185 96:186 97:187 98:188 99:189 100:190 101:191 102:192 103:193 104:194 105:195 106:195 107:196 108:197 109:198 110:199 111:200 112:201 113:202 114:203 115:204 116:205 117:206 118:207 119:208 120:209 121:209 122:210 123:211 124:212 125:213 126:214 127:215 128:216 129:217 130:218 131:219 132:220 133:221 134:222 135:223 136:223 137:224 138:225 139:226 140:227 141:228 142:229 143:230 144:231 145:232 146:233 147:234 148:235 149:236 150:237 151:237 152:237 153:237 154:238 155:239 156:240 157:241 158:242 159:243 160:244 161:245 162:246 163:247 164:248 165:249 166:250 167:251 168:251 169:252 170:253 171:254 172:255 173:256 174:257 175:258 176:259 177:260 178:261 179:262 180:263 181:264 182:265 183:265 184:265 185:265 186:266 187:267 188:268 189:269 190:270 191:271 192:272 193:273 194:274 195:275 196:276 197:277 198:277 199:278 200:279 201:280 202:281 203:282 204:283 205:283 206:284 207:285 208:286 209:287 210:288 211:289 212:290 213:291 214:291 215:292 216:293 217:294 218:295 219:296 220:297 221:297 222:298 223:299 224:300 225:301 226:302 227:303 228:304 229:305 230:305 231:306 232:307 233:308 234:309 235:310 236:311 237:311 238:312 239:313 240:314 241:315 242:316 243:317 244:318 245:319 246:319 247:320 248:321 249:322 250:323 251:324 252:325 253:325 254:326 255:327 256:328 257:329 258:330 259:331 260:332 261:333 262:333 263:333 264:333 265:334 266:335 267:336 268:337 269:338 270:339 271:339 272:340 273:341 274:342 275:343 276:344 277:345 278:346 279:347 280:347 281:348 282:349 283:350 284:351 285:352 286:353 287:353 288:354 289:355 290:356 291:357 292:358 293:359 294:360 295:361 296:361 297:361 298:361 299:362 300:363 301:364 302:365 303:366 304:367 305:367 306:368 307:369 308:370 309:371 310:372 311:373 312:373 313:374 314:375 315:376 316:377 317:378 318:379 319:380 320:381 321:382 322:383 323:384 324:385 325:386 326:386 327:387 328:388 329:389 330:390 331:391 332:392 333:393 334:394 335:395 336:396 337:397 338:398 339:399 340:399 341:400 342:401 343:402 344:403 345:404 346:405 347:406 348:407 349:408 350:409 351:410 352:411 353:412 354:412 355:413 356:414 357:415 358:415 359:415 360:416 361:417 362:418 363:419 364:420 365:421 366:422 367:423 368:424 369:425 370:425 371:426 372:427 373:428 374:429 375:430 376:431 377:432 378:433 379:434 380:435 381:436 382:437\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 11712 2011 2533 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 199\n",
            "INFO:tensorflow:end_position: 214\n",
            "INFO:tensorflow:answer: between jul 2015 and jun 2017 , the total quantity trend was this for department .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000042\n",
            "INFO:tensorflow:example_index: 8\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of quantity by department [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:225 11:226 12:227 13:228 14:229 15:230 16:231 17:232 18:233 19:234 20:235 21:236 22:237 23:237 24:237 25:237 26:238 27:239 28:240 29:241 30:242 31:243 32:244 33:245 34:246 35:247 36:248 37:249 38:250 39:251 40:251 41:252 42:253 43:254 44:255 45:256 46:257 47:258 48:259 49:260 50:261 51:262 52:263 53:264 54:265 55:265 56:265 57:265 58:266 59:267 60:268 61:269 62:270 63:271 64:272 65:273 66:274 67:275 68:276 69:277 70:277 71:278 72:279 73:280 74:281 75:282 76:283 77:283 78:284 79:285 80:286 81:287 82:288 83:289 84:290 85:291 86:291 87:292 88:293 89:294 90:295 91:296 92:297 93:297 94:298 95:299 96:300 97:301 98:302 99:303 100:304 101:305 102:305 103:306 104:307 105:308 106:309 107:310 108:311 109:311 110:312 111:313 112:314 113:315 114:316 115:317 116:318 117:319 118:319 119:320 120:321 121:322 122:323 123:324 124:325 125:325 126:326 127:327 128:328 129:329 130:330 131:331 132:332 133:333 134:333 135:333 136:333 137:334 138:335 139:336 140:337 141:338 142:339 143:339 144:340 145:341 146:342 147:343 148:344 149:345 150:346 151:347 152:347 153:348 154:349 155:350 156:351 157:352 158:353 159:353 160:354 161:355 162:356 163:357 164:358 165:359 166:360 167:361 168:361 169:361 170:361 171:362 172:363 173:364 174:365 175:366 176:367 177:367 178:368 179:369 180:370 181:371 182:372 183:373 184:373 185:374 186:375 187:376 188:377 189:378 190:379 191:380 192:381 193:382 194:383 195:384 196:385 197:386 198:386 199:387 200:388 201:389 202:390 203:391 204:392 205:393 206:394 207:395 208:396 209:397 210:398 211:399 212:399 213:400 214:401 215:402 216:403 217:404 218:405 219:406 220:407 221:408 222:409 223:410 224:411 225:412 226:412 227:413 228:414 229:415 230:415 231:415 232:416 233:417 234:418 235:419 236:420 237:421 238:422 239:423 240:424 241:425 242:425 243:426 244:427 245:428 246:429 247:430 248:431 249:432 250:433 251:434 252:435 253:436 254:437 255:438 256:438 257:439 258:440 259:441 260:441 261:441 262:442 263:443 264:444 265:445 266:446 267:447 268:448 269:449 270:450 271:451 272:451 273:452 274:453 275:454 276:454 277:454 278:455 279:456 280:457 281:458 282:459 283:460 284:461 285:462 286:463 287:464 288:464 289:464 290:465 291:466 292:467 293:468 294:469 295:470 296:471 297:472 298:473 299:474 300:475 301:476 302:477 303:477 304:478 305:479 306:480 307:481 308:482 309:483 310:484 311:485 312:486 313:487 314:488 315:489 316:490 317:490 318:491 319:492 320:493 321:494 322:495 323:496 324:497 325:498 326:499 327:500 328:501 329:502 330:503 331:503 332:504 333:505 334:506 335:506 336:506 337:507 338:508 339:509 340:510 341:511 342:512 343:513 344:514 345:515 346:516 347:516 348:517 349:518 350:519 351:520 352:521 353:522 354:523 355:524 356:525 357:526 358:527 359:528 360:529 361:529 362:530 363:531 364:532 365:532 366:532 367:533 368:534 369:535 370:536 371:537 372:538 373:539 374:540 375:541 376:542 377:542 378:543 379:544 380:545 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 11712 2011 2533 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 71\n",
            "INFO:tensorflow:end_position: 86\n",
            "INFO:tensorflow:answer: between jul 2015 and jun 2017 , the total quantity trend was this for department .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000043\n",
            "INFO:tensorflow:example_index: 8\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of quantity by department [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:335 11:336 12:337 13:338 14:339 15:339 16:340 17:341 18:342 19:343 20:344 21:345 22:346 23:347 24:347 25:348 26:349 27:350 28:351 29:352 30:353 31:353 32:354 33:355 34:356 35:357 36:358 37:359 38:360 39:361 40:361 41:361 42:361 43:362 44:363 45:364 46:365 47:366 48:367 49:367 50:368 51:369 52:370 53:371 54:372 55:373 56:373 57:374 58:375 59:376 60:377 61:378 62:379 63:380 64:381 65:382 66:383 67:384 68:385 69:386 70:386 71:387 72:388 73:389 74:390 75:391 76:392 77:393 78:394 79:395 80:396 81:397 82:398 83:399 84:399 85:400 86:401 87:402 88:403 89:404 90:405 91:406 92:407 93:408 94:409 95:410 96:411 97:412 98:412 99:413 100:414 101:415 102:415 103:415 104:416 105:417 106:418 107:419 108:420 109:421 110:422 111:423 112:424 113:425 114:425 115:426 116:427 117:428 118:429 119:430 120:431 121:432 122:433 123:434 124:435 125:436 126:437 127:438 128:438 129:439 130:440 131:441 132:441 133:441 134:442 135:443 136:444 137:445 138:446 139:447 140:448 141:449 142:450 143:451 144:451 145:452 146:453 147:454 148:454 149:454 150:455 151:456 152:457 153:458 154:459 155:460 156:461 157:462 158:463 159:464 160:464 161:464 162:465 163:466 164:467 165:468 166:469 167:470 168:471 169:472 170:473 171:474 172:475 173:476 174:477 175:477 176:478 177:479 178:480 179:481 180:482 181:483 182:484 183:485 184:486 185:487 186:488 187:489 188:490 189:490 190:491 191:492 192:493 193:494 194:495 195:496 196:497 197:498 198:499 199:500 200:501 201:502 202:503 203:503 204:504 205:505 206:506 207:506 208:506 209:507 210:508 211:509 212:510 213:511 214:512 215:513 216:514 217:515 218:516 219:516 220:517 221:518 222:519 223:520 224:521 225:522 226:523 227:524 228:525 229:526 230:527 231:528 232:529 233:529 234:530 235:531 236:532 237:532 238:532 239:533 240:534 241:535 242:536 243:537 244:538 245:539 246:540 247:541 248:542 249:542 250:543 251:544 252:545 253:545 254:545 255:546 256:547 257:548 258:549 259:550 260:551 261:552 262:553 263:554 264:555 265:555 266:555 267:556 268:557 269:558 270:559 271:560 272:561 273:562 274:563 275:564 276:565 277:566 278:567 279:568 280:569 281:569 282:570 283:571 284:572 285:573 286:574 287:575 288:576 289:577 290:578 291:579 292:580 293:581 294:582 295:583 296:583 297:584 298:585 299:586 300:587 301:588 302:589 303:590 304:591 305:592 306:593 307:594 308:595 309:596 310:597 311:597 312:598 313:599 314:600 315:601 316:602 317:603 318:604 319:605 320:606 321:607 322:608 323:609 324:610 325:611 326:611 327:611 328:611 329:612 330:613 331:614 332:615 333:616 334:617 335:618 336:619 337:620 338:621 339:622 340:623 341:624 342:625 343:625 344:626 345:627 346:628 347:629 348:630 349:631 350:632 351:633 352:634 353:635 354:636 355:637 356:638 357:639 358:639 359:639 360:639 361:640 362:641 363:642 364:643 365:644 366:645 367:646 368:647 369:648 370:649 371:650 372:651 373:651 374:652 375:653 376:654 377:655 378:656 379:657 380:657 381:658 382:659\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 11712 2011 2533 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000044\n",
            "INFO:tensorflow:example_index: 8\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of quantity by department [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:446 11:447 12:448 13:449 14:450 15:451 16:451 17:452 18:453 19:454 20:454 21:454 22:455 23:456 24:457 25:458 26:459 27:460 28:461 29:462 30:463 31:464 32:464 33:464 34:465 35:466 36:467 37:468 38:469 39:470 40:471 41:472 42:473 43:474 44:475 45:476 46:477 47:477 48:478 49:479 50:480 51:481 52:482 53:483 54:484 55:485 56:486 57:487 58:488 59:489 60:490 61:490 62:491 63:492 64:493 65:494 66:495 67:496 68:497 69:498 70:499 71:500 72:501 73:502 74:503 75:503 76:504 77:505 78:506 79:506 80:506 81:507 82:508 83:509 84:510 85:511 86:512 87:513 88:514 89:515 90:516 91:516 92:517 93:518 94:519 95:520 96:521 97:522 98:523 99:524 100:525 101:526 102:527 103:528 104:529 105:529 106:530 107:531 108:532 109:532 110:532 111:533 112:534 113:535 114:536 115:537 116:538 117:539 118:540 119:541 120:542 121:542 122:543 123:544 124:545 125:545 126:545 127:546 128:547 129:548 130:549 131:550 132:551 133:552 134:553 135:554 136:555 137:555 138:555 139:556 140:557 141:558 142:559 143:560 144:561 145:562 146:563 147:564 148:565 149:566 150:567 151:568 152:569 153:569 154:570 155:571 156:572 157:573 158:574 159:575 160:576 161:577 162:578 163:579 164:580 165:581 166:582 167:583 168:583 169:584 170:585 171:586 172:587 173:588 174:589 175:590 176:591 177:592 178:593 179:594 180:595 181:596 182:597 183:597 184:598 185:599 186:600 187:601 188:602 189:603 190:604 191:605 192:606 193:607 194:608 195:609 196:610 197:611 198:611 199:611 200:611 201:612 202:613 203:614 204:615 205:616 206:617 207:618 208:619 209:620 210:621 211:622 212:623 213:624 214:625 215:625 216:626 217:627 218:628 219:629 220:630 221:631 222:632 223:633 224:634 225:635 226:636 227:637 228:638 229:639 230:639 231:639 232:639 233:640 234:641 235:642 236:643 237:644 238:645 239:646 240:647 241:648 242:649 243:650 244:651 245:651 246:652 247:653 248:654 249:655 250:656 251:657 252:657 253:658 254:659 255:660 256:661 257:662 258:663 259:664 260:665 261:665 262:666 263:667 264:668 265:669 266:670 267:671 268:671 269:672 270:673 271:674 272:675 273:676 274:677 275:678 276:679 277:679 278:680 279:681 280:682 281:683 282:684 283:685 284:685 285:686 286:687 287:688 288:689 289:690 290:691 291:692 292:693 293:693 294:694 295:695 296:696 297:697 298:698 299:699 300:699 301:700 302:701 303:702 304:703 305:704 306:705 307:706 308:707 309:707 310:707 311:707 312:708 313:709 314:710 315:711 316:712 317:713 318:713 319:714 320:715 321:716 322:717 323:718 324:719 325:720 326:721 327:721 328:722 329:723 330:724 331:725 332:726 333:727 334:727 335:728 336:729 337:730 338:731 339:732 340:733 341:734 342:735 343:735 344:735 345:735 346:736 347:737 348:738 349:739 350:740 351:741 352:741 353:742 354:743 355:744 356:745 357:746 358:747 359:747\n",
            "INFO:tensorflow:token_is_max_context: 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 11712 2011 2533 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000045\n",
            "INFO:tensorflow:example_index: 9\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the top loyalty by sales [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:19 30:20 31:21 32:22 33:23 34:24 35:25 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:33 45:34 46:35 47:36 48:37 49:38 50:38 51:39 52:40 53:41 54:41 55:41 56:42 57:43 58:44 59:45 60:46 61:47 62:48 63:49 64:50 65:51 66:51 67:52 68:53 69:54 70:55 71:56 72:57 73:58 74:59 75:60 76:61 77:62 78:63 79:64 80:64 81:65 82:66 83:67 84:67 85:67 86:68 87:69 88:70 89:71 90:72 91:73 92:74 93:75 94:76 95:77 96:77 97:78 98:79 99:80 100:80 101:80 102:81 103:82 104:83 105:84 106:85 107:86 108:87 109:88 110:89 111:90 112:90 113:90 114:91 115:92 116:93 117:94 118:95 119:96 120:97 121:98 122:99 123:100 124:101 125:102 126:103 127:103 128:104 129:105 130:106 131:107 132:108 133:109 134:110 135:111 136:112 137:113 138:114 139:115 140:116 141:116 142:117 143:118 144:119 145:120 146:121 147:122 148:123 149:124 150:125 151:126 152:127 153:128 154:129 155:129 156:130 157:131 158:132 159:132 160:132 161:133 162:134 163:135 164:136 165:137 166:138 167:139 168:140 169:141 170:142 171:142 172:143 173:144 174:145 175:146 176:147 177:148 178:149 179:150 180:151 181:152 182:153 183:154 184:155 185:155 186:156 187:157 188:158 189:158 190:158 191:159 192:160 193:161 194:162 195:163 196:164 197:165 198:166 199:167 200:168 201:168 202:169 203:170 204:171 205:171 206:171 207:172 208:173 209:174 210:175 211:176 212:177 213:178 214:179 215:180 216:181 217:181 218:181 219:182 220:183 221:184 222:185 223:186 224:187 225:188 226:189 227:190 228:191 229:192 230:193 231:194 232:195 233:195 234:196 235:197 236:198 237:199 238:200 239:201 240:202 241:203 242:204 243:205 244:206 245:207 246:208 247:209 248:209 249:210 250:211 251:212 252:213 253:214 254:215 255:216 256:217 257:218 258:219 259:220 260:221 261:222 262:223 263:223 264:224 265:225 266:226 267:227 268:228 269:229 270:230 271:231 272:232 273:233 274:234 275:235 276:236 277:237 278:237 279:237 280:237 281:238 282:239 283:240 284:241 285:242 286:243 287:244 288:245 289:246 290:247 291:248 292:249 293:250 294:251 295:251 296:252 297:253 298:254 299:255 300:256 301:257 302:258 303:259 304:260 305:261 306:262 307:263 308:264 309:265 310:265 311:265 312:265 313:266 314:267 315:268 316:269 317:270 318:271 319:272 320:273 321:274 322:275 323:276 324:277 325:277 326:278 327:279 328:280 329:281 330:282 331:283 332:283 333:284 334:285 335:286 336:287 337:288 338:289 339:290 340:291 341:291 342:292 343:293 344:294 345:295 346:296 347:297 348:297 349:298 350:299 351:300 352:301 353:302 354:303 355:304 356:305 357:305 358:306 359:307 360:308 361:309 362:310 363:311 364:311 365:312 366:313 367:314 368:315 369:316 370:317 371:318 372:319 373:319 374:320 375:321 376:322 377:323 378:324 379:325 380:325 381:326 382:327\n",
            "INFO:tensorflow:token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 9721 2011 4341 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000046\n",
            "INFO:tensorflow:example_index: 9\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the top loyalty by sales [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:113 10:114 11:115 12:116 13:116 14:117 15:118 16:119 17:120 18:121 19:122 20:123 21:124 22:125 23:126 24:127 25:128 26:129 27:129 28:130 29:131 30:132 31:132 32:132 33:133 34:134 35:135 36:136 37:137 38:138 39:139 40:140 41:141 42:142 43:142 44:143 45:144 46:145 47:146 48:147 49:148 50:149 51:150 52:151 53:152 54:153 55:154 56:155 57:155 58:156 59:157 60:158 61:158 62:158 63:159 64:160 65:161 66:162 67:163 68:164 69:165 70:166 71:167 72:168 73:168 74:169 75:170 76:171 77:171 78:171 79:172 80:173 81:174 82:175 83:176 84:177 85:178 86:179 87:180 88:181 89:181 90:181 91:182 92:183 93:184 94:185 95:186 96:187 97:188 98:189 99:190 100:191 101:192 102:193 103:194 104:195 105:195 106:196 107:197 108:198 109:199 110:200 111:201 112:202 113:203 114:204 115:205 116:206 117:207 118:208 119:209 120:209 121:210 122:211 123:212 124:213 125:214 126:215 127:216 128:217 129:218 130:219 131:220 132:221 133:222 134:223 135:223 136:224 137:225 138:226 139:227 140:228 141:229 142:230 143:231 144:232 145:233 146:234 147:235 148:236 149:237 150:237 151:237 152:237 153:238 154:239 155:240 156:241 157:242 158:243 159:244 160:245 161:246 162:247 163:248 164:249 165:250 166:251 167:251 168:252 169:253 170:254 171:255 172:256 173:257 174:258 175:259 176:260 177:261 178:262 179:263 180:264 181:265 182:265 183:265 184:265 185:266 186:267 187:268 188:269 189:270 190:271 191:272 192:273 193:274 194:275 195:276 196:277 197:277 198:278 199:279 200:280 201:281 202:282 203:283 204:283 205:284 206:285 207:286 208:287 209:288 210:289 211:290 212:291 213:291 214:292 215:293 216:294 217:295 218:296 219:297 220:297 221:298 222:299 223:300 224:301 225:302 226:303 227:304 228:305 229:305 230:306 231:307 232:308 233:309 234:310 235:311 236:311 237:312 238:313 239:314 240:315 241:316 242:317 243:318 244:319 245:319 246:320 247:321 248:322 249:323 250:324 251:325 252:325 253:326 254:327 255:328 256:329 257:330 258:331 259:332 260:333 261:333 262:333 263:333 264:334 265:335 266:336 267:337 268:338 269:339 270:339 271:340 272:341 273:342 274:343 275:344 276:345 277:346 278:347 279:347 280:348 281:349 282:350 283:351 284:352 285:353 286:353 287:354 288:355 289:356 290:357 291:358 292:359 293:360 294:361 295:361 296:361 297:361 298:362 299:363 300:364 301:365 302:366 303:367 304:367 305:368 306:369 307:370 308:371 309:372 310:373 311:373 312:374 313:375 314:376 315:377 316:378 317:379 318:380 319:381 320:382 321:383 322:384 323:385 324:386 325:386 326:387 327:388 328:389 329:390 330:391 331:392 332:393 333:394 334:395 335:396 336:397 337:398 338:399 339:399 340:400 341:401 342:402 343:403 344:404 345:405 346:406 347:407 348:408 349:409 350:410 351:411 352:412 353:412 354:413 355:414 356:415 357:415 358:415 359:416 360:417 361:418 362:419 363:420 364:421 365:422 366:423 367:424 368:425 369:425 370:426 371:427 372:428 373:429 374:430 375:431 376:432 377:433 378:434 379:435 380:436 381:437 382:438\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 9721 2011 4341 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000047\n",
            "INFO:tensorflow:example_index: 9\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the top loyalty by sales [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:225 10:226 11:227 12:228 13:229 14:230 15:231 16:232 17:233 18:234 19:235 20:236 21:237 22:237 23:237 24:237 25:238 26:239 27:240 28:241 29:242 30:243 31:244 32:245 33:246 34:247 35:248 36:249 37:250 38:251 39:251 40:252 41:253 42:254 43:255 44:256 45:257 46:258 47:259 48:260 49:261 50:262 51:263 52:264 53:265 54:265 55:265 56:265 57:266 58:267 59:268 60:269 61:270 62:271 63:272 64:273 65:274 66:275 67:276 68:277 69:277 70:278 71:279 72:280 73:281 74:282 75:283 76:283 77:284 78:285 79:286 80:287 81:288 82:289 83:290 84:291 85:291 86:292 87:293 88:294 89:295 90:296 91:297 92:297 93:298 94:299 95:300 96:301 97:302 98:303 99:304 100:305 101:305 102:306 103:307 104:308 105:309 106:310 107:311 108:311 109:312 110:313 111:314 112:315 113:316 114:317 115:318 116:319 117:319 118:320 119:321 120:322 121:323 122:324 123:325 124:325 125:326 126:327 127:328 128:329 129:330 130:331 131:332 132:333 133:333 134:333 135:333 136:334 137:335 138:336 139:337 140:338 141:339 142:339 143:340 144:341 145:342 146:343 147:344 148:345 149:346 150:347 151:347 152:348 153:349 154:350 155:351 156:352 157:353 158:353 159:354 160:355 161:356 162:357 163:358 164:359 165:360 166:361 167:361 168:361 169:361 170:362 171:363 172:364 173:365 174:366 175:367 176:367 177:368 178:369 179:370 180:371 181:372 182:373 183:373 184:374 185:375 186:376 187:377 188:378 189:379 190:380 191:381 192:382 193:383 194:384 195:385 196:386 197:386 198:387 199:388 200:389 201:390 202:391 203:392 204:393 205:394 206:395 207:396 208:397 209:398 210:399 211:399 212:400 213:401 214:402 215:403 216:404 217:405 218:406 219:407 220:408 221:409 222:410 223:411 224:412 225:412 226:413 227:414 228:415 229:415 230:415 231:416 232:417 233:418 234:419 235:420 236:421 237:422 238:423 239:424 240:425 241:425 242:426 243:427 244:428 245:429 246:430 247:431 248:432 249:433 250:434 251:435 252:436 253:437 254:438 255:438 256:439 257:440 258:441 259:441 260:441 261:442 262:443 263:444 264:445 265:446 266:447 267:448 268:449 269:450 270:451 271:451 272:452 273:453 274:454 275:454 276:454 277:455 278:456 279:457 280:458 281:459 282:460 283:461 284:462 285:463 286:464 287:464 288:464 289:465 290:466 291:467 292:468 293:469 294:470 295:471 296:472 297:473 298:474 299:475 300:476 301:477 302:477 303:478 304:479 305:480 306:481 307:482 308:483 309:484 310:485 311:486 312:487 313:488 314:489 315:490 316:490 317:491 318:492 319:493 320:494 321:495 322:496 323:497 324:498 325:499 326:500 327:501 328:502 329:503 330:503 331:504 332:505 333:506 334:506 335:506 336:507 337:508 338:509 339:510 340:511 341:512 342:513 343:514 344:515 345:516 346:516 347:517 348:518 349:519 350:520 351:521 352:522 353:523 354:524 355:525 356:526 357:527 358:528 359:529 360:529 361:530 362:531 363:532 364:532 365:532 366:533 367:534 368:535 369:536 370:537 371:538 372:539 373:540 374:541 375:542 376:542 377:543 378:544 379:545 380:545 381:545 382:546\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 9721 2011 4341 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 242\n",
            "INFO:tensorflow:end_position: 255\n",
            "INFO:tensorflow:answer: the top loyalty for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000048\n",
            "INFO:tensorflow:example_index: 9\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the top loyalty by sales [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:335 10:336 11:337 12:338 13:339 14:339 15:340 16:341 17:342 18:343 19:344 20:345 21:346 22:347 23:347 24:348 25:349 26:350 27:351 28:352 29:353 30:353 31:354 32:355 33:356 34:357 35:358 36:359 37:360 38:361 39:361 40:361 41:361 42:362 43:363 44:364 45:365 46:366 47:367 48:367 49:368 50:369 51:370 52:371 53:372 54:373 55:373 56:374 57:375 58:376 59:377 60:378 61:379 62:380 63:381 64:382 65:383 66:384 67:385 68:386 69:386 70:387 71:388 72:389 73:390 74:391 75:392 76:393 77:394 78:395 79:396 80:397 81:398 82:399 83:399 84:400 85:401 86:402 87:403 88:404 89:405 90:406 91:407 92:408 93:409 94:410 95:411 96:412 97:412 98:413 99:414 100:415 101:415 102:415 103:416 104:417 105:418 106:419 107:420 108:421 109:422 110:423 111:424 112:425 113:425 114:426 115:427 116:428 117:429 118:430 119:431 120:432 121:433 122:434 123:435 124:436 125:437 126:438 127:438 128:439 129:440 130:441 131:441 132:441 133:442 134:443 135:444 136:445 137:446 138:447 139:448 140:449 141:450 142:451 143:451 144:452 145:453 146:454 147:454 148:454 149:455 150:456 151:457 152:458 153:459 154:460 155:461 156:462 157:463 158:464 159:464 160:464 161:465 162:466 163:467 164:468 165:469 166:470 167:471 168:472 169:473 170:474 171:475 172:476 173:477 174:477 175:478 176:479 177:480 178:481 179:482 180:483 181:484 182:485 183:486 184:487 185:488 186:489 187:490 188:490 189:491 190:492 191:493 192:494 193:495 194:496 195:497 196:498 197:499 198:500 199:501 200:502 201:503 202:503 203:504 204:505 205:506 206:506 207:506 208:507 209:508 210:509 211:510 212:511 213:512 214:513 215:514 216:515 217:516 218:516 219:517 220:518 221:519 222:520 223:521 224:522 225:523 226:524 227:525 228:526 229:527 230:528 231:529 232:529 233:530 234:531 235:532 236:532 237:532 238:533 239:534 240:535 241:536 242:537 243:538 244:539 245:540 246:541 247:542 248:542 249:543 250:544 251:545 252:545 253:545 254:546 255:547 256:548 257:549 258:550 259:551 260:552 261:553 262:554 263:555 264:555 265:555 266:556 267:557 268:558 269:559 270:560 271:561 272:562 273:563 274:564 275:565 276:566 277:567 278:568 279:569 280:569 281:570 282:571 283:572 284:573 285:574 286:575 287:576 288:577 289:578 290:579 291:580 292:581 293:582 294:583 295:583 296:584 297:585 298:586 299:587 300:588 301:589 302:590 303:591 304:592 305:593 306:594 307:595 308:596 309:597 310:597 311:598 312:599 313:600 314:601 315:602 316:603 317:604 318:605 319:606 320:607 321:608 322:609 323:610 324:611 325:611 326:611 327:611 328:612 329:613 330:614 331:615 332:616 333:617 334:618 335:619 336:620 337:621 338:622 339:623 340:624 341:625 342:625 343:626 344:627 345:628 346:629 347:630 348:631 349:632 350:633 351:634 352:635 353:636 354:637 355:638 356:639 357:639 358:639 359:639 360:640 361:641 362:642 363:643 364:644 365:645 366:646 367:647 368:648 369:649 370:650 371:651 372:651 373:652 374:653 375:654 376:655 377:656 378:657 379:657 380:658 381:659 382:660\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 9721 2011 4341 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 114\n",
            "INFO:tensorflow:end_position: 127\n",
            "INFO:tensorflow:answer: the top loyalty for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000049\n",
            "INFO:tensorflow:example_index: 9\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the top loyalty by sales [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:446 10:447 11:448 12:449 13:450 14:451 15:451 16:452 17:453 18:454 19:454 20:454 21:455 22:456 23:457 24:458 25:459 26:460 27:461 28:462 29:463 30:464 31:464 32:464 33:465 34:466 35:467 36:468 37:469 38:470 39:471 40:472 41:473 42:474 43:475 44:476 45:477 46:477 47:478 48:479 49:480 50:481 51:482 52:483 53:484 54:485 55:486 56:487 57:488 58:489 59:490 60:490 61:491 62:492 63:493 64:494 65:495 66:496 67:497 68:498 69:499 70:500 71:501 72:502 73:503 74:503 75:504 76:505 77:506 78:506 79:506 80:507 81:508 82:509 83:510 84:511 85:512 86:513 87:514 88:515 89:516 90:516 91:517 92:518 93:519 94:520 95:521 96:522 97:523 98:524 99:525 100:526 101:527 102:528 103:529 104:529 105:530 106:531 107:532 108:532 109:532 110:533 111:534 112:535 113:536 114:537 115:538 116:539 117:540 118:541 119:542 120:542 121:543 122:544 123:545 124:545 125:545 126:546 127:547 128:548 129:549 130:550 131:551 132:552 133:553 134:554 135:555 136:555 137:555 138:556 139:557 140:558 141:559 142:560 143:561 144:562 145:563 146:564 147:565 148:566 149:567 150:568 151:569 152:569 153:570 154:571 155:572 156:573 157:574 158:575 159:576 160:577 161:578 162:579 163:580 164:581 165:582 166:583 167:583 168:584 169:585 170:586 171:587 172:588 173:589 174:590 175:591 176:592 177:593 178:594 179:595 180:596 181:597 182:597 183:598 184:599 185:600 186:601 187:602 188:603 189:604 190:605 191:606 192:607 193:608 194:609 195:610 196:611 197:611 198:611 199:611 200:612 201:613 202:614 203:615 204:616 205:617 206:618 207:619 208:620 209:621 210:622 211:623 212:624 213:625 214:625 215:626 216:627 217:628 218:629 219:630 220:631 221:632 222:633 223:634 224:635 225:636 226:637 227:638 228:639 229:639 230:639 231:639 232:640 233:641 234:642 235:643 236:644 237:645 238:646 239:647 240:648 241:649 242:650 243:651 244:651 245:652 246:653 247:654 248:655 249:656 250:657 251:657 252:658 253:659 254:660 255:661 256:662 257:663 258:664 259:665 260:665 261:666 262:667 263:668 264:669 265:670 266:671 267:671 268:672 269:673 270:674 271:675 272:676 273:677 274:678 275:679 276:679 277:680 278:681 279:682 280:683 281:684 282:685 283:685 284:686 285:687 286:688 287:689 288:690 289:691 290:692 291:693 292:693 293:694 294:695 295:696 296:697 297:698 298:699 299:699 300:700 301:701 302:702 303:703 304:704 305:705 306:706 307:707 308:707 309:707 310:707 311:708 312:709 313:710 314:711 315:712 316:713 317:713 318:714 319:715 320:716 321:717 322:718 323:719 324:720 325:721 326:721 327:722 328:723 329:724 330:725 331:726 332:727 333:727 334:728 335:729 336:730 337:731 338:732 339:733 340:734 341:735 342:735 343:735 344:735 345:736 346:737 347:738 348:739 349:740 350:741 351:741 352:742 353:743 354:744 355:745 356:746 357:747 358:747\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 9721 2011 4341 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000050\n",
            "INFO:tensorflow:example_index: 10\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the top brand by quantity [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:19 30:20 31:21 32:22 33:23 34:24 35:25 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:33 45:34 46:35 47:36 48:37 49:38 50:38 51:39 52:40 53:41 54:41 55:41 56:42 57:43 58:44 59:45 60:46 61:47 62:48 63:49 64:50 65:51 66:51 67:52 68:53 69:54 70:55 71:56 72:57 73:58 74:59 75:60 76:61 77:62 78:63 79:64 80:64 81:65 82:66 83:67 84:67 85:67 86:68 87:69 88:70 89:71 90:72 91:73 92:74 93:75 94:76 95:77 96:77 97:78 98:79 99:80 100:80 101:80 102:81 103:82 104:83 105:84 106:85 107:86 108:87 109:88 110:89 111:90 112:90 113:90 114:91 115:92 116:93 117:94 118:95 119:96 120:97 121:98 122:99 123:100 124:101 125:102 126:103 127:103 128:104 129:105 130:106 131:107 132:108 133:109 134:110 135:111 136:112 137:113 138:114 139:115 140:116 141:116 142:117 143:118 144:119 145:120 146:121 147:122 148:123 149:124 150:125 151:126 152:127 153:128 154:129 155:129 156:130 157:131 158:132 159:132 160:132 161:133 162:134 163:135 164:136 165:137 166:138 167:139 168:140 169:141 170:142 171:142 172:143 173:144 174:145 175:146 176:147 177:148 178:149 179:150 180:151 181:152 182:153 183:154 184:155 185:155 186:156 187:157 188:158 189:158 190:158 191:159 192:160 193:161 194:162 195:163 196:164 197:165 198:166 199:167 200:168 201:168 202:169 203:170 204:171 205:171 206:171 207:172 208:173 209:174 210:175 211:176 212:177 213:178 214:179 215:180 216:181 217:181 218:181 219:182 220:183 221:184 222:185 223:186 224:187 225:188 226:189 227:190 228:191 229:192 230:193 231:194 232:195 233:195 234:196 235:197 236:198 237:199 238:200 239:201 240:202 241:203 242:204 243:205 244:206 245:207 246:208 247:209 248:209 249:210 250:211 251:212 252:213 253:214 254:215 255:216 256:217 257:218 258:219 259:220 260:221 261:222 262:223 263:223 264:224 265:225 266:226 267:227 268:228 269:229 270:230 271:231 272:232 273:233 274:234 275:235 276:236 277:237 278:237 279:237 280:237 281:238 282:239 283:240 284:241 285:242 286:243 287:244 288:245 289:246 290:247 291:248 292:249 293:250 294:251 295:251 296:252 297:253 298:254 299:255 300:256 301:257 302:258 303:259 304:260 305:261 306:262 307:263 308:264 309:265 310:265 311:265 312:265 313:266 314:267 315:268 316:269 317:270 318:271 319:272 320:273 321:274 322:275 323:276 324:277 325:277 326:278 327:279 328:280 329:281 330:282 331:283 332:283 333:284 334:285 335:286 336:287 337:288 338:289 339:290 340:291 341:291 342:292 343:293 344:294 345:295 346:296 347:297 348:297 349:298 350:299 351:300 352:301 353:302 354:303 355:304 356:305 357:305 358:306 359:307 360:308 361:309 362:310 363:311 364:311 365:312 366:313 367:314 368:315 369:316 370:317 371:318 372:319 373:319 374:320 375:321 376:322 377:323 378:324 379:325 380:325 381:326 382:327\n",
            "INFO:tensorflow:token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4435 2011 11712 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 23\n",
            "INFO:tensorflow:end_position: 36\n",
            "INFO:tensorflow:answer: the top brand for quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000051\n",
            "INFO:tensorflow:example_index: 10\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the top brand by quantity [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:113 10:114 11:115 12:116 13:116 14:117 15:118 16:119 17:120 18:121 19:122 20:123 21:124 22:125 23:126 24:127 25:128 26:129 27:129 28:130 29:131 30:132 31:132 32:132 33:133 34:134 35:135 36:136 37:137 38:138 39:139 40:140 41:141 42:142 43:142 44:143 45:144 46:145 47:146 48:147 49:148 50:149 51:150 52:151 53:152 54:153 55:154 56:155 57:155 58:156 59:157 60:158 61:158 62:158 63:159 64:160 65:161 66:162 67:163 68:164 69:165 70:166 71:167 72:168 73:168 74:169 75:170 76:171 77:171 78:171 79:172 80:173 81:174 82:175 83:176 84:177 85:178 86:179 87:180 88:181 89:181 90:181 91:182 92:183 93:184 94:185 95:186 96:187 97:188 98:189 99:190 100:191 101:192 102:193 103:194 104:195 105:195 106:196 107:197 108:198 109:199 110:200 111:201 112:202 113:203 114:204 115:205 116:206 117:207 118:208 119:209 120:209 121:210 122:211 123:212 124:213 125:214 126:215 127:216 128:217 129:218 130:219 131:220 132:221 133:222 134:223 135:223 136:224 137:225 138:226 139:227 140:228 141:229 142:230 143:231 144:232 145:233 146:234 147:235 148:236 149:237 150:237 151:237 152:237 153:238 154:239 155:240 156:241 157:242 158:243 159:244 160:245 161:246 162:247 163:248 164:249 165:250 166:251 167:251 168:252 169:253 170:254 171:255 172:256 173:257 174:258 175:259 176:260 177:261 178:262 179:263 180:264 181:265 182:265 183:265 184:265 185:266 186:267 187:268 188:269 189:270 190:271 191:272 192:273 193:274 194:275 195:276 196:277 197:277 198:278 199:279 200:280 201:281 202:282 203:283 204:283 205:284 206:285 207:286 208:287 209:288 210:289 211:290 212:291 213:291 214:292 215:293 216:294 217:295 218:296 219:297 220:297 221:298 222:299 223:300 224:301 225:302 226:303 227:304 228:305 229:305 230:306 231:307 232:308 233:309 234:310 235:311 236:311 237:312 238:313 239:314 240:315 241:316 242:317 243:318 244:319 245:319 246:320 247:321 248:322 249:323 250:324 251:325 252:325 253:326 254:327 255:328 256:329 257:330 258:331 259:332 260:333 261:333 262:333 263:333 264:334 265:335 266:336 267:337 268:338 269:339 270:339 271:340 272:341 273:342 274:343 275:344 276:345 277:346 278:347 279:347 280:348 281:349 282:350 283:351 284:352 285:353 286:353 287:354 288:355 289:356 290:357 291:358 292:359 293:360 294:361 295:361 296:361 297:361 298:362 299:363 300:364 301:365 302:366 303:367 304:367 305:368 306:369 307:370 308:371 309:372 310:373 311:373 312:374 313:375 314:376 315:377 316:378 317:379 318:380 319:381 320:382 321:383 322:384 323:385 324:386 325:386 326:387 327:388 328:389 329:390 330:391 331:392 332:393 333:394 334:395 335:396 336:397 337:398 338:399 339:399 340:400 341:401 342:402 343:403 344:404 345:405 346:406 347:407 348:408 349:409 350:410 351:411 352:412 353:412 354:413 355:414 356:415 357:415 358:415 359:416 360:417 361:418 362:419 363:420 364:421 365:422 366:423 367:424 368:425 369:425 370:426 371:427 372:428 373:429 374:430 375:431 376:432 377:433 378:434 379:435 380:436 381:437 382:438\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4435 2011 11712 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000052\n",
            "INFO:tensorflow:example_index: 10\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the top brand by quantity [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:225 10:226 11:227 12:228 13:229 14:230 15:231 16:232 17:233 18:234 19:235 20:236 21:237 22:237 23:237 24:237 25:238 26:239 27:240 28:241 29:242 30:243 31:244 32:245 33:246 34:247 35:248 36:249 37:250 38:251 39:251 40:252 41:253 42:254 43:255 44:256 45:257 46:258 47:259 48:260 49:261 50:262 51:263 52:264 53:265 54:265 55:265 56:265 57:266 58:267 59:268 60:269 61:270 62:271 63:272 64:273 65:274 66:275 67:276 68:277 69:277 70:278 71:279 72:280 73:281 74:282 75:283 76:283 77:284 78:285 79:286 80:287 81:288 82:289 83:290 84:291 85:291 86:292 87:293 88:294 89:295 90:296 91:297 92:297 93:298 94:299 95:300 96:301 97:302 98:303 99:304 100:305 101:305 102:306 103:307 104:308 105:309 106:310 107:311 108:311 109:312 110:313 111:314 112:315 113:316 114:317 115:318 116:319 117:319 118:320 119:321 120:322 121:323 122:324 123:325 124:325 125:326 126:327 127:328 128:329 129:330 130:331 131:332 132:333 133:333 134:333 135:333 136:334 137:335 138:336 139:337 140:338 141:339 142:339 143:340 144:341 145:342 146:343 147:344 148:345 149:346 150:347 151:347 152:348 153:349 154:350 155:351 156:352 157:353 158:353 159:354 160:355 161:356 162:357 163:358 164:359 165:360 166:361 167:361 168:361 169:361 170:362 171:363 172:364 173:365 174:366 175:367 176:367 177:368 178:369 179:370 180:371 181:372 182:373 183:373 184:374 185:375 186:376 187:377 188:378 189:379 190:380 191:381 192:382 193:383 194:384 195:385 196:386 197:386 198:387 199:388 200:389 201:390 202:391 203:392 204:393 205:394 206:395 207:396 208:397 209:398 210:399 211:399 212:400 213:401 214:402 215:403 216:404 217:405 218:406 219:407 220:408 221:409 222:410 223:411 224:412 225:412 226:413 227:414 228:415 229:415 230:415 231:416 232:417 233:418 234:419 235:420 236:421 237:422 238:423 239:424 240:425 241:425 242:426 243:427 244:428 245:429 246:430 247:431 248:432 249:433 250:434 251:435 252:436 253:437 254:438 255:438 256:439 257:440 258:441 259:441 260:441 261:442 262:443 263:444 264:445 265:446 266:447 267:448 268:449 269:450 270:451 271:451 272:452 273:453 274:454 275:454 276:454 277:455 278:456 279:457 280:458 281:459 282:460 283:461 284:462 285:463 286:464 287:464 288:464 289:465 290:466 291:467 292:468 293:469 294:470 295:471 296:472 297:473 298:474 299:475 300:476 301:477 302:477 303:478 304:479 305:480 306:481 307:482 308:483 309:484 310:485 311:486 312:487 313:488 314:489 315:490 316:490 317:491 318:492 319:493 320:494 321:495 322:496 323:497 324:498 325:499 326:500 327:501 328:502 329:503 330:503 331:504 332:505 333:506 334:506 335:506 336:507 337:508 338:509 339:510 340:511 341:512 342:513 343:514 344:515 345:516 346:516 347:517 348:518 349:519 350:520 351:521 352:522 353:523 354:524 355:525 356:526 357:527 358:528 359:529 360:529 361:530 362:531 363:532 364:532 365:532 366:533 367:534 368:535 369:536 370:537 371:538 372:539 373:540 374:541 375:542 376:542 377:543 378:544 379:545 380:545 381:545 382:546\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4435 2011 11712 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000053\n",
            "INFO:tensorflow:example_index: 10\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the top brand by quantity [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:335 10:336 11:337 12:338 13:339 14:339 15:340 16:341 17:342 18:343 19:344 20:345 21:346 22:347 23:347 24:348 25:349 26:350 27:351 28:352 29:353 30:353 31:354 32:355 33:356 34:357 35:358 36:359 37:360 38:361 39:361 40:361 41:361 42:362 43:363 44:364 45:365 46:366 47:367 48:367 49:368 50:369 51:370 52:371 53:372 54:373 55:373 56:374 57:375 58:376 59:377 60:378 61:379 62:380 63:381 64:382 65:383 66:384 67:385 68:386 69:386 70:387 71:388 72:389 73:390 74:391 75:392 76:393 77:394 78:395 79:396 80:397 81:398 82:399 83:399 84:400 85:401 86:402 87:403 88:404 89:405 90:406 91:407 92:408 93:409 94:410 95:411 96:412 97:412 98:413 99:414 100:415 101:415 102:415 103:416 104:417 105:418 106:419 107:420 108:421 109:422 110:423 111:424 112:425 113:425 114:426 115:427 116:428 117:429 118:430 119:431 120:432 121:433 122:434 123:435 124:436 125:437 126:438 127:438 128:439 129:440 130:441 131:441 132:441 133:442 134:443 135:444 136:445 137:446 138:447 139:448 140:449 141:450 142:451 143:451 144:452 145:453 146:454 147:454 148:454 149:455 150:456 151:457 152:458 153:459 154:460 155:461 156:462 157:463 158:464 159:464 160:464 161:465 162:466 163:467 164:468 165:469 166:470 167:471 168:472 169:473 170:474 171:475 172:476 173:477 174:477 175:478 176:479 177:480 178:481 179:482 180:483 181:484 182:485 183:486 184:487 185:488 186:489 187:490 188:490 189:491 190:492 191:493 192:494 193:495 194:496 195:497 196:498 197:499 198:500 199:501 200:502 201:503 202:503 203:504 204:505 205:506 206:506 207:506 208:507 209:508 210:509 211:510 212:511 213:512 214:513 215:514 216:515 217:516 218:516 219:517 220:518 221:519 222:520 223:521 224:522 225:523 226:524 227:525 228:526 229:527 230:528 231:529 232:529 233:530 234:531 235:532 236:532 237:532 238:533 239:534 240:535 241:536 242:537 243:538 244:539 245:540 246:541 247:542 248:542 249:543 250:544 251:545 252:545 253:545 254:546 255:547 256:548 257:549 258:550 259:551 260:552 261:553 262:554 263:555 264:555 265:555 266:556 267:557 268:558 269:559 270:560 271:561 272:562 273:563 274:564 275:565 276:566 277:567 278:568 279:569 280:569 281:570 282:571 283:572 284:573 285:574 286:575 287:576 288:577 289:578 290:579 291:580 292:581 293:582 294:583 295:583 296:584 297:585 298:586 299:587 300:588 301:589 302:590 303:591 304:592 305:593 306:594 307:595 308:596 309:597 310:597 311:598 312:599 313:600 314:601 315:602 316:603 317:604 318:605 319:606 320:607 321:608 322:609 323:610 324:611 325:611 326:611 327:611 328:612 329:613 330:614 331:615 332:616 333:617 334:618 335:619 336:620 337:621 338:622 339:623 340:624 341:625 342:625 343:626 344:627 345:628 346:629 347:630 348:631 349:632 350:633 351:634 352:635 353:636 354:637 355:638 356:639 357:639 358:639 359:639 360:640 361:641 362:642 363:643 364:644 365:645 366:646 367:647 368:648 369:649 370:650 371:651 372:651 373:652 374:653 375:654 376:655 377:656 378:657 379:657 380:658 381:659 382:660\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4435 2011 11712 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000054\n",
            "INFO:tensorflow:example_index: 10\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the top brand by quantity [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:446 10:447 11:448 12:449 13:450 14:451 15:451 16:452 17:453 18:454 19:454 20:454 21:455 22:456 23:457 24:458 25:459 26:460 27:461 28:462 29:463 30:464 31:464 32:464 33:465 34:466 35:467 36:468 37:469 38:470 39:471 40:472 41:473 42:474 43:475 44:476 45:477 46:477 47:478 48:479 49:480 50:481 51:482 52:483 53:484 54:485 55:486 56:487 57:488 58:489 59:490 60:490 61:491 62:492 63:493 64:494 65:495 66:496 67:497 68:498 69:499 70:500 71:501 72:502 73:503 74:503 75:504 76:505 77:506 78:506 79:506 80:507 81:508 82:509 83:510 84:511 85:512 86:513 87:514 88:515 89:516 90:516 91:517 92:518 93:519 94:520 95:521 96:522 97:523 98:524 99:525 100:526 101:527 102:528 103:529 104:529 105:530 106:531 107:532 108:532 109:532 110:533 111:534 112:535 113:536 114:537 115:538 116:539 117:540 118:541 119:542 120:542 121:543 122:544 123:545 124:545 125:545 126:546 127:547 128:548 129:549 130:550 131:551 132:552 133:553 134:554 135:555 136:555 137:555 138:556 139:557 140:558 141:559 142:560 143:561 144:562 145:563 146:564 147:565 148:566 149:567 150:568 151:569 152:569 153:570 154:571 155:572 156:573 157:574 158:575 159:576 160:577 161:578 162:579 163:580 164:581 165:582 166:583 167:583 168:584 169:585 170:586 171:587 172:588 173:589 174:590 175:591 176:592 177:593 178:594 179:595 180:596 181:597 182:597 183:598 184:599 185:600 186:601 187:602 188:603 189:604 190:605 191:606 192:607 193:608 194:609 195:610 196:611 197:611 198:611 199:611 200:612 201:613 202:614 203:615 204:616 205:617 206:618 207:619 208:620 209:621 210:622 211:623 212:624 213:625 214:625 215:626 216:627 217:628 218:629 219:630 220:631 221:632 222:633 223:634 224:635 225:636 226:637 227:638 228:639 229:639 230:639 231:639 232:640 233:641 234:642 235:643 236:644 237:645 238:646 239:647 240:648 241:649 242:650 243:651 244:651 245:652 246:653 247:654 248:655 249:656 250:657 251:657 252:658 253:659 254:660 255:661 256:662 257:663 258:664 259:665 260:665 261:666 262:667 263:668 264:669 265:670 266:671 267:671 268:672 269:673 270:674 271:675 272:676 273:677 274:678 275:679 276:679 277:680 278:681 279:682 280:683 281:684 282:685 283:685 284:686 285:687 286:688 287:689 288:690 289:691 290:692 291:693 292:693 293:694 294:695 295:696 296:697 297:698 298:699 299:699 300:700 301:701 302:702 303:703 304:704 305:705 306:706 307:707 308:707 309:707 310:707 311:708 312:709 313:710 314:711 315:712 316:713 317:713 318:714 319:715 320:716 321:717 322:718 323:719 324:720 325:721 326:721 327:722 328:723 329:724 330:725 331:726 332:727 333:727 334:728 335:729 336:730 337:731 338:732 339:733 340:734 341:735 342:735 343:735 344:735 345:736 346:737 347:738 348:739 349:740 350:741 351:741 352:742 353:743 354:744 355:745 356:746 357:747 358:747\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4435 2011 11712 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000055\n",
            "INFO:tensorflow:example_index: 11\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the top commodity by sales [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:19 30:20 31:21 32:22 33:23 34:24 35:25 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:33 45:34 46:35 47:36 48:37 49:38 50:38 51:39 52:40 53:41 54:41 55:41 56:42 57:43 58:44 59:45 60:46 61:47 62:48 63:49 64:50 65:51 66:51 67:52 68:53 69:54 70:55 71:56 72:57 73:58 74:59 75:60 76:61 77:62 78:63 79:64 80:64 81:65 82:66 83:67 84:67 85:67 86:68 87:69 88:70 89:71 90:72 91:73 92:74 93:75 94:76 95:77 96:77 97:78 98:79 99:80 100:80 101:80 102:81 103:82 104:83 105:84 106:85 107:86 108:87 109:88 110:89 111:90 112:90 113:90 114:91 115:92 116:93 117:94 118:95 119:96 120:97 121:98 122:99 123:100 124:101 125:102 126:103 127:103 128:104 129:105 130:106 131:107 132:108 133:109 134:110 135:111 136:112 137:113 138:114 139:115 140:116 141:116 142:117 143:118 144:119 145:120 146:121 147:122 148:123 149:124 150:125 151:126 152:127 153:128 154:129 155:129 156:130 157:131 158:132 159:132 160:132 161:133 162:134 163:135 164:136 165:137 166:138 167:139 168:140 169:141 170:142 171:142 172:143 173:144 174:145 175:146 176:147 177:148 178:149 179:150 180:151 181:152 182:153 183:154 184:155 185:155 186:156 187:157 188:158 189:158 190:158 191:159 192:160 193:161 194:162 195:163 196:164 197:165 198:166 199:167 200:168 201:168 202:169 203:170 204:171 205:171 206:171 207:172 208:173 209:174 210:175 211:176 212:177 213:178 214:179 215:180 216:181 217:181 218:181 219:182 220:183 221:184 222:185 223:186 224:187 225:188 226:189 227:190 228:191 229:192 230:193 231:194 232:195 233:195 234:196 235:197 236:198 237:199 238:200 239:201 240:202 241:203 242:204 243:205 244:206 245:207 246:208 247:209 248:209 249:210 250:211 251:212 252:213 253:214 254:215 255:216 256:217 257:218 258:219 259:220 260:221 261:222 262:223 263:223 264:224 265:225 266:226 267:227 268:228 269:229 270:230 271:231 272:232 273:233 274:234 275:235 276:236 277:237 278:237 279:237 280:237 281:238 282:239 283:240 284:241 285:242 286:243 287:244 288:245 289:246 290:247 291:248 292:249 293:250 294:251 295:251 296:252 297:253 298:254 299:255 300:256 301:257 302:258 303:259 304:260 305:261 306:262 307:263 308:264 309:265 310:265 311:265 312:265 313:266 314:267 315:268 316:269 317:270 318:271 319:272 320:273 321:274 322:275 323:276 324:277 325:277 326:278 327:279 328:280 329:281 330:282 331:283 332:283 333:284 334:285 335:286 336:287 337:288 338:289 339:290 340:291 341:291 342:292 343:293 344:294 345:295 346:296 347:297 348:297 349:298 350:299 351:300 352:301 353:302 354:303 355:304 356:305 357:305 358:306 359:307 360:308 361:309 362:310 363:311 364:311 365:312 366:313 367:314 368:315 369:316 370:317 371:318 372:319 373:319 374:320 375:321 376:322 377:323 378:324 379:325 380:325 381:326 382:327\n",
            "INFO:tensorflow:token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 19502 2011 4341 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000056\n",
            "INFO:tensorflow:example_index: 11\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the top commodity by sales [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:113 10:114 11:115 12:116 13:116 14:117 15:118 16:119 17:120 18:121 19:122 20:123 21:124 22:125 23:126 24:127 25:128 26:129 27:129 28:130 29:131 30:132 31:132 32:132 33:133 34:134 35:135 36:136 37:137 38:138 39:139 40:140 41:141 42:142 43:142 44:143 45:144 46:145 47:146 48:147 49:148 50:149 51:150 52:151 53:152 54:153 55:154 56:155 57:155 58:156 59:157 60:158 61:158 62:158 63:159 64:160 65:161 66:162 67:163 68:164 69:165 70:166 71:167 72:168 73:168 74:169 75:170 76:171 77:171 78:171 79:172 80:173 81:174 82:175 83:176 84:177 85:178 86:179 87:180 88:181 89:181 90:181 91:182 92:183 93:184 94:185 95:186 96:187 97:188 98:189 99:190 100:191 101:192 102:193 103:194 104:195 105:195 106:196 107:197 108:198 109:199 110:200 111:201 112:202 113:203 114:204 115:205 116:206 117:207 118:208 119:209 120:209 121:210 122:211 123:212 124:213 125:214 126:215 127:216 128:217 129:218 130:219 131:220 132:221 133:222 134:223 135:223 136:224 137:225 138:226 139:227 140:228 141:229 142:230 143:231 144:232 145:233 146:234 147:235 148:236 149:237 150:237 151:237 152:237 153:238 154:239 155:240 156:241 157:242 158:243 159:244 160:245 161:246 162:247 163:248 164:249 165:250 166:251 167:251 168:252 169:253 170:254 171:255 172:256 173:257 174:258 175:259 176:260 177:261 178:262 179:263 180:264 181:265 182:265 183:265 184:265 185:266 186:267 187:268 188:269 189:270 190:271 191:272 192:273 193:274 194:275 195:276 196:277 197:277 198:278 199:279 200:280 201:281 202:282 203:283 204:283 205:284 206:285 207:286 208:287 209:288 210:289 211:290 212:291 213:291 214:292 215:293 216:294 217:295 218:296 219:297 220:297 221:298 222:299 223:300 224:301 225:302 226:303 227:304 228:305 229:305 230:306 231:307 232:308 233:309 234:310 235:311 236:311 237:312 238:313 239:314 240:315 241:316 242:317 243:318 244:319 245:319 246:320 247:321 248:322 249:323 250:324 251:325 252:325 253:326 254:327 255:328 256:329 257:330 258:331 259:332 260:333 261:333 262:333 263:333 264:334 265:335 266:336 267:337 268:338 269:339 270:339 271:340 272:341 273:342 274:343 275:344 276:345 277:346 278:347 279:347 280:348 281:349 282:350 283:351 284:352 285:353 286:353 287:354 288:355 289:356 290:357 291:358 292:359 293:360 294:361 295:361 296:361 297:361 298:362 299:363 300:364 301:365 302:366 303:367 304:367 305:368 306:369 307:370 308:371 309:372 310:373 311:373 312:374 313:375 314:376 315:377 316:378 317:379 318:380 319:381 320:382 321:383 322:384 323:385 324:386 325:386 326:387 327:388 328:389 329:390 330:391 331:392 332:393 333:394 334:395 335:396 336:397 337:398 338:399 339:399 340:400 341:401 342:402 343:403 344:404 345:405 346:406 347:407 348:408 349:409 350:410 351:411 352:412 353:412 354:413 355:414 356:415 357:415 358:415 359:416 360:417 361:418 362:419 363:420 364:421 365:422 366:423 367:424 368:425 369:425 370:426 371:427 372:428 373:429 374:430 375:431 376:432 377:433 378:434 379:435 380:436 381:437 382:438\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 19502 2011 4341 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 340\n",
            "INFO:tensorflow:end_position: 353\n",
            "INFO:tensorflow:answer: the top commodity for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000057\n",
            "INFO:tensorflow:example_index: 11\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the top commodity by sales [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:225 10:226 11:227 12:228 13:229 14:230 15:231 16:232 17:233 18:234 19:235 20:236 21:237 22:237 23:237 24:237 25:238 26:239 27:240 28:241 29:242 30:243 31:244 32:245 33:246 34:247 35:248 36:249 37:250 38:251 39:251 40:252 41:253 42:254 43:255 44:256 45:257 46:258 47:259 48:260 49:261 50:262 51:263 52:264 53:265 54:265 55:265 56:265 57:266 58:267 59:268 60:269 61:270 62:271 63:272 64:273 65:274 66:275 67:276 68:277 69:277 70:278 71:279 72:280 73:281 74:282 75:283 76:283 77:284 78:285 79:286 80:287 81:288 82:289 83:290 84:291 85:291 86:292 87:293 88:294 89:295 90:296 91:297 92:297 93:298 94:299 95:300 96:301 97:302 98:303 99:304 100:305 101:305 102:306 103:307 104:308 105:309 106:310 107:311 108:311 109:312 110:313 111:314 112:315 113:316 114:317 115:318 116:319 117:319 118:320 119:321 120:322 121:323 122:324 123:325 124:325 125:326 126:327 127:328 128:329 129:330 130:331 131:332 132:333 133:333 134:333 135:333 136:334 137:335 138:336 139:337 140:338 141:339 142:339 143:340 144:341 145:342 146:343 147:344 148:345 149:346 150:347 151:347 152:348 153:349 154:350 155:351 156:352 157:353 158:353 159:354 160:355 161:356 162:357 163:358 164:359 165:360 166:361 167:361 168:361 169:361 170:362 171:363 172:364 173:365 174:366 175:367 176:367 177:368 178:369 179:370 180:371 181:372 182:373 183:373 184:374 185:375 186:376 187:377 188:378 189:379 190:380 191:381 192:382 193:383 194:384 195:385 196:386 197:386 198:387 199:388 200:389 201:390 202:391 203:392 204:393 205:394 206:395 207:396 208:397 209:398 210:399 211:399 212:400 213:401 214:402 215:403 216:404 217:405 218:406 219:407 220:408 221:409 222:410 223:411 224:412 225:412 226:413 227:414 228:415 229:415 230:415 231:416 232:417 233:418 234:419 235:420 236:421 237:422 238:423 239:424 240:425 241:425 242:426 243:427 244:428 245:429 246:430 247:431 248:432 249:433 250:434 251:435 252:436 253:437 254:438 255:438 256:439 257:440 258:441 259:441 260:441 261:442 262:443 263:444 264:445 265:446 266:447 267:448 268:449 269:450 270:451 271:451 272:452 273:453 274:454 275:454 276:454 277:455 278:456 279:457 280:458 281:459 282:460 283:461 284:462 285:463 286:464 287:464 288:464 289:465 290:466 291:467 292:468 293:469 294:470 295:471 296:472 297:473 298:474 299:475 300:476 301:477 302:477 303:478 304:479 305:480 306:481 307:482 308:483 309:484 310:485 311:486 312:487 313:488 314:489 315:490 316:490 317:491 318:492 319:493 320:494 321:495 322:496 323:497 324:498 325:499 326:500 327:501 328:502 329:503 330:503 331:504 332:505 333:506 334:506 335:506 336:507 337:508 338:509 339:510 340:511 341:512 342:513 343:514 344:515 345:516 346:516 347:517 348:518 349:519 350:520 351:521 352:522 353:523 354:524 355:525 356:526 357:527 358:528 359:529 360:529 361:530 362:531 363:532 364:532 365:532 366:533 367:534 368:535 369:536 370:537 371:538 372:539 373:540 374:541 375:542 376:542 377:543 378:544 379:545 380:545 381:545 382:546\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 19502 2011 4341 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 212\n",
            "INFO:tensorflow:end_position: 225\n",
            "INFO:tensorflow:answer: the top commodity for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000058\n",
            "INFO:tensorflow:example_index: 11\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the top commodity by sales [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:335 10:336 11:337 12:338 13:339 14:339 15:340 16:341 17:342 18:343 19:344 20:345 21:346 22:347 23:347 24:348 25:349 26:350 27:351 28:352 29:353 30:353 31:354 32:355 33:356 34:357 35:358 36:359 37:360 38:361 39:361 40:361 41:361 42:362 43:363 44:364 45:365 46:366 47:367 48:367 49:368 50:369 51:370 52:371 53:372 54:373 55:373 56:374 57:375 58:376 59:377 60:378 61:379 62:380 63:381 64:382 65:383 66:384 67:385 68:386 69:386 70:387 71:388 72:389 73:390 74:391 75:392 76:393 77:394 78:395 79:396 80:397 81:398 82:399 83:399 84:400 85:401 86:402 87:403 88:404 89:405 90:406 91:407 92:408 93:409 94:410 95:411 96:412 97:412 98:413 99:414 100:415 101:415 102:415 103:416 104:417 105:418 106:419 107:420 108:421 109:422 110:423 111:424 112:425 113:425 114:426 115:427 116:428 117:429 118:430 119:431 120:432 121:433 122:434 123:435 124:436 125:437 126:438 127:438 128:439 129:440 130:441 131:441 132:441 133:442 134:443 135:444 136:445 137:446 138:447 139:448 140:449 141:450 142:451 143:451 144:452 145:453 146:454 147:454 148:454 149:455 150:456 151:457 152:458 153:459 154:460 155:461 156:462 157:463 158:464 159:464 160:464 161:465 162:466 163:467 164:468 165:469 166:470 167:471 168:472 169:473 170:474 171:475 172:476 173:477 174:477 175:478 176:479 177:480 178:481 179:482 180:483 181:484 182:485 183:486 184:487 185:488 186:489 187:490 188:490 189:491 190:492 191:493 192:494 193:495 194:496 195:497 196:498 197:499 198:500 199:501 200:502 201:503 202:503 203:504 204:505 205:506 206:506 207:506 208:507 209:508 210:509 211:510 212:511 213:512 214:513 215:514 216:515 217:516 218:516 219:517 220:518 221:519 222:520 223:521 224:522 225:523 226:524 227:525 228:526 229:527 230:528 231:529 232:529 233:530 234:531 235:532 236:532 237:532 238:533 239:534 240:535 241:536 242:537 243:538 244:539 245:540 246:541 247:542 248:542 249:543 250:544 251:545 252:545 253:545 254:546 255:547 256:548 257:549 258:550 259:551 260:552 261:553 262:554 263:555 264:555 265:555 266:556 267:557 268:558 269:559 270:560 271:561 272:562 273:563 274:564 275:565 276:566 277:567 278:568 279:569 280:569 281:570 282:571 283:572 284:573 285:574 286:575 287:576 288:577 289:578 290:579 291:580 292:581 293:582 294:583 295:583 296:584 297:585 298:586 299:587 300:588 301:589 302:590 303:591 304:592 305:593 306:594 307:595 308:596 309:597 310:597 311:598 312:599 313:600 314:601 315:602 316:603 317:604 318:605 319:606 320:607 321:608 322:609 323:610 324:611 325:611 326:611 327:611 328:612 329:613 330:614 331:615 332:616 333:617 334:618 335:619 336:620 337:621 338:622 339:623 340:624 341:625 342:625 343:626 344:627 345:628 346:629 347:630 348:631 349:632 350:633 351:634 352:635 353:636 354:637 355:638 356:639 357:639 358:639 359:639 360:640 361:641 362:642 363:643 364:644 365:645 366:646 367:647 368:648 369:649 370:650 371:651 372:651 373:652 374:653 375:654 376:655 377:656 378:657 379:657 380:658 381:659 382:660\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 19502 2011 4341 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 84\n",
            "INFO:tensorflow:end_position: 97\n",
            "INFO:tensorflow:answer: the top commodity for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000059\n",
            "INFO:tensorflow:example_index: 11\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the top commodity by sales [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:446 10:447 11:448 12:449 13:450 14:451 15:451 16:452 17:453 18:454 19:454 20:454 21:455 22:456 23:457 24:458 25:459 26:460 27:461 28:462 29:463 30:464 31:464 32:464 33:465 34:466 35:467 36:468 37:469 38:470 39:471 40:472 41:473 42:474 43:475 44:476 45:477 46:477 47:478 48:479 49:480 50:481 51:482 52:483 53:484 54:485 55:486 56:487 57:488 58:489 59:490 60:490 61:491 62:492 63:493 64:494 65:495 66:496 67:497 68:498 69:499 70:500 71:501 72:502 73:503 74:503 75:504 76:505 77:506 78:506 79:506 80:507 81:508 82:509 83:510 84:511 85:512 86:513 87:514 88:515 89:516 90:516 91:517 92:518 93:519 94:520 95:521 96:522 97:523 98:524 99:525 100:526 101:527 102:528 103:529 104:529 105:530 106:531 107:532 108:532 109:532 110:533 111:534 112:535 113:536 114:537 115:538 116:539 117:540 118:541 119:542 120:542 121:543 122:544 123:545 124:545 125:545 126:546 127:547 128:548 129:549 130:550 131:551 132:552 133:553 134:554 135:555 136:555 137:555 138:556 139:557 140:558 141:559 142:560 143:561 144:562 145:563 146:564 147:565 148:566 149:567 150:568 151:569 152:569 153:570 154:571 155:572 156:573 157:574 158:575 159:576 160:577 161:578 162:579 163:580 164:581 165:582 166:583 167:583 168:584 169:585 170:586 171:587 172:588 173:589 174:590 175:591 176:592 177:593 178:594 179:595 180:596 181:597 182:597 183:598 184:599 185:600 186:601 187:602 188:603 189:604 190:605 191:606 192:607 193:608 194:609 195:610 196:611 197:611 198:611 199:611 200:612 201:613 202:614 203:615 204:616 205:617 206:618 207:619 208:620 209:621 210:622 211:623 212:624 213:625 214:625 215:626 216:627 217:628 218:629 219:630 220:631 221:632 222:633 223:634 224:635 225:636 226:637 227:638 228:639 229:639 230:639 231:639 232:640 233:641 234:642 235:643 236:644 237:645 238:646 239:647 240:648 241:649 242:650 243:651 244:651 245:652 246:653 247:654 248:655 249:656 250:657 251:657 252:658 253:659 254:660 255:661 256:662 257:663 258:664 259:665 260:665 261:666 262:667 263:668 264:669 265:670 266:671 267:671 268:672 269:673 270:674 271:675 272:676 273:677 274:678 275:679 276:679 277:680 278:681 279:682 280:683 281:684 282:685 283:685 284:686 285:687 286:688 287:689 288:690 289:691 290:692 291:693 292:693 293:694 294:695 295:696 296:697 297:698 298:699 299:699 300:700 301:701 302:702 303:703 304:704 305:705 306:706 307:707 308:707 309:707 310:707 311:708 312:709 313:710 314:711 315:712 316:713 317:713 318:714 319:715 320:716 321:717 322:718 323:719 324:720 325:721 326:721 327:722 328:723 329:724 330:725 331:726 332:727 333:727 334:728 335:729 336:730 337:731 338:732 339:733 340:734 341:735 342:735 343:735 344:735 345:736 346:737 347:738 348:739 349:740 350:741 351:741 352:742 353:743 354:744 355:745 356:746 357:747 358:747\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 19502 2011 4341 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000060\n",
            "INFO:tensorflow:example_index: 12\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:19 32:20 33:21 34:22 35:23 36:24 37:25 38:25 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:36 50:37 51:38 52:38 53:39 54:40 55:41 56:41 57:41 58:42 59:43 60:44 61:45 62:46 63:47 64:48 65:49 66:50 67:51 68:51 69:52 70:53 71:54 72:55 73:56 74:57 75:58 76:59 77:60 78:61 79:62 80:63 81:64 82:64 83:65 84:66 85:67 86:67 87:67 88:68 89:69 90:70 91:71 92:72 93:73 94:74 95:75 96:76 97:77 98:77 99:78 100:79 101:80 102:80 103:80 104:81 105:82 106:83 107:84 108:85 109:86 110:87 111:88 112:89 113:90 114:90 115:90 116:91 117:92 118:93 119:94 120:95 121:96 122:97 123:98 124:99 125:100 126:101 127:102 128:103 129:103 130:104 131:105 132:106 133:107 134:108 135:109 136:110 137:111 138:112 139:113 140:114 141:115 142:116 143:116 144:117 145:118 146:119 147:120 148:121 149:122 150:123 151:124 152:125 153:126 154:127 155:128 156:129 157:129 158:130 159:131 160:132 161:132 162:132 163:133 164:134 165:135 166:136 167:137 168:138 169:139 170:140 171:141 172:142 173:142 174:143 175:144 176:145 177:146 178:147 179:148 180:149 181:150 182:151 183:152 184:153 185:154 186:155 187:155 188:156 189:157 190:158 191:158 192:158 193:159 194:160 195:161 196:162 197:163 198:164 199:165 200:166 201:167 202:168 203:168 204:169 205:170 206:171 207:171 208:171 209:172 210:173 211:174 212:175 213:176 214:177 215:178 216:179 217:180 218:181 219:181 220:181 221:182 222:183 223:184 224:185 225:186 226:187 227:188 228:189 229:190 230:191 231:192 232:193 233:194 234:195 235:195 236:196 237:197 238:198 239:199 240:200 241:201 242:202 243:203 244:204 245:205 246:206 247:207 248:208 249:209 250:209 251:210 252:211 253:212 254:213 255:214 256:215 257:216 258:217 259:218 260:219 261:220 262:221 263:222 264:223 265:223 266:224 267:225 268:226 269:227 270:228 271:229 272:230 273:231 274:232 275:233 276:234 277:235 278:236 279:237 280:237 281:237 282:237 283:238 284:239 285:240 286:241 287:242 288:243 289:244 290:245 291:246 292:247 293:248 294:249 295:250 296:251 297:251 298:252 299:253 300:254 301:255 302:256 303:257 304:258 305:259 306:260 307:261 308:262 309:263 310:264 311:265 312:265 313:265 314:265 315:266 316:267 317:268 318:269 319:270 320:271 321:272 322:273 323:274 324:275 325:276 326:277 327:277 328:278 329:279 330:280 331:281 332:282 333:283 334:283 335:284 336:285 337:286 338:287 339:288 340:289 341:290 342:291 343:291 344:292 345:293 346:294 347:295 348:296 349:297 350:297 351:298 352:299 353:300 354:301 355:302 356:303 357:304 358:305 359:305 360:306 361:307 362:308 363:309 364:310 365:311 366:311 367:312 368:313 369:314 370:315 371:316 372:317 373:318 374:319 375:319 376:320 377:321 378:322 379:323 380:324 381:325 382:325\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000061\n",
            "INFO:tensorflow:example_index: 12\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:113 12:114 13:115 14:116 15:116 16:117 17:118 18:119 19:120 20:121 21:122 22:123 23:124 24:125 25:126 26:127 27:128 28:129 29:129 30:130 31:131 32:132 33:132 34:132 35:133 36:134 37:135 38:136 39:137 40:138 41:139 42:140 43:141 44:142 45:142 46:143 47:144 48:145 49:146 50:147 51:148 52:149 53:150 54:151 55:152 56:153 57:154 58:155 59:155 60:156 61:157 62:158 63:158 64:158 65:159 66:160 67:161 68:162 69:163 70:164 71:165 72:166 73:167 74:168 75:168 76:169 77:170 78:171 79:171 80:171 81:172 82:173 83:174 84:175 85:176 86:177 87:178 88:179 89:180 90:181 91:181 92:181 93:182 94:183 95:184 96:185 97:186 98:187 99:188 100:189 101:190 102:191 103:192 104:193 105:194 106:195 107:195 108:196 109:197 110:198 111:199 112:200 113:201 114:202 115:203 116:204 117:205 118:206 119:207 120:208 121:209 122:209 123:210 124:211 125:212 126:213 127:214 128:215 129:216 130:217 131:218 132:219 133:220 134:221 135:222 136:223 137:223 138:224 139:225 140:226 141:227 142:228 143:229 144:230 145:231 146:232 147:233 148:234 149:235 150:236 151:237 152:237 153:237 154:237 155:238 156:239 157:240 158:241 159:242 160:243 161:244 162:245 163:246 164:247 165:248 166:249 167:250 168:251 169:251 170:252 171:253 172:254 173:255 174:256 175:257 176:258 177:259 178:260 179:261 180:262 181:263 182:264 183:265 184:265 185:265 186:265 187:266 188:267 189:268 190:269 191:270 192:271 193:272 194:273 195:274 196:275 197:276 198:277 199:277 200:278 201:279 202:280 203:281 204:282 205:283 206:283 207:284 208:285 209:286 210:287 211:288 212:289 213:290 214:291 215:291 216:292 217:293 218:294 219:295 220:296 221:297 222:297 223:298 224:299 225:300 226:301 227:302 228:303 229:304 230:305 231:305 232:306 233:307 234:308 235:309 236:310 237:311 238:311 239:312 240:313 241:314 242:315 243:316 244:317 245:318 246:319 247:319 248:320 249:321 250:322 251:323 252:324 253:325 254:325 255:326 256:327 257:328 258:329 259:330 260:331 261:332 262:333 263:333 264:333 265:333 266:334 267:335 268:336 269:337 270:338 271:339 272:339 273:340 274:341 275:342 276:343 277:344 278:345 279:346 280:347 281:347 282:348 283:349 284:350 285:351 286:352 287:353 288:353 289:354 290:355 291:356 292:357 293:358 294:359 295:360 296:361 297:361 298:361 299:361 300:362 301:363 302:364 303:365 304:366 305:367 306:367 307:368 308:369 309:370 310:371 311:372 312:373 313:373 314:374 315:375 316:376 317:377 318:378 319:379 320:380 321:381 322:382 323:383 324:384 325:385 326:386 327:386 328:387 329:388 330:389 331:390 332:391 333:392 334:393 335:394 336:395 337:396 338:397 339:398 340:399 341:399 342:400 343:401 344:402 345:403 346:404 347:405 348:406 349:407 350:408 351:409 352:410 353:411 354:412 355:412 356:413 357:414 358:415 359:415 360:415 361:416 362:417 363:418 364:419 365:420 366:421 367:422 368:423 369:424 370:425 371:425 372:426 373:427 374:428 375:429 376:430 377:431 378:432 379:433 380:434 381:435 382:436\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000062\n",
            "INFO:tensorflow:example_index: 12\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:225 12:226 13:227 14:228 15:229 16:230 17:231 18:232 19:233 20:234 21:235 22:236 23:237 24:237 25:237 26:237 27:238 28:239 29:240 30:241 31:242 32:243 33:244 34:245 35:246 36:247 37:248 38:249 39:250 40:251 41:251 42:252 43:253 44:254 45:255 46:256 47:257 48:258 49:259 50:260 51:261 52:262 53:263 54:264 55:265 56:265 57:265 58:265 59:266 60:267 61:268 62:269 63:270 64:271 65:272 66:273 67:274 68:275 69:276 70:277 71:277 72:278 73:279 74:280 75:281 76:282 77:283 78:283 79:284 80:285 81:286 82:287 83:288 84:289 85:290 86:291 87:291 88:292 89:293 90:294 91:295 92:296 93:297 94:297 95:298 96:299 97:300 98:301 99:302 100:303 101:304 102:305 103:305 104:306 105:307 106:308 107:309 108:310 109:311 110:311 111:312 112:313 113:314 114:315 115:316 116:317 117:318 118:319 119:319 120:320 121:321 122:322 123:323 124:324 125:325 126:325 127:326 128:327 129:328 130:329 131:330 132:331 133:332 134:333 135:333 136:333 137:333 138:334 139:335 140:336 141:337 142:338 143:339 144:339 145:340 146:341 147:342 148:343 149:344 150:345 151:346 152:347 153:347 154:348 155:349 156:350 157:351 158:352 159:353 160:353 161:354 162:355 163:356 164:357 165:358 166:359 167:360 168:361 169:361 170:361 171:361 172:362 173:363 174:364 175:365 176:366 177:367 178:367 179:368 180:369 181:370 182:371 183:372 184:373 185:373 186:374 187:375 188:376 189:377 190:378 191:379 192:380 193:381 194:382 195:383 196:384 197:385 198:386 199:386 200:387 201:388 202:389 203:390 204:391 205:392 206:393 207:394 208:395 209:396 210:397 211:398 212:399 213:399 214:400 215:401 216:402 217:403 218:404 219:405 220:406 221:407 222:408 223:409 224:410 225:411 226:412 227:412 228:413 229:414 230:415 231:415 232:415 233:416 234:417 235:418 236:419 237:420 238:421 239:422 240:423 241:424 242:425 243:425 244:426 245:427 246:428 247:429 248:430 249:431 250:432 251:433 252:434 253:435 254:436 255:437 256:438 257:438 258:439 259:440 260:441 261:441 262:441 263:442 264:443 265:444 266:445 267:446 268:447 269:448 270:449 271:450 272:451 273:451 274:452 275:453 276:454 277:454 278:454 279:455 280:456 281:457 282:458 283:459 284:460 285:461 286:462 287:463 288:464 289:464 290:464 291:465 292:466 293:467 294:468 295:469 296:470 297:471 298:472 299:473 300:474 301:475 302:476 303:477 304:477 305:478 306:479 307:480 308:481 309:482 310:483 311:484 312:485 313:486 314:487 315:488 316:489 317:490 318:490 319:491 320:492 321:493 322:494 323:495 324:496 325:497 326:498 327:499 328:500 329:501 330:502 331:503 332:503 333:504 334:505 335:506 336:506 337:506 338:507 339:508 340:509 341:510 342:511 343:512 344:513 345:514 346:515 347:516 348:516 349:517 350:518 351:519 352:520 353:521 354:522 355:523 356:524 357:525 358:526 359:527 360:528 361:529 362:529 363:530 364:531 365:532 366:532 367:532 368:533 369:534 370:535 371:536 372:537 373:538 374:539 375:540 376:541 377:542 378:542 379:543 380:544 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000063\n",
            "INFO:tensorflow:example_index: 12\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:335 12:336 13:337 14:338 15:339 16:339 17:340 18:341 19:342 20:343 21:344 22:345 23:346 24:347 25:347 26:348 27:349 28:350 29:351 30:352 31:353 32:353 33:354 34:355 35:356 36:357 37:358 38:359 39:360 40:361 41:361 42:361 43:361 44:362 45:363 46:364 47:365 48:366 49:367 50:367 51:368 52:369 53:370 54:371 55:372 56:373 57:373 58:374 59:375 60:376 61:377 62:378 63:379 64:380 65:381 66:382 67:383 68:384 69:385 70:386 71:386 72:387 73:388 74:389 75:390 76:391 77:392 78:393 79:394 80:395 81:396 82:397 83:398 84:399 85:399 86:400 87:401 88:402 89:403 90:404 91:405 92:406 93:407 94:408 95:409 96:410 97:411 98:412 99:412 100:413 101:414 102:415 103:415 104:415 105:416 106:417 107:418 108:419 109:420 110:421 111:422 112:423 113:424 114:425 115:425 116:426 117:427 118:428 119:429 120:430 121:431 122:432 123:433 124:434 125:435 126:436 127:437 128:438 129:438 130:439 131:440 132:441 133:441 134:441 135:442 136:443 137:444 138:445 139:446 140:447 141:448 142:449 143:450 144:451 145:451 146:452 147:453 148:454 149:454 150:454 151:455 152:456 153:457 154:458 155:459 156:460 157:461 158:462 159:463 160:464 161:464 162:464 163:465 164:466 165:467 166:468 167:469 168:470 169:471 170:472 171:473 172:474 173:475 174:476 175:477 176:477 177:478 178:479 179:480 180:481 181:482 182:483 183:484 184:485 185:486 186:487 187:488 188:489 189:490 190:490 191:491 192:492 193:493 194:494 195:495 196:496 197:497 198:498 199:499 200:500 201:501 202:502 203:503 204:503 205:504 206:505 207:506 208:506 209:506 210:507 211:508 212:509 213:510 214:511 215:512 216:513 217:514 218:515 219:516 220:516 221:517 222:518 223:519 224:520 225:521 226:522 227:523 228:524 229:525 230:526 231:527 232:528 233:529 234:529 235:530 236:531 237:532 238:532 239:532 240:533 241:534 242:535 243:536 244:537 245:538 246:539 247:540 248:541 249:542 250:542 251:543 252:544 253:545 254:545 255:545 256:546 257:547 258:548 259:549 260:550 261:551 262:552 263:553 264:554 265:555 266:555 267:555 268:556 269:557 270:558 271:559 272:560 273:561 274:562 275:563 276:564 277:565 278:566 279:567 280:568 281:569 282:569 283:570 284:571 285:572 286:573 287:574 288:575 289:576 290:577 291:578 292:579 293:580 294:581 295:582 296:583 297:583 298:584 299:585 300:586 301:587 302:588 303:589 304:590 305:591 306:592 307:593 308:594 309:595 310:596 311:597 312:597 313:598 314:599 315:600 316:601 317:602 318:603 319:604 320:605 321:606 322:607 323:608 324:609 325:610 326:611 327:611 328:611 329:611 330:612 331:613 332:614 333:615 334:616 335:617 336:618 337:619 338:620 339:621 340:622 341:623 342:624 343:625 344:625 345:626 346:627 347:628 348:629 349:630 350:631 351:632 352:633 353:634 354:635 355:636 356:637 357:638 358:639 359:639 360:639 361:639 362:640 363:641 364:642 365:643 366:644 367:645 368:646 369:647 370:648 371:649 372:650 373:651 374:651 375:652 376:653 377:654 378:655 379:656 380:657 381:657 382:658\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 251\n",
            "INFO:tensorflow:end_position: 267\n",
            "INFO:tensorflow:answer: the bottom household _ type for sales between jul 2015 and jun 2017 was this . .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000064\n",
            "INFO:tensorflow:example_index: 12\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:446 12:447 13:448 14:449 15:450 16:451 17:451 18:452 19:453 20:454 21:454 22:454 23:455 24:456 25:457 26:458 27:459 28:460 29:461 30:462 31:463 32:464 33:464 34:464 35:465 36:466 37:467 38:468 39:469 40:470 41:471 42:472 43:473 44:474 45:475 46:476 47:477 48:477 49:478 50:479 51:480 52:481 53:482 54:483 55:484 56:485 57:486 58:487 59:488 60:489 61:490 62:490 63:491 64:492 65:493 66:494 67:495 68:496 69:497 70:498 71:499 72:500 73:501 74:502 75:503 76:503 77:504 78:505 79:506 80:506 81:506 82:507 83:508 84:509 85:510 86:511 87:512 88:513 89:514 90:515 91:516 92:516 93:517 94:518 95:519 96:520 97:521 98:522 99:523 100:524 101:525 102:526 103:527 104:528 105:529 106:529 107:530 108:531 109:532 110:532 111:532 112:533 113:534 114:535 115:536 116:537 117:538 118:539 119:540 120:541 121:542 122:542 123:543 124:544 125:545 126:545 127:545 128:546 129:547 130:548 131:549 132:550 133:551 134:552 135:553 136:554 137:555 138:555 139:555 140:556 141:557 142:558 143:559 144:560 145:561 146:562 147:563 148:564 149:565 150:566 151:567 152:568 153:569 154:569 155:570 156:571 157:572 158:573 159:574 160:575 161:576 162:577 163:578 164:579 165:580 166:581 167:582 168:583 169:583 170:584 171:585 172:586 173:587 174:588 175:589 176:590 177:591 178:592 179:593 180:594 181:595 182:596 183:597 184:597 185:598 186:599 187:600 188:601 189:602 190:603 191:604 192:605 193:606 194:607 195:608 196:609 197:610 198:611 199:611 200:611 201:611 202:612 203:613 204:614 205:615 206:616 207:617 208:618 209:619 210:620 211:621 212:622 213:623 214:624 215:625 216:625 217:626 218:627 219:628 220:629 221:630 222:631 223:632 224:633 225:634 226:635 227:636 228:637 229:638 230:639 231:639 232:639 233:639 234:640 235:641 236:642 237:643 238:644 239:645 240:646 241:647 242:648 243:649 244:650 245:651 246:651 247:652 248:653 249:654 250:655 251:656 252:657 253:657 254:658 255:659 256:660 257:661 258:662 259:663 260:664 261:665 262:665 263:666 264:667 265:668 266:669 267:670 268:671 269:671 270:672 271:673 272:674 273:675 274:676 275:677 276:678 277:679 278:679 279:680 280:681 281:682 282:683 283:684 284:685 285:685 286:686 287:687 288:688 289:689 290:690 291:691 292:692 293:693 294:693 295:694 296:695 297:696 298:697 299:698 300:699 301:699 302:700 303:701 304:702 305:703 306:704 307:705 308:706 309:707 310:707 311:707 312:707 313:708 314:709 315:710 316:711 317:712 318:713 319:713 320:714 321:715 322:716 323:717 324:718 325:719 326:720 327:721 328:721 329:722 330:723 331:724 332:725 333:726 334:727 335:727 336:728 337:729 338:730 339:731 340:732 341:733 342:734 343:735 344:735 345:735 346:735 347:736 348:737 349:738 350:739 351:740 352:741 353:741 354:742 355:743 356:744 357:745 358:746 359:747 360:747\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 123\n",
            "INFO:tensorflow:end_position: 139\n",
            "INFO:tensorflow:answer: the bottom household _ type for sales between jul 2015 and jun 2017 was this . .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000065\n",
            "INFO:tensorflow:example_index: 13\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by quantity [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:12 23:13 24:14 25:15 26:16 27:17 28:18 29:19 30:20 31:21 32:22 33:23 34:24 35:25 36:25 37:26 38:27 39:28 40:29 41:30 42:31 43:32 44:33 45:34 46:35 47:36 48:37 49:38 50:38 51:39 52:40 53:41 54:41 55:41 56:42 57:43 58:44 59:45 60:46 61:47 62:48 63:49 64:50 65:51 66:51 67:52 68:53 69:54 70:55 71:56 72:57 73:58 74:59 75:60 76:61 77:62 78:63 79:64 80:64 81:65 82:66 83:67 84:67 85:67 86:68 87:69 88:70 89:71 90:72 91:73 92:74 93:75 94:76 95:77 96:77 97:78 98:79 99:80 100:80 101:80 102:81 103:82 104:83 105:84 106:85 107:86 108:87 109:88 110:89 111:90 112:90 113:90 114:91 115:92 116:93 117:94 118:95 119:96 120:97 121:98 122:99 123:100 124:101 125:102 126:103 127:103 128:104 129:105 130:106 131:107 132:108 133:109 134:110 135:111 136:112 137:113 138:114 139:115 140:116 141:116 142:117 143:118 144:119 145:120 146:121 147:122 148:123 149:124 150:125 151:126 152:127 153:128 154:129 155:129 156:130 157:131 158:132 159:132 160:132 161:133 162:134 163:135 164:136 165:137 166:138 167:139 168:140 169:141 170:142 171:142 172:143 173:144 174:145 175:146 176:147 177:148 178:149 179:150 180:151 181:152 182:153 183:154 184:155 185:155 186:156 187:157 188:158 189:158 190:158 191:159 192:160 193:161 194:162 195:163 196:164 197:165 198:166 199:167 200:168 201:168 202:169 203:170 204:171 205:171 206:171 207:172 208:173 209:174 210:175 211:176 212:177 213:178 214:179 215:180 216:181 217:181 218:181 219:182 220:183 221:184 222:185 223:186 224:187 225:188 226:189 227:190 228:191 229:192 230:193 231:194 232:195 233:195 234:196 235:197 236:198 237:199 238:200 239:201 240:202 241:203 242:204 243:205 244:206 245:207 246:208 247:209 248:209 249:210 250:211 251:212 252:213 253:214 254:215 255:216 256:217 257:218 258:219 259:220 260:221 261:222 262:223 263:223 264:224 265:225 266:226 267:227 268:228 269:229 270:230 271:231 272:232 273:233 274:234 275:235 276:236 277:237 278:237 279:237 280:237 281:238 282:239 283:240 284:241 285:242 286:243 287:244 288:245 289:246 290:247 291:248 292:249 293:250 294:251 295:251 296:252 297:253 298:254 299:255 300:256 301:257 302:258 303:259 304:260 305:261 306:262 307:263 308:264 309:265 310:265 311:265 312:265 313:266 314:267 315:268 316:269 317:270 318:271 319:272 320:273 321:274 322:275 323:276 324:277 325:277 326:278 327:279 328:280 329:281 330:282 331:283 332:283 333:284 334:285 335:286 336:287 337:288 338:289 339:290 340:291 341:291 342:292 343:293 344:294 345:295 346:296 347:297 348:297 349:298 350:299 351:300 352:301 353:302 354:303 355:304 356:305 357:305 358:306 359:307 360:308 361:309 362:310 363:311 364:311 365:312 366:313 367:314 368:315 369:316 370:317 371:318 372:319 373:319 374:320 375:321 376:322 377:323 378:324 379:325 380:325 381:326 382:327\n",
            "INFO:tensorflow:token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 11712 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 142\n",
            "INFO:tensorflow:end_position: 155\n",
            "INFO:tensorflow:answer: the bottom commodity for quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000066\n",
            "INFO:tensorflow:example_index: 13\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by quantity [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:113 10:114 11:115 12:116 13:116 14:117 15:118 16:119 17:120 18:121 19:122 20:123 21:124 22:125 23:126 24:127 25:128 26:129 27:129 28:130 29:131 30:132 31:132 32:132 33:133 34:134 35:135 36:136 37:137 38:138 39:139 40:140 41:141 42:142 43:142 44:143 45:144 46:145 47:146 48:147 49:148 50:149 51:150 52:151 53:152 54:153 55:154 56:155 57:155 58:156 59:157 60:158 61:158 62:158 63:159 64:160 65:161 66:162 67:163 68:164 69:165 70:166 71:167 72:168 73:168 74:169 75:170 76:171 77:171 78:171 79:172 80:173 81:174 82:175 83:176 84:177 85:178 86:179 87:180 88:181 89:181 90:181 91:182 92:183 93:184 94:185 95:186 96:187 97:188 98:189 99:190 100:191 101:192 102:193 103:194 104:195 105:195 106:196 107:197 108:198 109:199 110:200 111:201 112:202 113:203 114:204 115:205 116:206 117:207 118:208 119:209 120:209 121:210 122:211 123:212 124:213 125:214 126:215 127:216 128:217 129:218 130:219 131:220 132:221 133:222 134:223 135:223 136:224 137:225 138:226 139:227 140:228 141:229 142:230 143:231 144:232 145:233 146:234 147:235 148:236 149:237 150:237 151:237 152:237 153:238 154:239 155:240 156:241 157:242 158:243 159:244 160:245 161:246 162:247 163:248 164:249 165:250 166:251 167:251 168:252 169:253 170:254 171:255 172:256 173:257 174:258 175:259 176:260 177:261 178:262 179:263 180:264 181:265 182:265 183:265 184:265 185:266 186:267 187:268 188:269 189:270 190:271 191:272 192:273 193:274 194:275 195:276 196:277 197:277 198:278 199:279 200:280 201:281 202:282 203:283 204:283 205:284 206:285 207:286 208:287 209:288 210:289 211:290 212:291 213:291 214:292 215:293 216:294 217:295 218:296 219:297 220:297 221:298 222:299 223:300 224:301 225:302 226:303 227:304 228:305 229:305 230:306 231:307 232:308 233:309 234:310 235:311 236:311 237:312 238:313 239:314 240:315 241:316 242:317 243:318 244:319 245:319 246:320 247:321 248:322 249:323 250:324 251:325 252:325 253:326 254:327 255:328 256:329 257:330 258:331 259:332 260:333 261:333 262:333 263:333 264:334 265:335 266:336 267:337 268:338 269:339 270:339 271:340 272:341 273:342 274:343 275:344 276:345 277:346 278:347 279:347 280:348 281:349 282:350 283:351 284:352 285:353 286:353 287:354 288:355 289:356 290:357 291:358 292:359 293:360 294:361 295:361 296:361 297:361 298:362 299:363 300:364 301:365 302:366 303:367 304:367 305:368 306:369 307:370 308:371 309:372 310:373 311:373 312:374 313:375 314:376 315:377 316:378 317:379 318:380 319:381 320:382 321:383 322:384 323:385 324:386 325:386 326:387 327:388 328:389 329:390 330:391 331:392 332:393 333:394 334:395 335:396 336:397 337:398 338:399 339:399 340:400 341:401 342:402 343:403 344:404 345:405 346:406 347:407 348:408 349:409 350:410 351:411 352:412 353:412 354:413 355:414 356:415 357:415 358:415 359:416 360:417 361:418 362:419 363:420 364:421 365:422 366:423 367:424 368:425 369:425 370:426 371:427 372:428 373:429 374:430 375:431 376:432 377:433 378:434 379:435 380:436 381:437 382:438\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 11712 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 14\n",
            "INFO:tensorflow:end_position: 27\n",
            "INFO:tensorflow:answer: the bottom commodity for quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000067\n",
            "INFO:tensorflow:example_index: 13\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by quantity [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:225 10:226 11:227 12:228 13:229 14:230 15:231 16:232 17:233 18:234 19:235 20:236 21:237 22:237 23:237 24:237 25:238 26:239 27:240 28:241 29:242 30:243 31:244 32:245 33:246 34:247 35:248 36:249 37:250 38:251 39:251 40:252 41:253 42:254 43:255 44:256 45:257 46:258 47:259 48:260 49:261 50:262 51:263 52:264 53:265 54:265 55:265 56:265 57:266 58:267 59:268 60:269 61:270 62:271 63:272 64:273 65:274 66:275 67:276 68:277 69:277 70:278 71:279 72:280 73:281 74:282 75:283 76:283 77:284 78:285 79:286 80:287 81:288 82:289 83:290 84:291 85:291 86:292 87:293 88:294 89:295 90:296 91:297 92:297 93:298 94:299 95:300 96:301 97:302 98:303 99:304 100:305 101:305 102:306 103:307 104:308 105:309 106:310 107:311 108:311 109:312 110:313 111:314 112:315 113:316 114:317 115:318 116:319 117:319 118:320 119:321 120:322 121:323 122:324 123:325 124:325 125:326 126:327 127:328 128:329 129:330 130:331 131:332 132:333 133:333 134:333 135:333 136:334 137:335 138:336 139:337 140:338 141:339 142:339 143:340 144:341 145:342 146:343 147:344 148:345 149:346 150:347 151:347 152:348 153:349 154:350 155:351 156:352 157:353 158:353 159:354 160:355 161:356 162:357 163:358 164:359 165:360 166:361 167:361 168:361 169:361 170:362 171:363 172:364 173:365 174:366 175:367 176:367 177:368 178:369 179:370 180:371 181:372 182:373 183:373 184:374 185:375 186:376 187:377 188:378 189:379 190:380 191:381 192:382 193:383 194:384 195:385 196:386 197:386 198:387 199:388 200:389 201:390 202:391 203:392 204:393 205:394 206:395 207:396 208:397 209:398 210:399 211:399 212:400 213:401 214:402 215:403 216:404 217:405 218:406 219:407 220:408 221:409 222:410 223:411 224:412 225:412 226:413 227:414 228:415 229:415 230:415 231:416 232:417 233:418 234:419 235:420 236:421 237:422 238:423 239:424 240:425 241:425 242:426 243:427 244:428 245:429 246:430 247:431 248:432 249:433 250:434 251:435 252:436 253:437 254:438 255:438 256:439 257:440 258:441 259:441 260:441 261:442 262:443 263:444 264:445 265:446 266:447 267:448 268:449 269:450 270:451 271:451 272:452 273:453 274:454 275:454 276:454 277:455 278:456 279:457 280:458 281:459 282:460 283:461 284:462 285:463 286:464 287:464 288:464 289:465 290:466 291:467 292:468 293:469 294:470 295:471 296:472 297:473 298:474 299:475 300:476 301:477 302:477 303:478 304:479 305:480 306:481 307:482 308:483 309:484 310:485 311:486 312:487 313:488 314:489 315:490 316:490 317:491 318:492 319:493 320:494 321:495 322:496 323:497 324:498 325:499 326:500 327:501 328:502 329:503 330:503 331:504 332:505 333:506 334:506 335:506 336:507 337:508 338:509 339:510 340:511 341:512 342:513 343:514 344:515 345:516 346:516 347:517 348:518 349:519 350:520 351:521 352:522 353:523 354:524 355:525 356:526 357:527 358:528 359:529 360:529 361:530 362:531 363:532 364:532 365:532 366:533 367:534 368:535 369:536 370:537 371:538 372:539 373:540 374:541 375:542 376:542 377:543 378:544 379:545 380:545 381:545 382:546\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 11712 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000068\n",
            "INFO:tensorflow:example_index: 13\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by quantity [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:335 10:336 11:337 12:338 13:339 14:339 15:340 16:341 17:342 18:343 19:344 20:345 21:346 22:347 23:347 24:348 25:349 26:350 27:351 28:352 29:353 30:353 31:354 32:355 33:356 34:357 35:358 36:359 37:360 38:361 39:361 40:361 41:361 42:362 43:363 44:364 45:365 46:366 47:367 48:367 49:368 50:369 51:370 52:371 53:372 54:373 55:373 56:374 57:375 58:376 59:377 60:378 61:379 62:380 63:381 64:382 65:383 66:384 67:385 68:386 69:386 70:387 71:388 72:389 73:390 74:391 75:392 76:393 77:394 78:395 79:396 80:397 81:398 82:399 83:399 84:400 85:401 86:402 87:403 88:404 89:405 90:406 91:407 92:408 93:409 94:410 95:411 96:412 97:412 98:413 99:414 100:415 101:415 102:415 103:416 104:417 105:418 106:419 107:420 108:421 109:422 110:423 111:424 112:425 113:425 114:426 115:427 116:428 117:429 118:430 119:431 120:432 121:433 122:434 123:435 124:436 125:437 126:438 127:438 128:439 129:440 130:441 131:441 132:441 133:442 134:443 135:444 136:445 137:446 138:447 139:448 140:449 141:450 142:451 143:451 144:452 145:453 146:454 147:454 148:454 149:455 150:456 151:457 152:458 153:459 154:460 155:461 156:462 157:463 158:464 159:464 160:464 161:465 162:466 163:467 164:468 165:469 166:470 167:471 168:472 169:473 170:474 171:475 172:476 173:477 174:477 175:478 176:479 177:480 178:481 179:482 180:483 181:484 182:485 183:486 184:487 185:488 186:489 187:490 188:490 189:491 190:492 191:493 192:494 193:495 194:496 195:497 196:498 197:499 198:500 199:501 200:502 201:503 202:503 203:504 204:505 205:506 206:506 207:506 208:507 209:508 210:509 211:510 212:511 213:512 214:513 215:514 216:515 217:516 218:516 219:517 220:518 221:519 222:520 223:521 224:522 225:523 226:524 227:525 228:526 229:527 230:528 231:529 232:529 233:530 234:531 235:532 236:532 237:532 238:533 239:534 240:535 241:536 242:537 243:538 244:539 245:540 246:541 247:542 248:542 249:543 250:544 251:545 252:545 253:545 254:546 255:547 256:548 257:549 258:550 259:551 260:552 261:553 262:554 263:555 264:555 265:555 266:556 267:557 268:558 269:559 270:560 271:561 272:562 273:563 274:564 275:565 276:566 277:567 278:568 279:569 280:569 281:570 282:571 283:572 284:573 285:574 286:575 287:576 288:577 289:578 290:579 291:580 292:581 293:582 294:583 295:583 296:584 297:585 298:586 299:587 300:588 301:589 302:590 303:591 304:592 305:593 306:594 307:595 308:596 309:597 310:597 311:598 312:599 313:600 314:601 315:602 316:603 317:604 318:605 319:606 320:607 321:608 322:609 323:610 324:611 325:611 326:611 327:611 328:612 329:613 330:614 331:615 332:616 333:617 334:618 335:619 336:620 337:621 338:622 339:623 340:624 341:625 342:625 343:626 344:627 345:628 346:629 347:630 348:631 349:632 350:633 351:634 352:635 353:636 354:637 355:638 356:639 357:639 358:639 359:639 360:640 361:641 362:642 363:643 364:644 365:645 366:646 367:647 368:648 369:649 370:650 371:651 372:651 373:652 374:653 375:654 376:655 377:656 378:657 379:657 380:658 381:659 382:660\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 11712 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000069\n",
            "INFO:tensorflow:example_index: 13\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom commodity by quantity [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 9:446 10:447 11:448 12:449 13:450 14:451 15:451 16:452 17:453 18:454 19:454 20:454 21:455 22:456 23:457 24:458 25:459 26:460 27:461 28:462 29:463 30:464 31:464 32:464 33:465 34:466 35:467 36:468 37:469 38:470 39:471 40:472 41:473 42:474 43:475 44:476 45:477 46:477 47:478 48:479 49:480 50:481 51:482 52:483 53:484 54:485 55:486 56:487 57:488 58:489 59:490 60:490 61:491 62:492 63:493 64:494 65:495 66:496 67:497 68:498 69:499 70:500 71:501 72:502 73:503 74:503 75:504 76:505 77:506 78:506 79:506 80:507 81:508 82:509 83:510 84:511 85:512 86:513 87:514 88:515 89:516 90:516 91:517 92:518 93:519 94:520 95:521 96:522 97:523 98:524 99:525 100:526 101:527 102:528 103:529 104:529 105:530 106:531 107:532 108:532 109:532 110:533 111:534 112:535 113:536 114:537 115:538 116:539 117:540 118:541 119:542 120:542 121:543 122:544 123:545 124:545 125:545 126:546 127:547 128:548 129:549 130:550 131:551 132:552 133:553 134:554 135:555 136:555 137:555 138:556 139:557 140:558 141:559 142:560 143:561 144:562 145:563 146:564 147:565 148:566 149:567 150:568 151:569 152:569 153:570 154:571 155:572 156:573 157:574 158:575 159:576 160:577 161:578 162:579 163:580 164:581 165:582 166:583 167:583 168:584 169:585 170:586 171:587 172:588 173:589 174:590 175:591 176:592 177:593 178:594 179:595 180:596 181:597 182:597 183:598 184:599 185:600 186:601 187:602 188:603 189:604 190:605 191:606 192:607 193:608 194:609 195:610 196:611 197:611 198:611 199:611 200:612 201:613 202:614 203:615 204:616 205:617 206:618 207:619 208:620 209:621 210:622 211:623 212:624 213:625 214:625 215:626 216:627 217:628 218:629 219:630 220:631 221:632 222:633 223:634 224:635 225:636 226:637 227:638 228:639 229:639 230:639 231:639 232:640 233:641 234:642 235:643 236:644 237:645 238:646 239:647 240:648 241:649 242:650 243:651 244:651 245:652 246:653 247:654 248:655 249:656 250:657 251:657 252:658 253:659 254:660 255:661 256:662 257:663 258:664 259:665 260:665 261:666 262:667 263:668 264:669 265:670 266:671 267:671 268:672 269:673 270:674 271:675 272:676 273:677 274:678 275:679 276:679 277:680 278:681 279:682 280:683 281:684 282:685 283:685 284:686 285:687 286:688 287:689 288:690 289:691 290:692 291:693 292:693 293:694 294:695 295:696 296:697 297:698 298:699 299:699 300:700 301:701 302:702 303:703 304:704 305:705 306:706 307:707 308:707 309:707 310:707 311:708 312:709 313:710 314:711 315:712 316:713 317:713 318:714 319:715 320:716 321:717 322:718 323:719 324:720 325:721 326:721 327:722 328:723 329:724 330:725 331:726 332:727 333:727 334:728 335:729 336:730 337:731 338:732 339:733 340:734 341:735 342:735 343:735 344:735 345:736 346:737 347:738 348:739 349:740 350:741 351:741 352:742 353:743 354:744 355:745 356:746 357:747 358:747\n",
            "INFO:tensorflow:token_is_max_context: 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 19502 2011 11712 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000070\n",
            "INFO:tensorflow:example_index: 14\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:7 16:8 17:9 18:10 19:11 20:12 21:12 22:13 23:14 24:15 25:16 26:17 27:18 28:19 29:20 30:21 31:22 32:23 33:24 34:25 35:25 36:26 37:27 38:28 39:29 40:30 41:31 42:32 43:33 44:34 45:35 46:36 47:37 48:38 49:38 50:39 51:40 52:41 53:41 54:41 55:42 56:43 57:44 58:45 59:46 60:47 61:48 62:49 63:50 64:51 65:51 66:52 67:53 68:54 69:55 70:56 71:57 72:58 73:59 74:60 75:61 76:62 77:63 78:64 79:64 80:65 81:66 82:67 83:67 84:67 85:68 86:69 87:70 88:71 89:72 90:73 91:74 92:75 93:76 94:77 95:77 96:78 97:79 98:80 99:80 100:80 101:81 102:82 103:83 104:84 105:85 106:86 107:87 108:88 109:89 110:90 111:90 112:90 113:91 114:92 115:93 116:94 117:95 118:96 119:97 120:98 121:99 122:100 123:101 124:102 125:103 126:103 127:104 128:105 129:106 130:107 131:108 132:109 133:110 134:111 135:112 136:113 137:114 138:115 139:116 140:116 141:117 142:118 143:119 144:120 145:121 146:122 147:123 148:124 149:125 150:126 151:127 152:128 153:129 154:129 155:130 156:131 157:132 158:132 159:132 160:133 161:134 162:135 163:136 164:137 165:138 166:139 167:140 168:141 169:142 170:142 171:143 172:144 173:145 174:146 175:147 176:148 177:149 178:150 179:151 180:152 181:153 182:154 183:155 184:155 185:156 186:157 187:158 188:158 189:158 190:159 191:160 192:161 193:162 194:163 195:164 196:165 197:166 198:167 199:168 200:168 201:169 202:170 203:171 204:171 205:171 206:172 207:173 208:174 209:175 210:176 211:177 212:178 213:179 214:180 215:181 216:181 217:181 218:182 219:183 220:184 221:185 222:186 223:187 224:188 225:189 226:190 227:191 228:192 229:193 230:194 231:195 232:195 233:196 234:197 235:198 236:199 237:200 238:201 239:202 240:203 241:204 242:205 243:206 244:207 245:208 246:209 247:209 248:210 249:211 250:212 251:213 252:214 253:215 254:216 255:217 256:218 257:219 258:220 259:221 260:222 261:223 262:223 263:224 264:225 265:226 266:227 267:228 268:229 269:230 270:231 271:232 272:233 273:234 274:235 275:236 276:237 277:237 278:237 279:237 280:238 281:239 282:240 283:241 284:242 285:243 286:244 287:245 288:246 289:247 290:248 291:249 292:250 293:251 294:251 295:252 296:253 297:254 298:255 299:256 300:257 301:258 302:259 303:260 304:261 305:262 306:263 307:264 308:265 309:265 310:265 311:265 312:266 313:267 314:268 315:269 316:270 317:271 318:272 319:273 320:274 321:275 322:276 323:277 324:277 325:278 326:279 327:280 328:281 329:282 330:283 331:283 332:284 333:285 334:286 335:287 336:288 337:289 338:290 339:291 340:291 341:292 342:293 343:294 344:295 345:296 346:297 347:297 348:298 349:299 350:300 351:301 352:302 353:303 354:304 355:305 356:305 357:306 358:307 359:308 360:309 361:310 362:311 363:311 364:312 365:313 366:314 367:315 368:316 369:317 370:318 371:319 372:319 373:320 374:321 375:322 376:323 377:324 378:325 379:325 380:326 381:327 382:328\n",
            "INFO:tensorflow:token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000071\n",
            "INFO:tensorflow:example_index: 14\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:113 9:114 10:115 11:116 12:116 13:117 14:118 15:119 16:120 17:121 18:122 19:123 20:124 21:125 22:126 23:127 24:128 25:129 26:129 27:130 28:131 29:132 30:132 31:132 32:133 33:134 34:135 35:136 36:137 37:138 38:139 39:140 40:141 41:142 42:142 43:143 44:144 45:145 46:146 47:147 48:148 49:149 50:150 51:151 52:152 53:153 54:154 55:155 56:155 57:156 58:157 59:158 60:158 61:158 62:159 63:160 64:161 65:162 66:163 67:164 68:165 69:166 70:167 71:168 72:168 73:169 74:170 75:171 76:171 77:171 78:172 79:173 80:174 81:175 82:176 83:177 84:178 85:179 86:180 87:181 88:181 89:181 90:182 91:183 92:184 93:185 94:186 95:187 96:188 97:189 98:190 99:191 100:192 101:193 102:194 103:195 104:195 105:196 106:197 107:198 108:199 109:200 110:201 111:202 112:203 113:204 114:205 115:206 116:207 117:208 118:209 119:209 120:210 121:211 122:212 123:213 124:214 125:215 126:216 127:217 128:218 129:219 130:220 131:221 132:222 133:223 134:223 135:224 136:225 137:226 138:227 139:228 140:229 141:230 142:231 143:232 144:233 145:234 146:235 147:236 148:237 149:237 150:237 151:237 152:238 153:239 154:240 155:241 156:242 157:243 158:244 159:245 160:246 161:247 162:248 163:249 164:250 165:251 166:251 167:252 168:253 169:254 170:255 171:256 172:257 173:258 174:259 175:260 176:261 177:262 178:263 179:264 180:265 181:265 182:265 183:265 184:266 185:267 186:268 187:269 188:270 189:271 190:272 191:273 192:274 193:275 194:276 195:277 196:277 197:278 198:279 199:280 200:281 201:282 202:283 203:283 204:284 205:285 206:286 207:287 208:288 209:289 210:290 211:291 212:291 213:292 214:293 215:294 216:295 217:296 218:297 219:297 220:298 221:299 222:300 223:301 224:302 225:303 226:304 227:305 228:305 229:306 230:307 231:308 232:309 233:310 234:311 235:311 236:312 237:313 238:314 239:315 240:316 241:317 242:318 243:319 244:319 245:320 246:321 247:322 248:323 249:324 250:325 251:325 252:326 253:327 254:328 255:329 256:330 257:331 258:332 259:333 260:333 261:333 262:333 263:334 264:335 265:336 266:337 267:338 268:339 269:339 270:340 271:341 272:342 273:343 274:344 275:345 276:346 277:347 278:347 279:348 280:349 281:350 282:351 283:352 284:353 285:353 286:354 287:355 288:356 289:357 290:358 291:359 292:360 293:361 294:361 295:361 296:361 297:362 298:363 299:364 300:365 301:366 302:367 303:367 304:368 305:369 306:370 307:371 308:372 309:373 310:373 311:374 312:375 313:376 314:377 315:378 316:379 317:380 318:381 319:382 320:383 321:384 322:385 323:386 324:386 325:387 326:388 327:389 328:390 329:391 330:392 331:393 332:394 333:395 334:396 335:397 336:398 337:399 338:399 339:400 340:401 341:402 342:403 343:404 344:405 345:406 346:407 347:408 348:409 349:410 350:411 351:412 352:412 353:413 354:414 355:415 356:415 357:415 358:416 359:417 360:418 361:419 362:420 363:421 364:422 365:423 366:424 367:425 368:425 369:426 370:427 371:428 372:429 373:430 374:431 375:432 376:433 377:434 378:435 379:436 380:437 381:438 382:438\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000072\n",
            "INFO:tensorflow:example_index: 14\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:225 9:226 10:227 11:228 12:229 13:230 14:231 15:232 16:233 17:234 18:235 19:236 20:237 21:237 22:237 23:237 24:238 25:239 26:240 27:241 28:242 29:243 30:244 31:245 32:246 33:247 34:248 35:249 36:250 37:251 38:251 39:252 40:253 41:254 42:255 43:256 44:257 45:258 46:259 47:260 48:261 49:262 50:263 51:264 52:265 53:265 54:265 55:265 56:266 57:267 58:268 59:269 60:270 61:271 62:272 63:273 64:274 65:275 66:276 67:277 68:277 69:278 70:279 71:280 72:281 73:282 74:283 75:283 76:284 77:285 78:286 79:287 80:288 81:289 82:290 83:291 84:291 85:292 86:293 87:294 88:295 89:296 90:297 91:297 92:298 93:299 94:300 95:301 96:302 97:303 98:304 99:305 100:305 101:306 102:307 103:308 104:309 105:310 106:311 107:311 108:312 109:313 110:314 111:315 112:316 113:317 114:318 115:319 116:319 117:320 118:321 119:322 120:323 121:324 122:325 123:325 124:326 125:327 126:328 127:329 128:330 129:331 130:332 131:333 132:333 133:333 134:333 135:334 136:335 137:336 138:337 139:338 140:339 141:339 142:340 143:341 144:342 145:343 146:344 147:345 148:346 149:347 150:347 151:348 152:349 153:350 154:351 155:352 156:353 157:353 158:354 159:355 160:356 161:357 162:358 163:359 164:360 165:361 166:361 167:361 168:361 169:362 170:363 171:364 172:365 173:366 174:367 175:367 176:368 177:369 178:370 179:371 180:372 181:373 182:373 183:374 184:375 185:376 186:377 187:378 188:379 189:380 190:381 191:382 192:383 193:384 194:385 195:386 196:386 197:387 198:388 199:389 200:390 201:391 202:392 203:393 204:394 205:395 206:396 207:397 208:398 209:399 210:399 211:400 212:401 213:402 214:403 215:404 216:405 217:406 218:407 219:408 220:409 221:410 222:411 223:412 224:412 225:413 226:414 227:415 228:415 229:415 230:416 231:417 232:418 233:419 234:420 235:421 236:422 237:423 238:424 239:425 240:425 241:426 242:427 243:428 244:429 245:430 246:431 247:432 248:433 249:434 250:435 251:436 252:437 253:438 254:438 255:439 256:440 257:441 258:441 259:441 260:442 261:443 262:444 263:445 264:446 265:447 266:448 267:449 268:450 269:451 270:451 271:452 272:453 273:454 274:454 275:454 276:455 277:456 278:457 279:458 280:459 281:460 282:461 283:462 284:463 285:464 286:464 287:464 288:465 289:466 290:467 291:468 292:469 293:470 294:471 295:472 296:473 297:474 298:475 299:476 300:477 301:477 302:478 303:479 304:480 305:481 306:482 307:483 308:484 309:485 310:486 311:487 312:488 313:489 314:490 315:490 316:491 317:492 318:493 319:494 320:495 321:496 322:497 323:498 324:499 325:500 326:501 327:502 328:503 329:503 330:504 331:505 332:506 333:506 334:506 335:507 336:508 337:509 338:510 339:511 340:512 341:513 342:514 343:515 344:516 345:516 346:517 347:518 348:519 349:520 350:521 351:522 352:523 353:524 354:525 355:526 356:527 357:528 358:529 359:529 360:530 361:531 362:532 363:532 364:532 365:533 366:534 367:535 368:536 369:537 370:538 371:539 372:540 373:541 374:542 375:542 376:543 377:544 378:545 379:545 380:545 381:546 382:547\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000073\n",
            "INFO:tensorflow:example_index: 14\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:335 9:336 10:337 11:338 12:339 13:339 14:340 15:341 16:342 17:343 18:344 19:345 20:346 21:347 22:347 23:348 24:349 25:350 26:351 27:352 28:353 29:353 30:354 31:355 32:356 33:357 34:358 35:359 36:360 37:361 38:361 39:361 40:361 41:362 42:363 43:364 44:365 45:366 46:367 47:367 48:368 49:369 50:370 51:371 52:372 53:373 54:373 55:374 56:375 57:376 58:377 59:378 60:379 61:380 62:381 63:382 64:383 65:384 66:385 67:386 68:386 69:387 70:388 71:389 72:390 73:391 74:392 75:393 76:394 77:395 78:396 79:397 80:398 81:399 82:399 83:400 84:401 85:402 86:403 87:404 88:405 89:406 90:407 91:408 92:409 93:410 94:411 95:412 96:412 97:413 98:414 99:415 100:415 101:415 102:416 103:417 104:418 105:419 106:420 107:421 108:422 109:423 110:424 111:425 112:425 113:426 114:427 115:428 116:429 117:430 118:431 119:432 120:433 121:434 122:435 123:436 124:437 125:438 126:438 127:439 128:440 129:441 130:441 131:441 132:442 133:443 134:444 135:445 136:446 137:447 138:448 139:449 140:450 141:451 142:451 143:452 144:453 145:454 146:454 147:454 148:455 149:456 150:457 151:458 152:459 153:460 154:461 155:462 156:463 157:464 158:464 159:464 160:465 161:466 162:467 163:468 164:469 165:470 166:471 167:472 168:473 169:474 170:475 171:476 172:477 173:477 174:478 175:479 176:480 177:481 178:482 179:483 180:484 181:485 182:486 183:487 184:488 185:489 186:490 187:490 188:491 189:492 190:493 191:494 192:495 193:496 194:497 195:498 196:499 197:500 198:501 199:502 200:503 201:503 202:504 203:505 204:506 205:506 206:506 207:507 208:508 209:509 210:510 211:511 212:512 213:513 214:514 215:515 216:516 217:516 218:517 219:518 220:519 221:520 222:521 223:522 224:523 225:524 226:525 227:526 228:527 229:528 230:529 231:529 232:530 233:531 234:532 235:532 236:532 237:533 238:534 239:535 240:536 241:537 242:538 243:539 244:540 245:541 246:542 247:542 248:543 249:544 250:545 251:545 252:545 253:546 254:547 255:548 256:549 257:550 258:551 259:552 260:553 261:554 262:555 263:555 264:555 265:556 266:557 267:558 268:559 269:560 270:561 271:562 272:563 273:564 274:565 275:566 276:567 277:568 278:569 279:569 280:570 281:571 282:572 283:573 284:574 285:575 286:576 287:577 288:578 289:579 290:580 291:581 292:582 293:583 294:583 295:584 296:585 297:586 298:587 299:588 300:589 301:590 302:591 303:592 304:593 305:594 306:595 307:596 308:597 309:597 310:598 311:599 312:600 313:601 314:602 315:603 316:604 317:605 318:606 319:607 320:608 321:609 322:610 323:611 324:611 325:611 326:611 327:612 328:613 329:614 330:615 331:616 332:617 333:618 334:619 335:620 336:621 337:622 338:623 339:624 340:625 341:625 342:626 343:627 344:628 345:629 346:630 347:631 348:632 349:633 350:634 351:635 352:636 353:637 354:638 355:639 356:639 357:639 358:639 359:640 360:641 361:642 362:643 363:644 364:645 365:646 366:647 367:648 368:649 369:650 370:651 371:651 372:652 373:653 374:654 375:655 376:656 377:657 378:657 379:658 380:659 381:660 382:661\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 359\n",
            "INFO:tensorflow:end_position: 371\n",
            "INFO:tensorflow:answer: the breakdown of sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000074\n",
            "INFO:tensorflow:example_index: 14\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:446 9:447 10:448 11:449 12:450 13:451 14:451 15:452 16:453 17:454 18:454 19:454 20:455 21:456 22:457 23:458 24:459 25:460 26:461 27:462 28:463 29:464 30:464 31:464 32:465 33:466 34:467 35:468 36:469 37:470 38:471 39:472 40:473 41:474 42:475 43:476 44:477 45:477 46:478 47:479 48:480 49:481 50:482 51:483 52:484 53:485 54:486 55:487 56:488 57:489 58:490 59:490 60:491 61:492 62:493 63:494 64:495 65:496 66:497 67:498 68:499 69:500 70:501 71:502 72:503 73:503 74:504 75:505 76:506 77:506 78:506 79:507 80:508 81:509 82:510 83:511 84:512 85:513 86:514 87:515 88:516 89:516 90:517 91:518 92:519 93:520 94:521 95:522 96:523 97:524 98:525 99:526 100:527 101:528 102:529 103:529 104:530 105:531 106:532 107:532 108:532 109:533 110:534 111:535 112:536 113:537 114:538 115:539 116:540 117:541 118:542 119:542 120:543 121:544 122:545 123:545 124:545 125:546 126:547 127:548 128:549 129:550 130:551 131:552 132:553 133:554 134:555 135:555 136:555 137:556 138:557 139:558 140:559 141:560 142:561 143:562 144:563 145:564 146:565 147:566 148:567 149:568 150:569 151:569 152:570 153:571 154:572 155:573 156:574 157:575 158:576 159:577 160:578 161:579 162:580 163:581 164:582 165:583 166:583 167:584 168:585 169:586 170:587 171:588 172:589 173:590 174:591 175:592 176:593 177:594 178:595 179:596 180:597 181:597 182:598 183:599 184:600 185:601 186:602 187:603 188:604 189:605 190:606 191:607 192:608 193:609 194:610 195:611 196:611 197:611 198:611 199:612 200:613 201:614 202:615 203:616 204:617 205:618 206:619 207:620 208:621 209:622 210:623 211:624 212:625 213:625 214:626 215:627 216:628 217:629 218:630 219:631 220:632 221:633 222:634 223:635 224:636 225:637 226:638 227:639 228:639 229:639 230:639 231:640 232:641 233:642 234:643 235:644 236:645 237:646 238:647 239:648 240:649 241:650 242:651 243:651 244:652 245:653 246:654 247:655 248:656 249:657 250:657 251:658 252:659 253:660 254:661 255:662 256:663 257:664 258:665 259:665 260:666 261:667 262:668 263:669 264:670 265:671 266:671 267:672 268:673 269:674 270:675 271:676 272:677 273:678 274:679 275:679 276:680 277:681 278:682 279:683 280:684 281:685 282:685 283:686 284:687 285:688 286:689 287:690 288:691 289:692 290:693 291:693 292:694 293:695 294:696 295:697 296:698 297:699 298:699 299:700 300:701 301:702 302:703 303:704 304:705 305:706 306:707 307:707 308:707 309:707 310:708 311:709 312:710 313:711 314:712 315:713 316:713 317:714 318:715 319:716 320:717 321:718 322:719 323:720 324:721 325:721 326:722 327:723 328:724 329:725 330:726 331:727 332:727 333:728 334:729 335:730 336:731 337:732 338:733 339:734 340:735 341:735 342:735 343:735 344:736 345:737 346:738 347:739 348:740 349:741 350:741 351:742 352:743 353:744 354:745 355:746 356:747 357:747\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 231\n",
            "INFO:tensorflow:end_position: 243\n",
            "INFO:tensorflow:answer: the breakdown of sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000075\n",
            "INFO:tensorflow:example_index: 15\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the top household _ type by sales [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:19 32:20 33:21 34:22 35:23 36:24 37:25 38:25 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:36 50:37 51:38 52:38 53:39 54:40 55:41 56:41 57:41 58:42 59:43 60:44 61:45 62:46 63:47 64:48 65:49 66:50 67:51 68:51 69:52 70:53 71:54 72:55 73:56 74:57 75:58 76:59 77:60 78:61 79:62 80:63 81:64 82:64 83:65 84:66 85:67 86:67 87:67 88:68 89:69 90:70 91:71 92:72 93:73 94:74 95:75 96:76 97:77 98:77 99:78 100:79 101:80 102:80 103:80 104:81 105:82 106:83 107:84 108:85 109:86 110:87 111:88 112:89 113:90 114:90 115:90 116:91 117:92 118:93 119:94 120:95 121:96 122:97 123:98 124:99 125:100 126:101 127:102 128:103 129:103 130:104 131:105 132:106 133:107 134:108 135:109 136:110 137:111 138:112 139:113 140:114 141:115 142:116 143:116 144:117 145:118 146:119 147:120 148:121 149:122 150:123 151:124 152:125 153:126 154:127 155:128 156:129 157:129 158:130 159:131 160:132 161:132 162:132 163:133 164:134 165:135 166:136 167:137 168:138 169:139 170:140 171:141 172:142 173:142 174:143 175:144 176:145 177:146 178:147 179:148 180:149 181:150 182:151 183:152 184:153 185:154 186:155 187:155 188:156 189:157 190:158 191:158 192:158 193:159 194:160 195:161 196:162 197:163 198:164 199:165 200:166 201:167 202:168 203:168 204:169 205:170 206:171 207:171 208:171 209:172 210:173 211:174 212:175 213:176 214:177 215:178 216:179 217:180 218:181 219:181 220:181 221:182 222:183 223:184 224:185 225:186 226:187 227:188 228:189 229:190 230:191 231:192 232:193 233:194 234:195 235:195 236:196 237:197 238:198 239:199 240:200 241:201 242:202 243:203 244:204 245:205 246:206 247:207 248:208 249:209 250:209 251:210 252:211 253:212 254:213 255:214 256:215 257:216 258:217 259:218 260:219 261:220 262:221 263:222 264:223 265:223 266:224 267:225 268:226 269:227 270:228 271:229 272:230 273:231 274:232 275:233 276:234 277:235 278:236 279:237 280:237 281:237 282:237 283:238 284:239 285:240 286:241 287:242 288:243 289:244 290:245 291:246 292:247 293:248 294:249 295:250 296:251 297:251 298:252 299:253 300:254 301:255 302:256 303:257 304:258 305:259 306:260 307:261 308:262 309:263 310:264 311:265 312:265 313:265 314:265 315:266 316:267 317:268 318:269 319:270 320:271 321:272 322:273 323:274 324:275 325:276 326:277 327:277 328:278 329:279 330:280 331:281 332:282 333:283 334:283 335:284 336:285 337:286 338:287 339:288 340:289 341:290 342:291 343:291 344:292 345:293 346:294 347:295 348:296 349:297 350:297 351:298 352:299 353:300 354:301 355:302 356:303 357:304 358:305 359:305 360:306 361:307 362:308 363:309 364:310 365:311 366:311 367:312 368:313 369:314 370:315 371:316 372:317 373:318 374:319 375:319 376:320 377:321 378:322 379:323 380:324 381:325 382:325\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4398 1035 2828 2011 4341 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000076\n",
            "INFO:tensorflow:example_index: 15\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the top household _ type by sales [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:113 12:114 13:115 14:116 15:116 16:117 17:118 18:119 19:120 20:121 21:122 22:123 23:124 24:125 25:126 26:127 27:128 28:129 29:129 30:130 31:131 32:132 33:132 34:132 35:133 36:134 37:135 38:136 39:137 40:138 41:139 42:140 43:141 44:142 45:142 46:143 47:144 48:145 49:146 50:147 51:148 52:149 53:150 54:151 55:152 56:153 57:154 58:155 59:155 60:156 61:157 62:158 63:158 64:158 65:159 66:160 67:161 68:162 69:163 70:164 71:165 72:166 73:167 74:168 75:168 76:169 77:170 78:171 79:171 80:171 81:172 82:173 83:174 84:175 85:176 86:177 87:178 88:179 89:180 90:181 91:181 92:181 93:182 94:183 95:184 96:185 97:186 98:187 99:188 100:189 101:190 102:191 103:192 104:193 105:194 106:195 107:195 108:196 109:197 110:198 111:199 112:200 113:201 114:202 115:203 116:204 117:205 118:206 119:207 120:208 121:209 122:209 123:210 124:211 125:212 126:213 127:214 128:215 129:216 130:217 131:218 132:219 133:220 134:221 135:222 136:223 137:223 138:224 139:225 140:226 141:227 142:228 143:229 144:230 145:231 146:232 147:233 148:234 149:235 150:236 151:237 152:237 153:237 154:237 155:238 156:239 157:240 158:241 159:242 160:243 161:244 162:245 163:246 164:247 165:248 166:249 167:250 168:251 169:251 170:252 171:253 172:254 173:255 174:256 175:257 176:258 177:259 178:260 179:261 180:262 181:263 182:264 183:265 184:265 185:265 186:265 187:266 188:267 189:268 190:269 191:270 192:271 193:272 194:273 195:274 196:275 197:276 198:277 199:277 200:278 201:279 202:280 203:281 204:282 205:283 206:283 207:284 208:285 209:286 210:287 211:288 212:289 213:290 214:291 215:291 216:292 217:293 218:294 219:295 220:296 221:297 222:297 223:298 224:299 225:300 226:301 227:302 228:303 229:304 230:305 231:305 232:306 233:307 234:308 235:309 236:310 237:311 238:311 239:312 240:313 241:314 242:315 243:316 244:317 245:318 246:319 247:319 248:320 249:321 250:322 251:323 252:324 253:325 254:325 255:326 256:327 257:328 258:329 259:330 260:331 261:332 262:333 263:333 264:333 265:333 266:334 267:335 268:336 269:337 270:338 271:339 272:339 273:340 274:341 275:342 276:343 277:344 278:345 279:346 280:347 281:347 282:348 283:349 284:350 285:351 286:352 287:353 288:353 289:354 290:355 291:356 292:357 293:358 294:359 295:360 296:361 297:361 298:361 299:361 300:362 301:363 302:364 303:365 304:366 305:367 306:367 307:368 308:369 309:370 310:371 311:372 312:373 313:373 314:374 315:375 316:376 317:377 318:378 319:379 320:380 321:381 322:382 323:383 324:384 325:385 326:386 327:386 328:387 329:388 330:389 331:390 332:391 333:392 334:393 335:394 336:395 337:396 338:397 339:398 340:399 341:399 342:400 343:401 344:402 345:403 346:404 347:405 348:406 349:407 350:408 351:409 352:410 353:411 354:412 355:412 356:413 357:414 358:415 359:415 360:415 361:416 362:417 363:418 364:419 365:420 366:421 367:422 368:423 369:424 370:425 371:425 372:426 373:427 374:428 375:429 376:430 377:431 378:432 379:433 380:434 381:435 382:436\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4398 1035 2828 2011 4341 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000077\n",
            "INFO:tensorflow:example_index: 15\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the top household _ type by sales [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:225 12:226 13:227 14:228 15:229 16:230 17:231 18:232 19:233 20:234 21:235 22:236 23:237 24:237 25:237 26:237 27:238 28:239 29:240 30:241 31:242 32:243 33:244 34:245 35:246 36:247 37:248 38:249 39:250 40:251 41:251 42:252 43:253 44:254 45:255 46:256 47:257 48:258 49:259 50:260 51:261 52:262 53:263 54:264 55:265 56:265 57:265 58:265 59:266 60:267 61:268 62:269 63:270 64:271 65:272 66:273 67:274 68:275 69:276 70:277 71:277 72:278 73:279 74:280 75:281 76:282 77:283 78:283 79:284 80:285 81:286 82:287 83:288 84:289 85:290 86:291 87:291 88:292 89:293 90:294 91:295 92:296 93:297 94:297 95:298 96:299 97:300 98:301 99:302 100:303 101:304 102:305 103:305 104:306 105:307 106:308 107:309 108:310 109:311 110:311 111:312 112:313 113:314 114:315 115:316 116:317 117:318 118:319 119:319 120:320 121:321 122:322 123:323 124:324 125:325 126:325 127:326 128:327 129:328 130:329 131:330 132:331 133:332 134:333 135:333 136:333 137:333 138:334 139:335 140:336 141:337 142:338 143:339 144:339 145:340 146:341 147:342 148:343 149:344 150:345 151:346 152:347 153:347 154:348 155:349 156:350 157:351 158:352 159:353 160:353 161:354 162:355 163:356 164:357 165:358 166:359 167:360 168:361 169:361 170:361 171:361 172:362 173:363 174:364 175:365 176:366 177:367 178:367 179:368 180:369 181:370 182:371 183:372 184:373 185:373 186:374 187:375 188:376 189:377 190:378 191:379 192:380 193:381 194:382 195:383 196:384 197:385 198:386 199:386 200:387 201:388 202:389 203:390 204:391 205:392 206:393 207:394 208:395 209:396 210:397 211:398 212:399 213:399 214:400 215:401 216:402 217:403 218:404 219:405 220:406 221:407 222:408 223:409 224:410 225:411 226:412 227:412 228:413 229:414 230:415 231:415 232:415 233:416 234:417 235:418 236:419 237:420 238:421 239:422 240:423 241:424 242:425 243:425 244:426 245:427 246:428 247:429 248:430 249:431 250:432 251:433 252:434 253:435 254:436 255:437 256:438 257:438 258:439 259:440 260:441 261:441 262:441 263:442 264:443 265:444 266:445 267:446 268:447 269:448 270:449 271:450 272:451 273:451 274:452 275:453 276:454 277:454 278:454 279:455 280:456 281:457 282:458 283:459 284:460 285:461 286:462 287:463 288:464 289:464 290:464 291:465 292:466 293:467 294:468 295:469 296:470 297:471 298:472 299:473 300:474 301:475 302:476 303:477 304:477 305:478 306:479 307:480 308:481 309:482 310:483 311:484 312:485 313:486 314:487 315:488 316:489 317:490 318:490 319:491 320:492 321:493 322:494 323:495 324:496 325:497 326:498 327:499 328:500 329:501 330:502 331:503 332:503 333:504 334:505 335:506 336:506 337:506 338:507 339:508 340:509 341:510 342:511 343:512 344:513 345:514 346:515 347:516 348:516 349:517 350:518 351:519 352:520 353:521 354:522 355:523 356:524 357:525 358:526 359:527 360:528 361:529 362:529 363:530 364:531 365:532 366:532 367:532 368:533 369:534 370:535 371:536 372:537 373:538 374:539 375:540 376:541 377:542 378:542 379:543 380:544 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4398 1035 2828 2011 4341 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 274\n",
            "INFO:tensorflow:end_position: 290\n",
            "INFO:tensorflow:answer: the top household _ type for sales between jul 2015 and jun 2017 was this . .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000078\n",
            "INFO:tensorflow:example_index: 15\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the top household _ type by sales [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:335 12:336 13:337 14:338 15:339 16:339 17:340 18:341 19:342 20:343 21:344 22:345 23:346 24:347 25:347 26:348 27:349 28:350 29:351 30:352 31:353 32:353 33:354 34:355 35:356 36:357 37:358 38:359 39:360 40:361 41:361 42:361 43:361 44:362 45:363 46:364 47:365 48:366 49:367 50:367 51:368 52:369 53:370 54:371 55:372 56:373 57:373 58:374 59:375 60:376 61:377 62:378 63:379 64:380 65:381 66:382 67:383 68:384 69:385 70:386 71:386 72:387 73:388 74:389 75:390 76:391 77:392 78:393 79:394 80:395 81:396 82:397 83:398 84:399 85:399 86:400 87:401 88:402 89:403 90:404 91:405 92:406 93:407 94:408 95:409 96:410 97:411 98:412 99:412 100:413 101:414 102:415 103:415 104:415 105:416 106:417 107:418 108:419 109:420 110:421 111:422 112:423 113:424 114:425 115:425 116:426 117:427 118:428 119:429 120:430 121:431 122:432 123:433 124:434 125:435 126:436 127:437 128:438 129:438 130:439 131:440 132:441 133:441 134:441 135:442 136:443 137:444 138:445 139:446 140:447 141:448 142:449 143:450 144:451 145:451 146:452 147:453 148:454 149:454 150:454 151:455 152:456 153:457 154:458 155:459 156:460 157:461 158:462 159:463 160:464 161:464 162:464 163:465 164:466 165:467 166:468 167:469 168:470 169:471 170:472 171:473 172:474 173:475 174:476 175:477 176:477 177:478 178:479 179:480 180:481 181:482 182:483 183:484 184:485 185:486 186:487 187:488 188:489 189:490 190:490 191:491 192:492 193:493 194:494 195:495 196:496 197:497 198:498 199:499 200:500 201:501 202:502 203:503 204:503 205:504 206:505 207:506 208:506 209:506 210:507 211:508 212:509 213:510 214:511 215:512 216:513 217:514 218:515 219:516 220:516 221:517 222:518 223:519 224:520 225:521 226:522 227:523 228:524 229:525 230:526 231:527 232:528 233:529 234:529 235:530 236:531 237:532 238:532 239:532 240:533 241:534 242:535 243:536 244:537 245:538 246:539 247:540 248:541 249:542 250:542 251:543 252:544 253:545 254:545 255:545 256:546 257:547 258:548 259:549 260:550 261:551 262:552 263:553 264:554 265:555 266:555 267:555 268:556 269:557 270:558 271:559 272:560 273:561 274:562 275:563 276:564 277:565 278:566 279:567 280:568 281:569 282:569 283:570 284:571 285:572 286:573 287:574 288:575 289:576 290:577 291:578 292:579 293:580 294:581 295:582 296:583 297:583 298:584 299:585 300:586 301:587 302:588 303:589 304:590 305:591 306:592 307:593 308:594 309:595 310:596 311:597 312:597 313:598 314:599 315:600 316:601 317:602 318:603 319:604 320:605 321:606 322:607 323:608 324:609 325:610 326:611 327:611 328:611 329:611 330:612 331:613 332:614 333:615 334:616 335:617 336:618 337:619 338:620 339:621 340:622 341:623 342:624 343:625 344:625 345:626 346:627 347:628 348:629 349:630 350:631 351:632 352:633 353:634 354:635 355:636 356:637 357:638 358:639 359:639 360:639 361:639 362:640 363:641 364:642 365:643 366:644 367:645 368:646 369:647 370:648 371:649 372:650 373:651 374:651 375:652 376:653 377:654 378:655 379:656 380:657 381:657 382:658\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4398 1035 2828 2011 4341 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 146\n",
            "INFO:tensorflow:end_position: 162\n",
            "INFO:tensorflow:answer: the top household _ type for sales between jul 2015 and jun 2017 was this . .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000079\n",
            "INFO:tensorflow:example_index: 15\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the top household _ type by sales [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:446 12:447 13:448 14:449 15:450 16:451 17:451 18:452 19:453 20:454 21:454 22:454 23:455 24:456 25:457 26:458 27:459 28:460 29:461 30:462 31:463 32:464 33:464 34:464 35:465 36:466 37:467 38:468 39:469 40:470 41:471 42:472 43:473 44:474 45:475 46:476 47:477 48:477 49:478 50:479 51:480 52:481 53:482 54:483 55:484 56:485 57:486 58:487 59:488 60:489 61:490 62:490 63:491 64:492 65:493 66:494 67:495 68:496 69:497 70:498 71:499 72:500 73:501 74:502 75:503 76:503 77:504 78:505 79:506 80:506 81:506 82:507 83:508 84:509 85:510 86:511 87:512 88:513 89:514 90:515 91:516 92:516 93:517 94:518 95:519 96:520 97:521 98:522 99:523 100:524 101:525 102:526 103:527 104:528 105:529 106:529 107:530 108:531 109:532 110:532 111:532 112:533 113:534 114:535 115:536 116:537 117:538 118:539 119:540 120:541 121:542 122:542 123:543 124:544 125:545 126:545 127:545 128:546 129:547 130:548 131:549 132:550 133:551 134:552 135:553 136:554 137:555 138:555 139:555 140:556 141:557 142:558 143:559 144:560 145:561 146:562 147:563 148:564 149:565 150:566 151:567 152:568 153:569 154:569 155:570 156:571 157:572 158:573 159:574 160:575 161:576 162:577 163:578 164:579 165:580 166:581 167:582 168:583 169:583 170:584 171:585 172:586 173:587 174:588 175:589 176:590 177:591 178:592 179:593 180:594 181:595 182:596 183:597 184:597 185:598 186:599 187:600 188:601 189:602 190:603 191:604 192:605 193:606 194:607 195:608 196:609 197:610 198:611 199:611 200:611 201:611 202:612 203:613 204:614 205:615 206:616 207:617 208:618 209:619 210:620 211:621 212:622 213:623 214:624 215:625 216:625 217:626 218:627 219:628 220:629 221:630 222:631 223:632 224:633 225:634 226:635 227:636 228:637 229:638 230:639 231:639 232:639 233:639 234:640 235:641 236:642 237:643 238:644 239:645 240:646 241:647 242:648 243:649 244:650 245:651 246:651 247:652 248:653 249:654 250:655 251:656 252:657 253:657 254:658 255:659 256:660 257:661 258:662 259:663 260:664 261:665 262:665 263:666 264:667 265:668 266:669 267:670 268:671 269:671 270:672 271:673 272:674 273:675 274:676 275:677 276:678 277:679 278:679 279:680 280:681 281:682 282:683 283:684 284:685 285:685 286:686 287:687 288:688 289:689 290:690 291:691 292:692 293:693 294:693 295:694 296:695 297:696 298:697 299:698 300:699 301:699 302:700 303:701 304:702 305:703 306:704 307:705 308:706 309:707 310:707 311:707 312:707 313:708 314:709 315:710 316:711 317:712 318:713 319:713 320:714 321:715 322:716 323:717 324:718 325:719 326:720 327:721 328:721 329:722 330:723 331:724 332:725 333:726 334:727 335:727 336:728 337:729 338:730 339:731 340:732 341:733 342:734 343:735 344:735 345:735 346:735 347:736 348:737 349:738 350:739 351:740 352:741 353:741 354:742 355:743 356:744 357:745 358:746 359:747 360:747\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 2327 4398 1035 2828 2011 4341 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 18\n",
            "INFO:tensorflow:end_position: 34\n",
            "INFO:tensorflow:answer: the top household _ type for sales between jul 2015 and jun 2017 was this . .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000080\n",
            "INFO:tensorflow:example_index: 16\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by household _ type [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:12 26:13 27:14 28:15 29:16 30:17 31:18 32:19 33:20 34:21 35:22 36:23 37:24 38:25 39:25 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:33 48:34 49:35 50:36 51:37 52:38 53:38 54:39 55:40 56:41 57:41 58:41 59:42 60:43 61:44 62:45 63:46 64:47 65:48 66:49 67:50 68:51 69:51 70:52 71:53 72:54 73:55 74:56 75:57 76:58 77:59 78:60 79:61 80:62 81:63 82:64 83:64 84:65 85:66 86:67 87:67 88:67 89:68 90:69 91:70 92:71 93:72 94:73 95:74 96:75 97:76 98:77 99:77 100:78 101:79 102:80 103:80 104:80 105:81 106:82 107:83 108:84 109:85 110:86 111:87 112:88 113:89 114:90 115:90 116:90 117:91 118:92 119:93 120:94 121:95 122:96 123:97 124:98 125:99 126:100 127:101 128:102 129:103 130:103 131:104 132:105 133:106 134:107 135:108 136:109 137:110 138:111 139:112 140:113 141:114 142:115 143:116 144:116 145:117 146:118 147:119 148:120 149:121 150:122 151:123 152:124 153:125 154:126 155:127 156:128 157:129 158:129 159:130 160:131 161:132 162:132 163:132 164:133 165:134 166:135 167:136 168:137 169:138 170:139 171:140 172:141 173:142 174:142 175:143 176:144 177:145 178:146 179:147 180:148 181:149 182:150 183:151 184:152 185:153 186:154 187:155 188:155 189:156 190:157 191:158 192:158 193:158 194:159 195:160 196:161 197:162 198:163 199:164 200:165 201:166 202:167 203:168 204:168 205:169 206:170 207:171 208:171 209:171 210:172 211:173 212:174 213:175 214:176 215:177 216:178 217:179 218:180 219:181 220:181 221:181 222:182 223:183 224:184 225:185 226:186 227:187 228:188 229:189 230:190 231:191 232:192 233:193 234:194 235:195 236:195 237:196 238:197 239:198 240:199 241:200 242:201 243:202 244:203 245:204 246:205 247:206 248:207 249:208 250:209 251:209 252:210 253:211 254:212 255:213 256:214 257:215 258:216 259:217 260:218 261:219 262:220 263:221 264:222 265:223 266:223 267:224 268:225 269:226 270:227 271:228 272:229 273:230 274:231 275:232 276:233 277:234 278:235 279:236 280:237 281:237 282:237 283:237 284:238 285:239 286:240 287:241 288:242 289:243 290:244 291:245 292:246 293:247 294:248 295:249 296:250 297:251 298:251 299:252 300:253 301:254 302:255 303:256 304:257 305:258 306:259 307:260 308:261 309:262 310:263 311:264 312:265 313:265 314:265 315:265 316:266 317:267 318:268 319:269 320:270 321:271 322:272 323:273 324:274 325:275 326:276 327:277 328:277 329:278 330:279 331:280 332:281 333:282 334:283 335:283 336:284 337:285 338:286 339:287 340:288 341:289 342:290 343:291 344:291 345:292 346:293 347:294 348:295 349:296 350:297 351:297 352:298 353:299 354:300 355:301 356:302 357:303 358:304 359:305 360:305 361:306 362:307 363:308 364:309 365:310 366:311 367:311 368:312 369:313 370:314 371:315 372:316 373:317 374:318 375:319 376:319 377:320 378:321 379:322 380:323 381:324 382:325\n",
            "INFO:tensorflow:token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 4398 1035 2828 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000081\n",
            "INFO:tensorflow:example_index: 16\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by household _ type [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:113 13:114 14:115 15:116 16:116 17:117 18:118 19:119 20:120 21:121 22:122 23:123 24:124 25:125 26:126 27:127 28:128 29:129 30:129 31:130 32:131 33:132 34:132 35:132 36:133 37:134 38:135 39:136 40:137 41:138 42:139 43:140 44:141 45:142 46:142 47:143 48:144 49:145 50:146 51:147 52:148 53:149 54:150 55:151 56:152 57:153 58:154 59:155 60:155 61:156 62:157 63:158 64:158 65:158 66:159 67:160 68:161 69:162 70:163 71:164 72:165 73:166 74:167 75:168 76:168 77:169 78:170 79:171 80:171 81:171 82:172 83:173 84:174 85:175 86:176 87:177 88:178 89:179 90:180 91:181 92:181 93:181 94:182 95:183 96:184 97:185 98:186 99:187 100:188 101:189 102:190 103:191 104:192 105:193 106:194 107:195 108:195 109:196 110:197 111:198 112:199 113:200 114:201 115:202 116:203 117:204 118:205 119:206 120:207 121:208 122:209 123:209 124:210 125:211 126:212 127:213 128:214 129:215 130:216 131:217 132:218 133:219 134:220 135:221 136:222 137:223 138:223 139:224 140:225 141:226 142:227 143:228 144:229 145:230 146:231 147:232 148:233 149:234 150:235 151:236 152:237 153:237 154:237 155:237 156:238 157:239 158:240 159:241 160:242 161:243 162:244 163:245 164:246 165:247 166:248 167:249 168:250 169:251 170:251 171:252 172:253 173:254 174:255 175:256 176:257 177:258 178:259 179:260 180:261 181:262 182:263 183:264 184:265 185:265 186:265 187:265 188:266 189:267 190:268 191:269 192:270 193:271 194:272 195:273 196:274 197:275 198:276 199:277 200:277 201:278 202:279 203:280 204:281 205:282 206:283 207:283 208:284 209:285 210:286 211:287 212:288 213:289 214:290 215:291 216:291 217:292 218:293 219:294 220:295 221:296 222:297 223:297 224:298 225:299 226:300 227:301 228:302 229:303 230:304 231:305 232:305 233:306 234:307 235:308 236:309 237:310 238:311 239:311 240:312 241:313 242:314 243:315 244:316 245:317 246:318 247:319 248:319 249:320 250:321 251:322 252:323 253:324 254:325 255:325 256:326 257:327 258:328 259:329 260:330 261:331 262:332 263:333 264:333 265:333 266:333 267:334 268:335 269:336 270:337 271:338 272:339 273:339 274:340 275:341 276:342 277:343 278:344 279:345 280:346 281:347 282:347 283:348 284:349 285:350 286:351 287:352 288:353 289:353 290:354 291:355 292:356 293:357 294:358 295:359 296:360 297:361 298:361 299:361 300:361 301:362 302:363 303:364 304:365 305:366 306:367 307:367 308:368 309:369 310:370 311:371 312:372 313:373 314:373 315:374 316:375 317:376 318:377 319:378 320:379 321:380 322:381 323:382 324:383 325:384 326:385 327:386 328:386 329:387 330:388 331:389 332:390 333:391 334:392 335:393 336:394 337:395 338:396 339:397 340:398 341:399 342:399 343:400 344:401 345:402 346:403 347:404 348:405 349:406 350:407 351:408 352:409 353:410 354:411 355:412 356:412 357:413 358:414 359:415 360:415 361:415 362:416 363:417 364:418 365:419 366:420 367:421 368:422 369:423 370:424 371:425 372:425 373:426 374:427 375:428 376:429 377:430 378:431 379:432 380:433 381:434 382:435\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 4398 1035 2828 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000082\n",
            "INFO:tensorflow:example_index: 16\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by household _ type [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:225 13:226 14:227 15:228 16:229 17:230 18:231 19:232 20:233 21:234 22:235 23:236 24:237 25:237 26:237 27:237 28:238 29:239 30:240 31:241 32:242 33:243 34:244 35:245 36:246 37:247 38:248 39:249 40:250 41:251 42:251 43:252 44:253 45:254 46:255 47:256 48:257 49:258 50:259 51:260 52:261 53:262 54:263 55:264 56:265 57:265 58:265 59:265 60:266 61:267 62:268 63:269 64:270 65:271 66:272 67:273 68:274 69:275 70:276 71:277 72:277 73:278 74:279 75:280 76:281 77:282 78:283 79:283 80:284 81:285 82:286 83:287 84:288 85:289 86:290 87:291 88:291 89:292 90:293 91:294 92:295 93:296 94:297 95:297 96:298 97:299 98:300 99:301 100:302 101:303 102:304 103:305 104:305 105:306 106:307 107:308 108:309 109:310 110:311 111:311 112:312 113:313 114:314 115:315 116:316 117:317 118:318 119:319 120:319 121:320 122:321 123:322 124:323 125:324 126:325 127:325 128:326 129:327 130:328 131:329 132:330 133:331 134:332 135:333 136:333 137:333 138:333 139:334 140:335 141:336 142:337 143:338 144:339 145:339 146:340 147:341 148:342 149:343 150:344 151:345 152:346 153:347 154:347 155:348 156:349 157:350 158:351 159:352 160:353 161:353 162:354 163:355 164:356 165:357 166:358 167:359 168:360 169:361 170:361 171:361 172:361 173:362 174:363 175:364 176:365 177:366 178:367 179:367 180:368 181:369 182:370 183:371 184:372 185:373 186:373 187:374 188:375 189:376 190:377 191:378 192:379 193:380 194:381 195:382 196:383 197:384 198:385 199:386 200:386 201:387 202:388 203:389 204:390 205:391 206:392 207:393 208:394 209:395 210:396 211:397 212:398 213:399 214:399 215:400 216:401 217:402 218:403 219:404 220:405 221:406 222:407 223:408 224:409 225:410 226:411 227:412 228:412 229:413 230:414 231:415 232:415 233:415 234:416 235:417 236:418 237:419 238:420 239:421 240:422 241:423 242:424 243:425 244:425 245:426 246:427 247:428 248:429 249:430 250:431 251:432 252:433 253:434 254:435 255:436 256:437 257:438 258:438 259:439 260:440 261:441 262:441 263:441 264:442 265:443 266:444 267:445 268:446 269:447 270:448 271:449 272:450 273:451 274:451 275:452 276:453 277:454 278:454 279:454 280:455 281:456 282:457 283:458 284:459 285:460 286:461 287:462 288:463 289:464 290:464 291:464 292:465 293:466 294:467 295:468 296:469 297:470 298:471 299:472 300:473 301:474 302:475 303:476 304:477 305:477 306:478 307:479 308:480 309:481 310:482 311:483 312:484 313:485 314:486 315:487 316:488 317:489 318:490 319:490 320:491 321:492 322:493 323:494 324:495 325:496 326:497 327:498 328:499 329:500 330:501 331:502 332:503 333:503 334:504 335:505 336:506 337:506 338:506 339:507 340:508 341:509 342:510 343:511 344:512 345:513 346:514 347:515 348:516 349:516 350:517 351:518 352:519 353:520 354:521 355:522 356:523 357:524 358:525 359:526 360:527 361:528 362:529 363:529 364:530 365:531 366:532 367:532 368:532 369:533 370:534 371:535 372:536 373:537 374:538 375:539 376:540 377:541 378:542 379:542 380:543 381:544 382:545\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 4398 1035 2828 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000083\n",
            "INFO:tensorflow:example_index: 16\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by household _ type [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:335 13:336 14:337 15:338 16:339 17:339 18:340 19:341 20:342 21:343 22:344 23:345 24:346 25:347 26:347 27:348 28:349 29:350 30:351 31:352 32:353 33:353 34:354 35:355 36:356 37:357 38:358 39:359 40:360 41:361 42:361 43:361 44:361 45:362 46:363 47:364 48:365 49:366 50:367 51:367 52:368 53:369 54:370 55:371 56:372 57:373 58:373 59:374 60:375 61:376 62:377 63:378 64:379 65:380 66:381 67:382 68:383 69:384 70:385 71:386 72:386 73:387 74:388 75:389 76:390 77:391 78:392 79:393 80:394 81:395 82:396 83:397 84:398 85:399 86:399 87:400 88:401 89:402 90:403 91:404 92:405 93:406 94:407 95:408 96:409 97:410 98:411 99:412 100:412 101:413 102:414 103:415 104:415 105:415 106:416 107:417 108:418 109:419 110:420 111:421 112:422 113:423 114:424 115:425 116:425 117:426 118:427 119:428 120:429 121:430 122:431 123:432 124:433 125:434 126:435 127:436 128:437 129:438 130:438 131:439 132:440 133:441 134:441 135:441 136:442 137:443 138:444 139:445 140:446 141:447 142:448 143:449 144:450 145:451 146:451 147:452 148:453 149:454 150:454 151:454 152:455 153:456 154:457 155:458 156:459 157:460 158:461 159:462 160:463 161:464 162:464 163:464 164:465 165:466 166:467 167:468 168:469 169:470 170:471 171:472 172:473 173:474 174:475 175:476 176:477 177:477 178:478 179:479 180:480 181:481 182:482 183:483 184:484 185:485 186:486 187:487 188:488 189:489 190:490 191:490 192:491 193:492 194:493 195:494 196:495 197:496 198:497 199:498 200:499 201:500 202:501 203:502 204:503 205:503 206:504 207:505 208:506 209:506 210:506 211:507 212:508 213:509 214:510 215:511 216:512 217:513 218:514 219:515 220:516 221:516 222:517 223:518 224:519 225:520 226:521 227:522 228:523 229:524 230:525 231:526 232:527 233:528 234:529 235:529 236:530 237:531 238:532 239:532 240:532 241:533 242:534 243:535 244:536 245:537 246:538 247:539 248:540 249:541 250:542 251:542 252:543 253:544 254:545 255:545 256:545 257:546 258:547 259:548 260:549 261:550 262:551 263:552 264:553 265:554 266:555 267:555 268:555 269:556 270:557 271:558 272:559 273:560 274:561 275:562 276:563 277:564 278:565 279:566 280:567 281:568 282:569 283:569 284:570 285:571 286:572 287:573 288:574 289:575 290:576 291:577 292:578 293:579 294:580 295:581 296:582 297:583 298:583 299:584 300:585 301:586 302:587 303:588 304:589 305:590 306:591 307:592 308:593 309:594 310:595 311:596 312:597 313:597 314:598 315:599 316:600 317:601 318:602 319:603 320:604 321:605 322:606 323:607 324:608 325:609 326:610 327:611 328:611 329:611 330:611 331:612 332:613 333:614 334:615 335:616 336:617 337:618 338:619 339:620 340:621 341:622 342:623 343:624 344:625 345:625 346:626 347:627 348:628 349:629 350:630 351:631 352:632 353:633 354:634 355:635 356:636 357:637 358:638 359:639 360:639 361:639 362:639 363:640 364:641 365:642 366:643 367:644 368:645 369:646 370:647 371:648 372:649 373:650 374:651 375:651 376:652 377:653 378:654 379:655 380:656 381:657 382:657\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 4398 1035 2828 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 346\n",
            "INFO:tensorflow:end_position: 362\n",
            "INFO:tensorflow:answer: the breakdown of sales between jul 2015 and jun 2017 was this for household _ type .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000084\n",
            "INFO:tensorflow:example_index: 16\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of sales by household _ type [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:446 13:447 14:448 15:449 16:450 17:451 18:451 19:452 20:453 21:454 22:454 23:454 24:455 25:456 26:457 27:458 28:459 29:460 30:461 31:462 32:463 33:464 34:464 35:464 36:465 37:466 38:467 39:468 40:469 41:470 42:471 43:472 44:473 45:474 46:475 47:476 48:477 49:477 50:478 51:479 52:480 53:481 54:482 55:483 56:484 57:485 58:486 59:487 60:488 61:489 62:490 63:490 64:491 65:492 66:493 67:494 68:495 69:496 70:497 71:498 72:499 73:500 74:501 75:502 76:503 77:503 78:504 79:505 80:506 81:506 82:506 83:507 84:508 85:509 86:510 87:511 88:512 89:513 90:514 91:515 92:516 93:516 94:517 95:518 96:519 97:520 98:521 99:522 100:523 101:524 102:525 103:526 104:527 105:528 106:529 107:529 108:530 109:531 110:532 111:532 112:532 113:533 114:534 115:535 116:536 117:537 118:538 119:539 120:540 121:541 122:542 123:542 124:543 125:544 126:545 127:545 128:545 129:546 130:547 131:548 132:549 133:550 134:551 135:552 136:553 137:554 138:555 139:555 140:555 141:556 142:557 143:558 144:559 145:560 146:561 147:562 148:563 149:564 150:565 151:566 152:567 153:568 154:569 155:569 156:570 157:571 158:572 159:573 160:574 161:575 162:576 163:577 164:578 165:579 166:580 167:581 168:582 169:583 170:583 171:584 172:585 173:586 174:587 175:588 176:589 177:590 178:591 179:592 180:593 181:594 182:595 183:596 184:597 185:597 186:598 187:599 188:600 189:601 190:602 191:603 192:604 193:605 194:606 195:607 196:608 197:609 198:610 199:611 200:611 201:611 202:611 203:612 204:613 205:614 206:615 207:616 208:617 209:618 210:619 211:620 212:621 213:622 214:623 215:624 216:625 217:625 218:626 219:627 220:628 221:629 222:630 223:631 224:632 225:633 226:634 227:635 228:636 229:637 230:638 231:639 232:639 233:639 234:639 235:640 236:641 237:642 238:643 239:644 240:645 241:646 242:647 243:648 244:649 245:650 246:651 247:651 248:652 249:653 250:654 251:655 252:656 253:657 254:657 255:658 256:659 257:660 258:661 259:662 260:663 261:664 262:665 263:665 264:666 265:667 266:668 267:669 268:670 269:671 270:671 271:672 272:673 273:674 274:675 275:676 276:677 277:678 278:679 279:679 280:680 281:681 282:682 283:683 284:684 285:685 286:685 287:686 288:687 289:688 290:689 291:690 292:691 293:692 294:693 295:693 296:694 297:695 298:696 299:697 300:698 301:699 302:699 303:700 304:701 305:702 306:703 307:704 308:705 309:706 310:707 311:707 312:707 313:707 314:708 315:709 316:710 317:711 318:712 319:713 320:713 321:714 322:715 323:716 324:717 325:718 326:719 327:720 328:721 329:721 330:722 331:723 332:724 333:725 334:726 335:727 336:727 337:728 338:729 339:730 340:731 341:732 342:733 343:734 344:735 345:735 346:735 347:735 348:736 349:737 350:738 351:739 352:740 353:741 354:741 355:742 356:743 357:744 358:745 359:746 360:747 361:747\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True 361:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 4341 2011 4398 1035 2828 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 218\n",
            "INFO:tensorflow:end_position: 234\n",
            "INFO:tensorflow:answer: the breakdown of sales between jul 2015 and jun 2017 was this for household _ type .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000085\n",
            "INFO:tensorflow:example_index: 17\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:7 16:8 17:9 18:10 19:11 20:12 21:12 22:13 23:14 24:15 25:16 26:17 27:18 28:19 29:20 30:21 31:22 32:23 33:24 34:25 35:25 36:26 37:27 38:28 39:29 40:30 41:31 42:32 43:33 44:34 45:35 46:36 47:37 48:38 49:38 50:39 51:40 52:41 53:41 54:41 55:42 56:43 57:44 58:45 59:46 60:47 61:48 62:49 63:50 64:51 65:51 66:52 67:53 68:54 69:55 70:56 71:57 72:58 73:59 74:60 75:61 76:62 77:63 78:64 79:64 80:65 81:66 82:67 83:67 84:67 85:68 86:69 87:70 88:71 89:72 90:73 91:74 92:75 93:76 94:77 95:77 96:78 97:79 98:80 99:80 100:80 101:81 102:82 103:83 104:84 105:85 106:86 107:87 108:88 109:89 110:90 111:90 112:90 113:91 114:92 115:93 116:94 117:95 118:96 119:97 120:98 121:99 122:100 123:101 124:102 125:103 126:103 127:104 128:105 129:106 130:107 131:108 132:109 133:110 134:111 135:112 136:113 137:114 138:115 139:116 140:116 141:117 142:118 143:119 144:120 145:121 146:122 147:123 148:124 149:125 150:126 151:127 152:128 153:129 154:129 155:130 156:131 157:132 158:132 159:132 160:133 161:134 162:135 163:136 164:137 165:138 166:139 167:140 168:141 169:142 170:142 171:143 172:144 173:145 174:146 175:147 176:148 177:149 178:150 179:151 180:152 181:153 182:154 183:155 184:155 185:156 186:157 187:158 188:158 189:158 190:159 191:160 192:161 193:162 194:163 195:164 196:165 197:166 198:167 199:168 200:168 201:169 202:170 203:171 204:171 205:171 206:172 207:173 208:174 209:175 210:176 211:177 212:178 213:179 214:180 215:181 216:181 217:181 218:182 219:183 220:184 221:185 222:186 223:187 224:188 225:189 226:190 227:191 228:192 229:193 230:194 231:195 232:195 233:196 234:197 235:198 236:199 237:200 238:201 239:202 240:203 241:204 242:205 243:206 244:207 245:208 246:209 247:209 248:210 249:211 250:212 251:213 252:214 253:215 254:216 255:217 256:218 257:219 258:220 259:221 260:222 261:223 262:223 263:224 264:225 265:226 266:227 267:228 268:229 269:230 270:231 271:232 272:233 273:234 274:235 275:236 276:237 277:237 278:237 279:237 280:238 281:239 282:240 283:241 284:242 285:243 286:244 287:245 288:246 289:247 290:248 291:249 292:250 293:251 294:251 295:252 296:253 297:254 298:255 299:256 300:257 301:258 302:259 303:260 304:261 305:262 306:263 307:264 308:265 309:265 310:265 311:265 312:266 313:267 314:268 315:269 316:270 317:271 318:272 319:273 320:274 321:275 322:276 323:277 324:277 325:278 326:279 327:280 328:281 329:282 330:283 331:283 332:284 333:285 334:286 335:287 336:288 337:289 338:290 339:291 340:291 341:292 342:293 343:294 344:295 345:296 346:297 347:297 348:298 349:299 350:300 351:301 352:302 353:303 354:304 355:305 356:305 357:306 358:307 359:308 360:309 361:310 362:311 363:311 364:312 365:313 366:314 367:315 368:316 369:317 370:318 371:319 372:319 373:320 374:321 375:322 376:323 377:324 378:325 379:325 380:326 381:327 382:328\n",
            "INFO:tensorflow:token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 312\n",
            "INFO:tensorflow:end_position: 324\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000086\n",
            "INFO:tensorflow:example_index: 17\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:113 9:114 10:115 11:116 12:116 13:117 14:118 15:119 16:120 17:121 18:122 19:123 20:124 21:125 22:126 23:127 24:128 25:129 26:129 27:130 28:131 29:132 30:132 31:132 32:133 33:134 34:135 35:136 36:137 37:138 38:139 39:140 40:141 41:142 42:142 43:143 44:144 45:145 46:146 47:147 48:148 49:149 50:150 51:151 52:152 53:153 54:154 55:155 56:155 57:156 58:157 59:158 60:158 61:158 62:159 63:160 64:161 65:162 66:163 67:164 68:165 69:166 70:167 71:168 72:168 73:169 74:170 75:171 76:171 77:171 78:172 79:173 80:174 81:175 82:176 83:177 84:178 85:179 86:180 87:181 88:181 89:181 90:182 91:183 92:184 93:185 94:186 95:187 96:188 97:189 98:190 99:191 100:192 101:193 102:194 103:195 104:195 105:196 106:197 107:198 108:199 109:200 110:201 111:202 112:203 113:204 114:205 115:206 116:207 117:208 118:209 119:209 120:210 121:211 122:212 123:213 124:214 125:215 126:216 127:217 128:218 129:219 130:220 131:221 132:222 133:223 134:223 135:224 136:225 137:226 138:227 139:228 140:229 141:230 142:231 143:232 144:233 145:234 146:235 147:236 148:237 149:237 150:237 151:237 152:238 153:239 154:240 155:241 156:242 157:243 158:244 159:245 160:246 161:247 162:248 163:249 164:250 165:251 166:251 167:252 168:253 169:254 170:255 171:256 172:257 173:258 174:259 175:260 176:261 177:262 178:263 179:264 180:265 181:265 182:265 183:265 184:266 185:267 186:268 187:269 188:270 189:271 190:272 191:273 192:274 193:275 194:276 195:277 196:277 197:278 198:279 199:280 200:281 201:282 202:283 203:283 204:284 205:285 206:286 207:287 208:288 209:289 210:290 211:291 212:291 213:292 214:293 215:294 216:295 217:296 218:297 219:297 220:298 221:299 222:300 223:301 224:302 225:303 226:304 227:305 228:305 229:306 230:307 231:308 232:309 233:310 234:311 235:311 236:312 237:313 238:314 239:315 240:316 241:317 242:318 243:319 244:319 245:320 246:321 247:322 248:323 249:324 250:325 251:325 252:326 253:327 254:328 255:329 256:330 257:331 258:332 259:333 260:333 261:333 262:333 263:334 264:335 265:336 266:337 267:338 268:339 269:339 270:340 271:341 272:342 273:343 274:344 275:345 276:346 277:347 278:347 279:348 280:349 281:350 282:351 283:352 284:353 285:353 286:354 287:355 288:356 289:357 290:358 291:359 292:360 293:361 294:361 295:361 296:361 297:362 298:363 299:364 300:365 301:366 302:367 303:367 304:368 305:369 306:370 307:371 308:372 309:373 310:373 311:374 312:375 313:376 314:377 315:378 316:379 317:380 318:381 319:382 320:383 321:384 322:385 323:386 324:386 325:387 326:388 327:389 328:390 329:391 330:392 331:393 332:394 333:395 334:396 335:397 336:398 337:399 338:399 339:400 340:401 341:402 342:403 343:404 344:405 345:406 346:407 347:408 348:409 349:410 350:411 351:412 352:412 353:413 354:414 355:415 356:415 357:415 358:416 359:417 360:418 361:419 362:420 363:421 364:422 365:423 366:424 367:425 368:425 369:426 370:427 371:428 372:429 373:430 374:431 375:432 376:433 377:434 378:435 379:436 380:437 381:438 382:438\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 184\n",
            "INFO:tensorflow:end_position: 196\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000087\n",
            "INFO:tensorflow:example_index: 17\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:225 9:226 10:227 11:228 12:229 13:230 14:231 15:232 16:233 17:234 18:235 19:236 20:237 21:237 22:237 23:237 24:238 25:239 26:240 27:241 28:242 29:243 30:244 31:245 32:246 33:247 34:248 35:249 36:250 37:251 38:251 39:252 40:253 41:254 42:255 43:256 44:257 45:258 46:259 47:260 48:261 49:262 50:263 51:264 52:265 53:265 54:265 55:265 56:266 57:267 58:268 59:269 60:270 61:271 62:272 63:273 64:274 65:275 66:276 67:277 68:277 69:278 70:279 71:280 72:281 73:282 74:283 75:283 76:284 77:285 78:286 79:287 80:288 81:289 82:290 83:291 84:291 85:292 86:293 87:294 88:295 89:296 90:297 91:297 92:298 93:299 94:300 95:301 96:302 97:303 98:304 99:305 100:305 101:306 102:307 103:308 104:309 105:310 106:311 107:311 108:312 109:313 110:314 111:315 112:316 113:317 114:318 115:319 116:319 117:320 118:321 119:322 120:323 121:324 122:325 123:325 124:326 125:327 126:328 127:329 128:330 129:331 130:332 131:333 132:333 133:333 134:333 135:334 136:335 137:336 138:337 139:338 140:339 141:339 142:340 143:341 144:342 145:343 146:344 147:345 148:346 149:347 150:347 151:348 152:349 153:350 154:351 155:352 156:353 157:353 158:354 159:355 160:356 161:357 162:358 163:359 164:360 165:361 166:361 167:361 168:361 169:362 170:363 171:364 172:365 173:366 174:367 175:367 176:368 177:369 178:370 179:371 180:372 181:373 182:373 183:374 184:375 185:376 186:377 187:378 188:379 189:380 190:381 191:382 192:383 193:384 194:385 195:386 196:386 197:387 198:388 199:389 200:390 201:391 202:392 203:393 204:394 205:395 206:396 207:397 208:398 209:399 210:399 211:400 212:401 213:402 214:403 215:404 216:405 217:406 218:407 219:408 220:409 221:410 222:411 223:412 224:412 225:413 226:414 227:415 228:415 229:415 230:416 231:417 232:418 233:419 234:420 235:421 236:422 237:423 238:424 239:425 240:425 241:426 242:427 243:428 244:429 245:430 246:431 247:432 248:433 249:434 250:435 251:436 252:437 253:438 254:438 255:439 256:440 257:441 258:441 259:441 260:442 261:443 262:444 263:445 264:446 265:447 266:448 267:449 268:450 269:451 270:451 271:452 272:453 273:454 274:454 275:454 276:455 277:456 278:457 279:458 280:459 281:460 282:461 283:462 284:463 285:464 286:464 287:464 288:465 289:466 290:467 291:468 292:469 293:470 294:471 295:472 296:473 297:474 298:475 299:476 300:477 301:477 302:478 303:479 304:480 305:481 306:482 307:483 308:484 309:485 310:486 311:487 312:488 313:489 314:490 315:490 316:491 317:492 318:493 319:494 320:495 321:496 322:497 323:498 324:499 325:500 326:501 327:502 328:503 329:503 330:504 331:505 332:506 333:506 334:506 335:507 336:508 337:509 338:510 339:511 340:512 341:513 342:514 343:515 344:516 345:516 346:517 347:518 348:519 349:520 350:521 351:522 352:523 353:524 354:525 355:526 356:527 357:528 358:529 359:529 360:530 361:531 362:532 363:532 364:532 365:533 366:534 367:535 368:536 369:537 370:538 371:539 372:540 373:541 374:542 375:542 376:543 377:544 378:545 379:545 380:545 381:546 382:547\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 56\n",
            "INFO:tensorflow:end_position: 68\n",
            "INFO:tensorflow:answer: the breakdown of quantity between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000088\n",
            "INFO:tensorflow:example_index: 17\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:335 9:336 10:337 11:338 12:339 13:339 14:340 15:341 16:342 17:343 18:344 19:345 20:346 21:347 22:347 23:348 24:349 25:350 26:351 27:352 28:353 29:353 30:354 31:355 32:356 33:357 34:358 35:359 36:360 37:361 38:361 39:361 40:361 41:362 42:363 43:364 44:365 45:366 46:367 47:367 48:368 49:369 50:370 51:371 52:372 53:373 54:373 55:374 56:375 57:376 58:377 59:378 60:379 61:380 62:381 63:382 64:383 65:384 66:385 67:386 68:386 69:387 70:388 71:389 72:390 73:391 74:392 75:393 76:394 77:395 78:396 79:397 80:398 81:399 82:399 83:400 84:401 85:402 86:403 87:404 88:405 89:406 90:407 91:408 92:409 93:410 94:411 95:412 96:412 97:413 98:414 99:415 100:415 101:415 102:416 103:417 104:418 105:419 106:420 107:421 108:422 109:423 110:424 111:425 112:425 113:426 114:427 115:428 116:429 117:430 118:431 119:432 120:433 121:434 122:435 123:436 124:437 125:438 126:438 127:439 128:440 129:441 130:441 131:441 132:442 133:443 134:444 135:445 136:446 137:447 138:448 139:449 140:450 141:451 142:451 143:452 144:453 145:454 146:454 147:454 148:455 149:456 150:457 151:458 152:459 153:460 154:461 155:462 156:463 157:464 158:464 159:464 160:465 161:466 162:467 163:468 164:469 165:470 166:471 167:472 168:473 169:474 170:475 171:476 172:477 173:477 174:478 175:479 176:480 177:481 178:482 179:483 180:484 181:485 182:486 183:487 184:488 185:489 186:490 187:490 188:491 189:492 190:493 191:494 192:495 193:496 194:497 195:498 196:499 197:500 198:501 199:502 200:503 201:503 202:504 203:505 204:506 205:506 206:506 207:507 208:508 209:509 210:510 211:511 212:512 213:513 214:514 215:515 216:516 217:516 218:517 219:518 220:519 221:520 222:521 223:522 224:523 225:524 226:525 227:526 228:527 229:528 230:529 231:529 232:530 233:531 234:532 235:532 236:532 237:533 238:534 239:535 240:536 241:537 242:538 243:539 244:540 245:541 246:542 247:542 248:543 249:544 250:545 251:545 252:545 253:546 254:547 255:548 256:549 257:550 258:551 259:552 260:553 261:554 262:555 263:555 264:555 265:556 266:557 267:558 268:559 269:560 270:561 271:562 272:563 273:564 274:565 275:566 276:567 277:568 278:569 279:569 280:570 281:571 282:572 283:573 284:574 285:575 286:576 287:577 288:578 289:579 290:580 291:581 292:582 293:583 294:583 295:584 296:585 297:586 298:587 299:588 300:589 301:590 302:591 303:592 304:593 305:594 306:595 307:596 308:597 309:597 310:598 311:599 312:600 313:601 314:602 315:603 316:604 317:605 318:606 319:607 320:608 321:609 322:610 323:611 324:611 325:611 326:611 327:612 328:613 329:614 330:615 331:616 332:617 333:618 334:619 335:620 336:621 337:622 338:623 339:624 340:625 341:625 342:626 343:627 344:628 345:629 346:630 347:631 348:632 349:633 350:634 351:635 352:636 353:637 354:638 355:639 356:639 357:639 358:639 359:640 360:641 361:642 362:643 363:644 364:645 365:646 366:647 367:648 368:649 369:650 370:651 371:651 372:652 373:653 374:654 375:655 376:656 377:657 378:657 379:658 380:659 381:660 382:661\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:False 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000089\n",
            "INFO:tensorflow:example_index: 17\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the mix of quantity [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 8:446 9:447 10:448 11:449 12:450 13:451 14:451 15:452 16:453 17:454 18:454 19:454 20:455 21:456 22:457 23:458 24:459 25:460 26:461 27:462 28:463 29:464 30:464 31:464 32:465 33:466 34:467 35:468 36:469 37:470 38:471 39:472 40:473 41:474 42:475 43:476 44:477 45:477 46:478 47:479 48:480 49:481 50:482 51:483 52:484 53:485 54:486 55:487 56:488 57:489 58:490 59:490 60:491 61:492 62:493 63:494 64:495 65:496 66:497 67:498 68:499 69:500 70:501 71:502 72:503 73:503 74:504 75:505 76:506 77:506 78:506 79:507 80:508 81:509 82:510 83:511 84:512 85:513 86:514 87:515 88:516 89:516 90:517 91:518 92:519 93:520 94:521 95:522 96:523 97:524 98:525 99:526 100:527 101:528 102:529 103:529 104:530 105:531 106:532 107:532 108:532 109:533 110:534 111:535 112:536 113:537 114:538 115:539 116:540 117:541 118:542 119:542 120:543 121:544 122:545 123:545 124:545 125:546 126:547 127:548 128:549 129:550 130:551 131:552 132:553 133:554 134:555 135:555 136:555 137:556 138:557 139:558 140:559 141:560 142:561 143:562 144:563 145:564 146:565 147:566 148:567 149:568 150:569 151:569 152:570 153:571 154:572 155:573 156:574 157:575 158:576 159:577 160:578 161:579 162:580 163:581 164:582 165:583 166:583 167:584 168:585 169:586 170:587 171:588 172:589 173:590 174:591 175:592 176:593 177:594 178:595 179:596 180:597 181:597 182:598 183:599 184:600 185:601 186:602 187:603 188:604 189:605 190:606 191:607 192:608 193:609 194:610 195:611 196:611 197:611 198:611 199:612 200:613 201:614 202:615 203:616 204:617 205:618 206:619 207:620 208:621 209:622 210:623 211:624 212:625 213:625 214:626 215:627 216:628 217:629 218:630 219:631 220:632 221:633 222:634 223:635 224:636 225:637 226:638 227:639 228:639 229:639 230:639 231:640 232:641 233:642 234:643 235:644 236:645 237:646 238:647 239:648 240:649 241:650 242:651 243:651 244:652 245:653 246:654 247:655 248:656 249:657 250:657 251:658 252:659 253:660 254:661 255:662 256:663 257:664 258:665 259:665 260:666 261:667 262:668 263:669 264:670 265:671 266:671 267:672 268:673 269:674 270:675 271:676 272:677 273:678 274:679 275:679 276:680 277:681 278:682 279:683 280:684 281:685 282:685 283:686 284:687 285:688 286:689 287:690 288:691 289:692 290:693 291:693 292:694 293:695 294:696 295:697 296:698 297:699 298:699 299:700 300:701 301:702 302:703 303:704 304:705 305:706 306:707 307:707 308:707 309:707 310:708 311:709 312:710 313:711 314:712 315:713 316:713 317:714 318:715 319:716 320:717 321:718 322:719 323:720 324:721 325:721 326:722 327:723 328:724 329:725 330:726 331:727 332:727 333:728 334:729 335:730 336:731 337:732 338:733 339:734 340:735 341:735 342:735 343:735 344:736 345:737 346:738 347:739 348:740 349:741 350:741 351:742 352:743 353:744 354:745 355:746 356:747 357:747\n",
            "INFO:tensorflow:token_is_max_context: 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 4666 1997 11712 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000090\n",
            "INFO:tensorflow:example_index: 18\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by quantity [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:19 32:20 33:21 34:22 35:23 36:24 37:25 38:25 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:36 50:37 51:38 52:38 53:39 54:40 55:41 56:41 57:41 58:42 59:43 60:44 61:45 62:46 63:47 64:48 65:49 66:50 67:51 68:51 69:52 70:53 71:54 72:55 73:56 74:57 75:58 76:59 77:60 78:61 79:62 80:63 81:64 82:64 83:65 84:66 85:67 86:67 87:67 88:68 89:69 90:70 91:71 92:72 93:73 94:74 95:75 96:76 97:77 98:77 99:78 100:79 101:80 102:80 103:80 104:81 105:82 106:83 107:84 108:85 109:86 110:87 111:88 112:89 113:90 114:90 115:90 116:91 117:92 118:93 119:94 120:95 121:96 122:97 123:98 124:99 125:100 126:101 127:102 128:103 129:103 130:104 131:105 132:106 133:107 134:108 135:109 136:110 137:111 138:112 139:113 140:114 141:115 142:116 143:116 144:117 145:118 146:119 147:120 148:121 149:122 150:123 151:124 152:125 153:126 154:127 155:128 156:129 157:129 158:130 159:131 160:132 161:132 162:132 163:133 164:134 165:135 166:136 167:137 168:138 169:139 170:140 171:141 172:142 173:142 174:143 175:144 176:145 177:146 178:147 179:148 180:149 181:150 182:151 183:152 184:153 185:154 186:155 187:155 188:156 189:157 190:158 191:158 192:158 193:159 194:160 195:161 196:162 197:163 198:164 199:165 200:166 201:167 202:168 203:168 204:169 205:170 206:171 207:171 208:171 209:172 210:173 211:174 212:175 213:176 214:177 215:178 216:179 217:180 218:181 219:181 220:181 221:182 222:183 223:184 224:185 225:186 226:187 227:188 228:189 229:190 230:191 231:192 232:193 233:194 234:195 235:195 236:196 237:197 238:198 239:199 240:200 241:201 242:202 243:203 244:204 245:205 246:206 247:207 248:208 249:209 250:209 251:210 252:211 253:212 254:213 255:214 256:215 257:216 258:217 259:218 260:219 261:220 262:221 263:222 264:223 265:223 266:224 267:225 268:226 269:227 270:228 271:229 272:230 273:231 274:232 275:233 276:234 277:235 278:236 279:237 280:237 281:237 282:237 283:238 284:239 285:240 286:241 287:242 288:243 289:244 290:245 291:246 292:247 293:248 294:249 295:250 296:251 297:251 298:252 299:253 300:254 301:255 302:256 303:257 304:258 305:259 306:260 307:261 308:262 309:263 310:264 311:265 312:265 313:265 314:265 315:266 316:267 317:268 318:269 319:270 320:271 321:272 322:273 323:274 324:275 325:276 326:277 327:277 328:278 329:279 330:280 331:281 332:282 333:283 334:283 335:284 336:285 337:286 338:287 339:288 340:289 341:290 342:291 343:291 344:292 345:293 346:294 347:295 348:296 349:297 350:297 351:298 352:299 353:300 354:301 355:302 356:303 357:304 358:305 359:305 360:306 361:307 362:308 363:309 364:310 365:311 366:311 367:312 368:313 369:314 370:315 371:316 372:317 373:318 374:319 375:319 376:320 377:321 378:322 379:323 380:324 381:325 382:325\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 11712 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 204\n",
            "INFO:tensorflow:end_position: 220\n",
            "INFO:tensorflow:answer: the bottom household _ type for quantity between jul 2015 and jun 2017 was this . .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000091\n",
            "INFO:tensorflow:example_index: 18\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by quantity [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:113 12:114 13:115 14:116 15:116 16:117 17:118 18:119 19:120 20:121 21:122 22:123 23:124 24:125 25:126 26:127 27:128 28:129 29:129 30:130 31:131 32:132 33:132 34:132 35:133 36:134 37:135 38:136 39:137 40:138 41:139 42:140 43:141 44:142 45:142 46:143 47:144 48:145 49:146 50:147 51:148 52:149 53:150 54:151 55:152 56:153 57:154 58:155 59:155 60:156 61:157 62:158 63:158 64:158 65:159 66:160 67:161 68:162 69:163 70:164 71:165 72:166 73:167 74:168 75:168 76:169 77:170 78:171 79:171 80:171 81:172 82:173 83:174 84:175 85:176 86:177 87:178 88:179 89:180 90:181 91:181 92:181 93:182 94:183 95:184 96:185 97:186 98:187 99:188 100:189 101:190 102:191 103:192 104:193 105:194 106:195 107:195 108:196 109:197 110:198 111:199 112:200 113:201 114:202 115:203 116:204 117:205 118:206 119:207 120:208 121:209 122:209 123:210 124:211 125:212 126:213 127:214 128:215 129:216 130:217 131:218 132:219 133:220 134:221 135:222 136:223 137:223 138:224 139:225 140:226 141:227 142:228 143:229 144:230 145:231 146:232 147:233 148:234 149:235 150:236 151:237 152:237 153:237 154:237 155:238 156:239 157:240 158:241 159:242 160:243 161:244 162:245 163:246 164:247 165:248 166:249 167:250 168:251 169:251 170:252 171:253 172:254 173:255 174:256 175:257 176:258 177:259 178:260 179:261 180:262 181:263 182:264 183:265 184:265 185:265 186:265 187:266 188:267 189:268 190:269 191:270 192:271 193:272 194:273 195:274 196:275 197:276 198:277 199:277 200:278 201:279 202:280 203:281 204:282 205:283 206:283 207:284 208:285 209:286 210:287 211:288 212:289 213:290 214:291 215:291 216:292 217:293 218:294 219:295 220:296 221:297 222:297 223:298 224:299 225:300 226:301 227:302 228:303 229:304 230:305 231:305 232:306 233:307 234:308 235:309 236:310 237:311 238:311 239:312 240:313 241:314 242:315 243:316 244:317 245:318 246:319 247:319 248:320 249:321 250:322 251:323 252:324 253:325 254:325 255:326 256:327 257:328 258:329 259:330 260:331 261:332 262:333 263:333 264:333 265:333 266:334 267:335 268:336 269:337 270:338 271:339 272:339 273:340 274:341 275:342 276:343 277:344 278:345 279:346 280:347 281:347 282:348 283:349 284:350 285:351 286:352 287:353 288:353 289:354 290:355 291:356 292:357 293:358 294:359 295:360 296:361 297:361 298:361 299:361 300:362 301:363 302:364 303:365 304:366 305:367 306:367 307:368 308:369 309:370 310:371 311:372 312:373 313:373 314:374 315:375 316:376 317:377 318:378 319:379 320:380 321:381 322:382 323:383 324:384 325:385 326:386 327:386 328:387 329:388 330:389 331:390 332:391 333:392 334:393 335:394 336:395 337:396 338:397 339:398 340:399 341:399 342:400 343:401 344:402 345:403 346:404 347:405 348:406 349:407 350:408 351:409 352:410 353:411 354:412 355:412 356:413 357:414 358:415 359:415 360:415 361:416 362:417 363:418 364:419 365:420 366:421 367:422 368:423 369:424 370:425 371:425 372:426 373:427 374:428 375:429 376:430 377:431 378:432 379:433 380:434 381:435 382:436\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 11712 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 76\n",
            "INFO:tensorflow:end_position: 92\n",
            "INFO:tensorflow:answer: the bottom household _ type for quantity between jul 2015 and jun 2017 was this . .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000092\n",
            "INFO:tensorflow:example_index: 18\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by quantity [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:225 12:226 13:227 14:228 15:229 16:230 17:231 18:232 19:233 20:234 21:235 22:236 23:237 24:237 25:237 26:237 27:238 28:239 29:240 30:241 31:242 32:243 33:244 34:245 35:246 36:247 37:248 38:249 39:250 40:251 41:251 42:252 43:253 44:254 45:255 46:256 47:257 48:258 49:259 50:260 51:261 52:262 53:263 54:264 55:265 56:265 57:265 58:265 59:266 60:267 61:268 62:269 63:270 64:271 65:272 66:273 67:274 68:275 69:276 70:277 71:277 72:278 73:279 74:280 75:281 76:282 77:283 78:283 79:284 80:285 81:286 82:287 83:288 84:289 85:290 86:291 87:291 88:292 89:293 90:294 91:295 92:296 93:297 94:297 95:298 96:299 97:300 98:301 99:302 100:303 101:304 102:305 103:305 104:306 105:307 106:308 107:309 108:310 109:311 110:311 111:312 112:313 113:314 114:315 115:316 116:317 117:318 118:319 119:319 120:320 121:321 122:322 123:323 124:324 125:325 126:325 127:326 128:327 129:328 130:329 131:330 132:331 133:332 134:333 135:333 136:333 137:333 138:334 139:335 140:336 141:337 142:338 143:339 144:339 145:340 146:341 147:342 148:343 149:344 150:345 151:346 152:347 153:347 154:348 155:349 156:350 157:351 158:352 159:353 160:353 161:354 162:355 163:356 164:357 165:358 166:359 167:360 168:361 169:361 170:361 171:361 172:362 173:363 174:364 175:365 176:366 177:367 178:367 179:368 180:369 181:370 182:371 183:372 184:373 185:373 186:374 187:375 188:376 189:377 190:378 191:379 192:380 193:381 194:382 195:383 196:384 197:385 198:386 199:386 200:387 201:388 202:389 203:390 204:391 205:392 206:393 207:394 208:395 209:396 210:397 211:398 212:399 213:399 214:400 215:401 216:402 217:403 218:404 219:405 220:406 221:407 222:408 223:409 224:410 225:411 226:412 227:412 228:413 229:414 230:415 231:415 232:415 233:416 234:417 235:418 236:419 237:420 238:421 239:422 240:423 241:424 242:425 243:425 244:426 245:427 246:428 247:429 248:430 249:431 250:432 251:433 252:434 253:435 254:436 255:437 256:438 257:438 258:439 259:440 260:441 261:441 262:441 263:442 264:443 265:444 266:445 267:446 268:447 269:448 270:449 271:450 272:451 273:451 274:452 275:453 276:454 277:454 278:454 279:455 280:456 281:457 282:458 283:459 284:460 285:461 286:462 287:463 288:464 289:464 290:464 291:465 292:466 293:467 294:468 295:469 296:470 297:471 298:472 299:473 300:474 301:475 302:476 303:477 304:477 305:478 306:479 307:480 308:481 309:482 310:483 311:484 312:485 313:486 314:487 315:488 316:489 317:490 318:490 319:491 320:492 321:493 322:494 323:495 324:496 325:497 326:498 327:499 328:500 329:501 330:502 331:503 332:503 333:504 334:505 335:506 336:506 337:506 338:507 339:508 340:509 341:510 342:511 343:512 344:513 345:514 346:515 347:516 348:516 349:517 350:518 351:519 352:520 353:521 354:522 355:523 356:524 357:525 358:526 359:527 360:528 361:529 362:529 363:530 364:531 365:532 366:532 367:532 368:533 369:534 370:535 371:536 372:537 373:538 374:539 375:540 376:541 377:542 378:542 379:543 380:544 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 11712 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000093\n",
            "INFO:tensorflow:example_index: 18\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by quantity [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:335 12:336 13:337 14:338 15:339 16:339 17:340 18:341 19:342 20:343 21:344 22:345 23:346 24:347 25:347 26:348 27:349 28:350 29:351 30:352 31:353 32:353 33:354 34:355 35:356 36:357 37:358 38:359 39:360 40:361 41:361 42:361 43:361 44:362 45:363 46:364 47:365 48:366 49:367 50:367 51:368 52:369 53:370 54:371 55:372 56:373 57:373 58:374 59:375 60:376 61:377 62:378 63:379 64:380 65:381 66:382 67:383 68:384 69:385 70:386 71:386 72:387 73:388 74:389 75:390 76:391 77:392 78:393 79:394 80:395 81:396 82:397 83:398 84:399 85:399 86:400 87:401 88:402 89:403 90:404 91:405 92:406 93:407 94:408 95:409 96:410 97:411 98:412 99:412 100:413 101:414 102:415 103:415 104:415 105:416 106:417 107:418 108:419 109:420 110:421 111:422 112:423 113:424 114:425 115:425 116:426 117:427 118:428 119:429 120:430 121:431 122:432 123:433 124:434 125:435 126:436 127:437 128:438 129:438 130:439 131:440 132:441 133:441 134:441 135:442 136:443 137:444 138:445 139:446 140:447 141:448 142:449 143:450 144:451 145:451 146:452 147:453 148:454 149:454 150:454 151:455 152:456 153:457 154:458 155:459 156:460 157:461 158:462 159:463 160:464 161:464 162:464 163:465 164:466 165:467 166:468 167:469 168:470 169:471 170:472 171:473 172:474 173:475 174:476 175:477 176:477 177:478 178:479 179:480 180:481 181:482 182:483 183:484 184:485 185:486 186:487 187:488 188:489 189:490 190:490 191:491 192:492 193:493 194:494 195:495 196:496 197:497 198:498 199:499 200:500 201:501 202:502 203:503 204:503 205:504 206:505 207:506 208:506 209:506 210:507 211:508 212:509 213:510 214:511 215:512 216:513 217:514 218:515 219:516 220:516 221:517 222:518 223:519 224:520 225:521 226:522 227:523 228:524 229:525 230:526 231:527 232:528 233:529 234:529 235:530 236:531 237:532 238:532 239:532 240:533 241:534 242:535 243:536 244:537 245:538 246:539 247:540 248:541 249:542 250:542 251:543 252:544 253:545 254:545 255:545 256:546 257:547 258:548 259:549 260:550 261:551 262:552 263:553 264:554 265:555 266:555 267:555 268:556 269:557 270:558 271:559 272:560 273:561 274:562 275:563 276:564 277:565 278:566 279:567 280:568 281:569 282:569 283:570 284:571 285:572 286:573 287:574 288:575 289:576 290:577 291:578 292:579 293:580 294:581 295:582 296:583 297:583 298:584 299:585 300:586 301:587 302:588 303:589 304:590 305:591 306:592 307:593 308:594 309:595 310:596 311:597 312:597 313:598 314:599 315:600 316:601 317:602 318:603 319:604 320:605 321:606 322:607 323:608 324:609 325:610 326:611 327:611 328:611 329:611 330:612 331:613 332:614 333:615 334:616 335:617 336:618 337:619 338:620 339:621 340:622 341:623 342:624 343:625 344:625 345:626 346:627 347:628 348:629 349:630 350:631 351:632 352:633 353:634 354:635 355:636 356:637 357:638 358:639 359:639 360:639 361:639 362:640 363:641 364:642 365:643 366:644 367:645 368:646 369:647 370:648 371:649 372:650 373:651 374:651 375:652 376:653 377:654 378:655 379:656 380:657 381:657 382:658\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 11712 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000094\n",
            "INFO:tensorflow:example_index: 18\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by quantity [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:446 12:447 13:448 14:449 15:450 16:451 17:451 18:452 19:453 20:454 21:454 22:454 23:455 24:456 25:457 26:458 27:459 28:460 29:461 30:462 31:463 32:464 33:464 34:464 35:465 36:466 37:467 38:468 39:469 40:470 41:471 42:472 43:473 44:474 45:475 46:476 47:477 48:477 49:478 50:479 51:480 52:481 53:482 54:483 55:484 56:485 57:486 58:487 59:488 60:489 61:490 62:490 63:491 64:492 65:493 66:494 67:495 68:496 69:497 70:498 71:499 72:500 73:501 74:502 75:503 76:503 77:504 78:505 79:506 80:506 81:506 82:507 83:508 84:509 85:510 86:511 87:512 88:513 89:514 90:515 91:516 92:516 93:517 94:518 95:519 96:520 97:521 98:522 99:523 100:524 101:525 102:526 103:527 104:528 105:529 106:529 107:530 108:531 109:532 110:532 111:532 112:533 113:534 114:535 115:536 116:537 117:538 118:539 119:540 120:541 121:542 122:542 123:543 124:544 125:545 126:545 127:545 128:546 129:547 130:548 131:549 132:550 133:551 134:552 135:553 136:554 137:555 138:555 139:555 140:556 141:557 142:558 143:559 144:560 145:561 146:562 147:563 148:564 149:565 150:566 151:567 152:568 153:569 154:569 155:570 156:571 157:572 158:573 159:574 160:575 161:576 162:577 163:578 164:579 165:580 166:581 167:582 168:583 169:583 170:584 171:585 172:586 173:587 174:588 175:589 176:590 177:591 178:592 179:593 180:594 181:595 182:596 183:597 184:597 185:598 186:599 187:600 188:601 189:602 190:603 191:604 192:605 193:606 194:607 195:608 196:609 197:610 198:611 199:611 200:611 201:611 202:612 203:613 204:614 205:615 206:616 207:617 208:618 209:619 210:620 211:621 212:622 213:623 214:624 215:625 216:625 217:626 218:627 219:628 220:629 221:630 222:631 223:632 224:633 225:634 226:635 227:636 228:637 229:638 230:639 231:639 232:639 233:639 234:640 235:641 236:642 237:643 238:644 239:645 240:646 241:647 242:648 243:649 244:650 245:651 246:651 247:652 248:653 249:654 250:655 251:656 252:657 253:657 254:658 255:659 256:660 257:661 258:662 259:663 260:664 261:665 262:665 263:666 264:667 265:668 266:669 267:670 268:671 269:671 270:672 271:673 272:674 273:675 274:676 275:677 276:678 277:679 278:679 279:680 280:681 281:682 282:683 283:684 284:685 285:685 286:686 287:687 288:688 289:689 290:690 291:691 292:692 293:693 294:693 295:694 296:695 297:696 298:697 299:698 300:699 301:699 302:700 303:701 304:702 305:703 306:704 307:705 308:706 309:707 310:707 311:707 312:707 313:708 314:709 315:710 316:711 317:712 318:713 319:713 320:714 321:715 322:716 323:717 324:718 325:719 326:720 327:721 328:721 329:722 330:723 331:724 332:725 333:726 334:727 335:727 336:728 337:729 338:730 339:731 340:732 341:733 342:734 343:735 344:735 345:735 346:735 347:736 348:737 349:738 350:739 351:740 352:741 353:741 354:742 355:743 356:744 357:745 358:746 359:747 360:747\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 11712 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000095\n",
            "INFO:tensorflow:example_index: 19\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:19 32:20 33:21 34:22 35:23 36:24 37:25 38:25 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:36 50:37 51:38 52:38 53:39 54:40 55:41 56:41 57:41 58:42 59:43 60:44 61:45 62:46 63:47 64:48 65:49 66:50 67:51 68:51 69:52 70:53 71:54 72:55 73:56 74:57 75:58 76:59 77:60 78:61 79:62 80:63 81:64 82:64 83:65 84:66 85:67 86:67 87:67 88:68 89:69 90:70 91:71 92:72 93:73 94:74 95:75 96:76 97:77 98:77 99:78 100:79 101:80 102:80 103:80 104:81 105:82 106:83 107:84 108:85 109:86 110:87 111:88 112:89 113:90 114:90 115:90 116:91 117:92 118:93 119:94 120:95 121:96 122:97 123:98 124:99 125:100 126:101 127:102 128:103 129:103 130:104 131:105 132:106 133:107 134:108 135:109 136:110 137:111 138:112 139:113 140:114 141:115 142:116 143:116 144:117 145:118 146:119 147:120 148:121 149:122 150:123 151:124 152:125 153:126 154:127 155:128 156:129 157:129 158:130 159:131 160:132 161:132 162:132 163:133 164:134 165:135 166:136 167:137 168:138 169:139 170:140 171:141 172:142 173:142 174:143 175:144 176:145 177:146 178:147 179:148 180:149 181:150 182:151 183:152 184:153 185:154 186:155 187:155 188:156 189:157 190:158 191:158 192:158 193:159 194:160 195:161 196:162 197:163 198:164 199:165 200:166 201:167 202:168 203:168 204:169 205:170 206:171 207:171 208:171 209:172 210:173 211:174 212:175 213:176 214:177 215:178 216:179 217:180 218:181 219:181 220:181 221:182 222:183 223:184 224:185 225:186 226:187 227:188 228:189 229:190 230:191 231:192 232:193 233:194 234:195 235:195 236:196 237:197 238:198 239:199 240:200 241:201 242:202 243:203 244:204 245:205 246:206 247:207 248:208 249:209 250:209 251:210 252:211 253:212 254:213 255:214 256:215 257:216 258:217 259:218 260:219 261:220 262:221 263:222 264:223 265:223 266:224 267:225 268:226 269:227 270:228 271:229 272:230 273:231 274:232 275:233 276:234 277:235 278:236 279:237 280:237 281:237 282:237 283:238 284:239 285:240 286:241 287:242 288:243 289:244 290:245 291:246 292:247 293:248 294:249 295:250 296:251 297:251 298:252 299:253 300:254 301:255 302:256 303:257 304:258 305:259 306:260 307:261 308:262 309:263 310:264 311:265 312:265 313:265 314:265 315:266 316:267 317:268 318:269 319:270 320:271 321:272 322:273 323:274 324:275 325:276 326:277 327:277 328:278 329:279 330:280 331:281 332:282 333:283 334:283 335:284 336:285 337:286 338:287 339:288 340:289 341:290 342:291 343:291 344:292 345:293 346:294 347:295 348:296 349:297 350:297 351:298 352:299 353:300 354:301 355:302 356:303 357:304 358:305 359:305 360:306 361:307 362:308 363:309 364:310 365:311 366:311 367:312 368:313 369:314 370:315 371:316 372:317 373:318 374:319 375:319 376:320 377:321 378:322 379:323 380:324 381:325 382:325\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000096\n",
            "INFO:tensorflow:example_index: 19\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:113 12:114 13:115 14:116 15:116 16:117 17:118 18:119 19:120 20:121 21:122 22:123 23:124 24:125 25:126 26:127 27:128 28:129 29:129 30:130 31:131 32:132 33:132 34:132 35:133 36:134 37:135 38:136 39:137 40:138 41:139 42:140 43:141 44:142 45:142 46:143 47:144 48:145 49:146 50:147 51:148 52:149 53:150 54:151 55:152 56:153 57:154 58:155 59:155 60:156 61:157 62:158 63:158 64:158 65:159 66:160 67:161 68:162 69:163 70:164 71:165 72:166 73:167 74:168 75:168 76:169 77:170 78:171 79:171 80:171 81:172 82:173 83:174 84:175 85:176 86:177 87:178 88:179 89:180 90:181 91:181 92:181 93:182 94:183 95:184 96:185 97:186 98:187 99:188 100:189 101:190 102:191 103:192 104:193 105:194 106:195 107:195 108:196 109:197 110:198 111:199 112:200 113:201 114:202 115:203 116:204 117:205 118:206 119:207 120:208 121:209 122:209 123:210 124:211 125:212 126:213 127:214 128:215 129:216 130:217 131:218 132:219 133:220 134:221 135:222 136:223 137:223 138:224 139:225 140:226 141:227 142:228 143:229 144:230 145:231 146:232 147:233 148:234 149:235 150:236 151:237 152:237 153:237 154:237 155:238 156:239 157:240 158:241 159:242 160:243 161:244 162:245 163:246 164:247 165:248 166:249 167:250 168:251 169:251 170:252 171:253 172:254 173:255 174:256 175:257 176:258 177:259 178:260 179:261 180:262 181:263 182:264 183:265 184:265 185:265 186:265 187:266 188:267 189:268 190:269 191:270 192:271 193:272 194:273 195:274 196:275 197:276 198:277 199:277 200:278 201:279 202:280 203:281 204:282 205:283 206:283 207:284 208:285 209:286 210:287 211:288 212:289 213:290 214:291 215:291 216:292 217:293 218:294 219:295 220:296 221:297 222:297 223:298 224:299 225:300 226:301 227:302 228:303 229:304 230:305 231:305 232:306 233:307 234:308 235:309 236:310 237:311 238:311 239:312 240:313 241:314 242:315 243:316 244:317 245:318 246:319 247:319 248:320 249:321 250:322 251:323 252:324 253:325 254:325 255:326 256:327 257:328 258:329 259:330 260:331 261:332 262:333 263:333 264:333 265:333 266:334 267:335 268:336 269:337 270:338 271:339 272:339 273:340 274:341 275:342 276:343 277:344 278:345 279:346 280:347 281:347 282:348 283:349 284:350 285:351 286:352 287:353 288:353 289:354 290:355 291:356 292:357 293:358 294:359 295:360 296:361 297:361 298:361 299:361 300:362 301:363 302:364 303:365 304:366 305:367 306:367 307:368 308:369 309:370 310:371 311:372 312:373 313:373 314:374 315:375 316:376 317:377 318:378 319:379 320:380 321:381 322:382 323:383 324:384 325:385 326:386 327:386 328:387 329:388 330:389 331:390 332:391 333:392 334:393 335:394 336:395 337:396 338:397 339:398 340:399 341:399 342:400 343:401 344:402 345:403 346:404 347:405 348:406 349:407 350:408 351:409 352:410 353:411 354:412 355:412 356:413 357:414 358:415 359:415 360:415 361:416 362:417 363:418 364:419 365:420 366:421 367:422 368:423 369:424 370:425 371:425 372:426 373:427 374:428 375:429 376:430 377:431 378:432 379:433 380:434 381:435 382:436\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 0\n",
            "INFO:tensorflow:end_position: 0\n",
            "INFO:tensorflow:answer: [CLS]\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000097\n",
            "INFO:tensorflow:example_index: 19\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:225 12:226 13:227 14:228 15:229 16:230 17:231 18:232 19:233 20:234 21:235 22:236 23:237 24:237 25:237 26:237 27:238 28:239 29:240 30:241 31:242 32:243 33:244 34:245 35:246 36:247 37:248 38:249 39:250 40:251 41:251 42:252 43:253 44:254 45:255 46:256 47:257 48:258 49:259 50:260 51:261 52:262 53:263 54:264 55:265 56:265 57:265 58:265 59:266 60:267 61:268 62:269 63:270 64:271 65:272 66:273 67:274 68:275 69:276 70:277 71:277 72:278 73:279 74:280 75:281 76:282 77:283 78:283 79:284 80:285 81:286 82:287 83:288 84:289 85:290 86:291 87:291 88:292 89:293 90:294 91:295 92:296 93:297 94:297 95:298 96:299 97:300 98:301 99:302 100:303 101:304 102:305 103:305 104:306 105:307 106:308 107:309 108:310 109:311 110:311 111:312 112:313 113:314 114:315 115:316 116:317 117:318 118:319 119:319 120:320 121:321 122:322 123:323 124:324 125:325 126:325 127:326 128:327 129:328 130:329 131:330 132:331 133:332 134:333 135:333 136:333 137:333 138:334 139:335 140:336 141:337 142:338 143:339 144:339 145:340 146:341 147:342 148:343 149:344 150:345 151:346 152:347 153:347 154:348 155:349 156:350 157:351 158:352 159:353 160:353 161:354 162:355 163:356 164:357 165:358 166:359 167:360 168:361 169:361 170:361 171:361 172:362 173:363 174:364 175:365 176:366 177:367 178:367 179:368 180:369 181:370 182:371 183:372 184:373 185:373 186:374 187:375 188:376 189:377 190:378 191:379 192:380 193:381 194:382 195:383 196:384 197:385 198:386 199:386 200:387 201:388 202:389 203:390 204:391 205:392 206:393 207:394 208:395 209:396 210:397 211:398 212:399 213:399 214:400 215:401 216:402 217:403 218:404 219:405 220:406 221:407 222:408 223:409 224:410 225:411 226:412 227:412 228:413 229:414 230:415 231:415 232:415 233:416 234:417 235:418 236:419 237:420 238:421 239:422 240:423 241:424 242:425 243:425 244:426 245:427 246:428 247:429 248:430 249:431 250:432 251:433 252:434 253:435 254:436 255:437 256:438 257:438 258:439 259:440 260:441 261:441 262:441 263:442 264:443 265:444 266:445 267:446 268:447 269:448 270:449 271:450 272:451 273:451 274:452 275:453 276:454 277:454 278:454 279:455 280:456 281:457 282:458 283:459 284:460 285:461 286:462 287:463 288:464 289:464 290:464 291:465 292:466 293:467 294:468 295:469 296:470 297:471 298:472 299:473 300:474 301:475 302:476 303:477 304:477 305:478 306:479 307:480 308:481 309:482 310:483 311:484 312:485 313:486 314:487 315:488 316:489 317:490 318:490 319:491 320:492 321:493 322:494 323:495 324:496 325:497 326:498 327:499 328:500 329:501 330:502 331:503 332:503 333:504 334:505 335:506 336:506 337:506 338:507 339:508 340:509 341:510 342:511 343:512 344:513 345:514 346:515 347:516 348:516 349:517 350:518 351:519 352:520 353:521 354:522 355:523 356:524 357:525 358:526 359:527 360:528 361:529 362:529 363:530 364:531 365:532 366:532 367:532 368:533 369:534 370:535 371:536 372:537 373:538 374:539 375:540 376:541 377:542 378:542 379:543 380:544 381:545 382:545\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 363\n",
            "INFO:tensorflow:end_position: 378\n",
            "INFO:tensorflow:answer: the bottom household _ type for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000098\n",
            "INFO:tensorflow:example_index: 19\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:335 12:336 13:337 14:338 15:339 16:339 17:340 18:341 19:342 20:343 21:344 22:345 23:346 24:347 25:347 26:348 27:349 28:350 29:351 30:352 31:353 32:353 33:354 34:355 35:356 36:357 37:358 38:359 39:360 40:361 41:361 42:361 43:361 44:362 45:363 46:364 47:365 48:366 49:367 50:367 51:368 52:369 53:370 54:371 55:372 56:373 57:373 58:374 59:375 60:376 61:377 62:378 63:379 64:380 65:381 66:382 67:383 68:384 69:385 70:386 71:386 72:387 73:388 74:389 75:390 76:391 77:392 78:393 79:394 80:395 81:396 82:397 83:398 84:399 85:399 86:400 87:401 88:402 89:403 90:404 91:405 92:406 93:407 94:408 95:409 96:410 97:411 98:412 99:412 100:413 101:414 102:415 103:415 104:415 105:416 106:417 107:418 108:419 109:420 110:421 111:422 112:423 113:424 114:425 115:425 116:426 117:427 118:428 119:429 120:430 121:431 122:432 123:433 124:434 125:435 126:436 127:437 128:438 129:438 130:439 131:440 132:441 133:441 134:441 135:442 136:443 137:444 138:445 139:446 140:447 141:448 142:449 143:450 144:451 145:451 146:452 147:453 148:454 149:454 150:454 151:455 152:456 153:457 154:458 155:459 156:460 157:461 158:462 159:463 160:464 161:464 162:464 163:465 164:466 165:467 166:468 167:469 168:470 169:471 170:472 171:473 172:474 173:475 174:476 175:477 176:477 177:478 178:479 179:480 180:481 181:482 182:483 183:484 184:485 185:486 186:487 187:488 188:489 189:490 190:490 191:491 192:492 193:493 194:494 195:495 196:496 197:497 198:498 199:499 200:500 201:501 202:502 203:503 204:503 205:504 206:505 207:506 208:506 209:506 210:507 211:508 212:509 213:510 214:511 215:512 216:513 217:514 218:515 219:516 220:516 221:517 222:518 223:519 224:520 225:521 226:522 227:523 228:524 229:525 230:526 231:527 232:528 233:529 234:529 235:530 236:531 237:532 238:532 239:532 240:533 241:534 242:535 243:536 244:537 245:538 246:539 247:540 248:541 249:542 250:542 251:543 252:544 253:545 254:545 255:545 256:546 257:547 258:548 259:549 260:550 261:551 262:552 263:553 264:554 265:555 266:555 267:555 268:556 269:557 270:558 271:559 272:560 273:561 274:562 275:563 276:564 277:565 278:566 279:567 280:568 281:569 282:569 283:570 284:571 285:572 286:573 287:574 288:575 289:576 290:577 291:578 292:579 293:580 294:581 295:582 296:583 297:583 298:584 299:585 300:586 301:587 302:588 303:589 304:590 305:591 306:592 307:593 308:594 309:595 310:596 311:597 312:597 313:598 314:599 315:600 316:601 317:602 318:603 319:604 320:605 321:606 322:607 323:608 324:609 325:610 326:611 327:611 328:611 329:611 330:612 331:613 332:614 333:615 334:616 335:617 336:618 337:619 338:620 339:621 340:622 341:623 342:624 343:625 344:625 345:626 346:627 347:628 348:629 349:630 350:631 351:632 352:633 353:634 354:635 355:636 356:637 357:638 358:639 359:639 360:639 361:639 362:640 363:641 364:642 365:643 366:644 367:645 368:646 369:647 370:648 371:649 372:650 373:651 374:651 375:652 376:653 377:654 378:655 379:656 380:657 381:657 382:658\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:False 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:start_position: 235\n",
            "INFO:tensorflow:end_position: 250\n",
            "INFO:tensorflow:answer: the bottom household _ type for sales between jul 2015 and jun 2017 was this .\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000099\n",
            "INFO:tensorflow:example_index: 19\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the bottom household _ type by sales [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:446 12:447 13:448 14:449 15:450 16:451 17:451 18:452 19:453 20:454 21:454 22:454 23:455 24:456 25:457 26:458 27:459 28:460 29:461 30:462 31:463 32:464 33:464 34:464 35:465 36:466 37:467 38:468 39:469 40:470 41:471 42:472 43:473 44:474 45:475 46:476 47:477 48:477 49:478 50:479 51:480 52:481 53:482 54:483 55:484 56:485 57:486 58:487 59:488 60:489 61:490 62:490 63:491 64:492 65:493 66:494 67:495 68:496 69:497 70:498 71:499 72:500 73:501 74:502 75:503 76:503 77:504 78:505 79:506 80:506 81:506 82:507 83:508 84:509 85:510 86:511 87:512 88:513 89:514 90:515 91:516 92:516 93:517 94:518 95:519 96:520 97:521 98:522 99:523 100:524 101:525 102:526 103:527 104:528 105:529 106:529 107:530 108:531 109:532 110:532 111:532 112:533 113:534 114:535 115:536 116:537 117:538 118:539 119:540 120:541 121:542 122:542 123:543 124:544 125:545 126:545 127:545 128:546 129:547 130:548 131:549 132:550 133:551 134:552 135:553 136:554 137:555 138:555 139:555 140:556 141:557 142:558 143:559 144:560 145:561 146:562 147:563 148:564 149:565 150:566 151:567 152:568 153:569 154:569 155:570 156:571 157:572 158:573 159:574 160:575 161:576 162:577 163:578 164:579 165:580 166:581 167:582 168:583 169:583 170:584 171:585 172:586 173:587 174:588 175:589 176:590 177:591 178:592 179:593 180:594 181:595 182:596 183:597 184:597 185:598 186:599 187:600 188:601 189:602 190:603 191:604 192:605 193:606 194:607 195:608 196:609 197:610 198:611 199:611 200:611 201:611 202:612 203:613 204:614 205:615 206:616 207:617 208:618 209:619 210:620 211:621 212:622 213:623 214:624 215:625 216:625 217:626 218:627 219:628 220:629 221:630 222:631 223:632 224:633 225:634 226:635 227:636 228:637 229:638 230:639 231:639 232:639 233:639 234:640 235:641 236:642 237:643 238:644 239:645 240:646 241:647 242:648 243:649 244:650 245:651 246:651 247:652 248:653 249:654 250:655 251:656 252:657 253:657 254:658 255:659 256:660 257:661 258:662 259:663 260:664 261:665 262:665 263:666 264:667 265:668 266:669 267:670 268:671 269:671 270:672 271:673 272:674 273:675 274:676 275:677 276:678 277:679 278:679 279:680 280:681 281:682 282:683 283:684 284:685 285:685 286:686 287:687 288:688 289:689 290:690 291:691 292:692 293:693 294:693 295:694 296:695 297:696 298:697 299:698 300:699 301:699 302:700 303:701 304:702 305:703 306:704 307:705 308:706 309:707 310:707 311:707 312:707 313:708 314:709 315:710 316:711 317:712 318:713 319:713 320:714 321:715 322:716 323:717 324:718 325:719 326:720 327:721 328:721 329:722 330:723 331:724 332:725 333:726 334:727 335:727 336:728 337:729 338:730 339:731 340:732 341:733 342:734 343:735 344:735 345:735 346:735 347:736 348:737 349:738 350:739 351:740 352:741 353:741 354:742 355:743 356:744 357:745 358:746 359:747 360:747\n",
            "INFO:tensorflow:token_is_max_context: 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3953 4398 1035 2828 2011 4341 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:start_position: 107\n",
            "INFO:tensorflow:end_position: 122\n",
            "INFO:tensorflow:answer: the bottom household _ type for sales between jul 2015 and jun 2017 was this .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YOsRUlQ3UHXs",
        "colab_type": "code",
        "outputId": "d358ee93-d9a0-4d8c-f258-94089a400a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "#num_train_steps=1500\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "num_train_steps"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "Vts0UbvPQJ1-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=init_checkpoint,\n",
        "      learning_rate=learning_rate,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lG7sil9fUMYC",
        "colab_type": "code",
        "outputId": "1eaaa076-17e4-49b7-de51-6d89c146172c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'output_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3b172c9be0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vhaz76C0qix3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  final_hidden = model.get_sequence_output()\n",
        "\n",
        "  final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\n",
        "  batch_size = final_hidden_shape[0]\n",
        "  seq_length = final_hidden_shape[1]\n",
        "  hidden_size = final_hidden_shape[2]\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"cls/squad/output_weights\", [2, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
        "\n",
        "  final_hidden_matrix = tf.reshape(final_hidden,\n",
        "                                   [batch_size * seq_length, hidden_size])\n",
        "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
        "  logits = tf.nn.bias_add(logits, output_bias)\n",
        "\n",
        "  logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
        "  logits = tf.transpose(logits, [2, 0, 1])\n",
        "\n",
        "  unstacked_logits = tf.unstack(logits, axis=0)\n",
        "\n",
        "  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
        "\n",
        "  return (start_logits, end_logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aPyLsFwlUw95",
        "colab_type": "code",
        "outputId": "5fea1c41-16dc-4f0c-a0b2-69a6e0b7fe27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4202
        }
      },
      "cell_type": "code",
      "source": [
        "train_input_fn = run_squad.input_fn_builder(\n",
        "        input_file=train_writer.filename,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_squad.py:730: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_squad.py:710: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = end_positions, shape = (?,)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 384)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 384)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 384)\n",
            "INFO:tensorflow:  name = start_positions, shape = (?,)\n",
            "INFO:tensorflow:  name = unique_ids, shape = (?,)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = cls/squad/output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = cls/squad/output_bias:0, shape = (2,)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into output_model/model.ckpt.\n",
            "INFO:tensorflow:loss = 5.7776318, step = 0\n",
            "INFO:tensorflow:Saving checkpoints for 21 into output_model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 2.3508282.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f3b172c9a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "am19PSRVeOKe",
        "colab_type": "code",
        "outputId": "9e8ecae9-9550-4265-e405-fe0b897c22a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/My\\ Drive/Colab\\ Notebooks/data/output_model.zip output_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: output_model/ (stored 0%)\n",
            "updating: output_model/model.ckpt-500.meta (deflated 92%)\n",
            "updating: output_model/model.ckpt-0.index (deflated 77%)\n",
            "updating: output_model/graph.pbtxt (deflated 97%)\n",
            "updating: output_model/checkpoint (deflated 67%)\n",
            "updating: output_model/model.ckpt-500.data-00000-of-00001 (deflated 11%)\n",
            "updating: output_model/model.ckpt-0.meta (deflated 92%)\n",
            "updating: output_model/model.ckpt-500.index (deflated 69%)\n",
            "updating: output_model/model.ckpt-0.data-00000-of-00001 (deflated 69%)\n",
            "updating: output_model/train.tf_record (deflated 78%)\n",
            "updating: output_model/eval.tf_record (deflated 78%)\n",
            "  adding: output_model/model.ckpt-1000.data-00000-of-00001 (deflated 11%)\n",
            "  adding: output_model/model.ckpt-1000.meta (deflated 92%)\n",
            "  adding: output_model/model.ckpt-1000.index (deflated 69%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-tit_iA0vPVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test out examples now"
      ]
    },
    {
      "metadata": {
        "id": "8DbbNo0vvWWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_file = \"/content/drive/My Drive/Colab Notebooks/data/dev-v2.0.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PVVvEmx-vF7j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_examples = read_squad_examples(\n",
        "        input_file=test_file, is_training=False,version_2_with_negative=True)\n",
        "# Get random sample of it because of size\n",
        "eval_examples = [eval_examples[random.randrange(len(eval_examples))]\n",
        "              for item in range(1000)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Go3muLCKvgzZ",
        "colab_type": "code",
        "outputId": "bf02efcd-af2c-426d-f3ae-e136f2e278b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3437
        }
      },
      "cell_type": "code",
      "source": [
        "eval_writer = run_squad.FeatureWriter(\n",
        "        filename=os.path.join(output_dir, \"eval.tf_record\"),\n",
        "        is_training=False)\n",
        "eval_features = []\n",
        "\n",
        "def append_feature(feature):\n",
        "  eval_features.append(feature)\n",
        "  eval_writer.process_feature(feature)\n",
        "\n",
        "run_squad.convert_examples_to_features(\n",
        "        examples=eval_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=max_seq_length,\n",
        "        doc_stride=doc_stride,\n",
        "        max_query_length=max_query_length,\n",
        "        is_training=False,\n",
        "        output_fn=append_feature)\n",
        "eval_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000000\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] in what irish city can few signs remain to be seen of the hug ##uen ##ots ? [SEP] a number of hug ##uen ##ots served as mayors in dublin , cork , you ##gh ##al and waterford in the 17th and 18th centuries . numerous signs of hug ##uen ##ot presence can still be seen with names still in use , and with areas of the main towns and cities named after the people who settled there . examples include the hug ##uen ##ot district and french church street in cork city ; and d ' ol ##ier street in dublin , named after a high sheriff and one of the founders of the bank of ireland . a french church in port ##ar ##lington dates back to 169 ##6 , and was built to serve the significant new hug ##uen ##ot community in the town . at the time , they constituted the majority of the townspeople . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 19:0 20:1 21:2 22:3 23:3 24:3 25:4 26:5 27:6 28:7 29:8 30:8 31:9 32:9 33:10 34:10 35:10 36:11 37:12 38:13 39:14 40:15 41:16 42:17 43:18 44:18 45:19 46:20 47:21 48:22 49:22 50:22 51:23 52:24 53:25 54:26 55:27 56:28 57:29 58:30 59:31 60:32 61:32 62:33 63:34 64:35 65:36 66:37 67:38 68:39 69:40 70:41 71:42 72:43 73:44 74:45 75:46 76:47 77:48 78:48 79:49 80:50 81:51 82:52 83:52 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:59 92:60 93:60 94:61 95:62 96:62 97:62 98:62 99:63 100:64 101:65 102:65 103:66 104:67 105:68 106:69 107:70 108:71 109:72 110:73 111:74 112:75 113:76 114:77 115:78 116:79 117:80 118:80 119:81 120:82 121:83 122:84 123:85 124:85 125:85 126:86 127:87 128:88 129:89 130:89 131:89 132:90 133:91 134:92 135:93 136:94 137:95 138:96 139:97 140:98 141:98 142:98 143:99 144:100 145:101 146:102 147:102 148:103 149:104 150:105 151:105 152:106 153:107 154:108 155:109 156:110 157:111 158:112 159:112\n",
            "INFO:tensorflow:token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True\n",
            "INFO:tensorflow:input_ids: 101 1999 2054 3493 2103 2064 2261 5751 3961 2000 2022 2464 1997 1996 8549 24997 12868 1029 102 1037 2193 1997 8549 24997 12868 2366 2004 21941 1999 5772 1010 8513 1010 2017 5603 2389 1998 17769 1999 1996 5550 1998 4985 4693 1012 3365 5751 1997 8549 24997 4140 3739 2064 2145 2022 2464 2007 3415 2145 1999 2224 1010 1998 2007 2752 1997 1996 2364 4865 1998 3655 2315 2044 1996 2111 2040 3876 2045 1012 4973 2421 1996 8549 24997 4140 2212 1998 2413 2277 2395 1999 8513 2103 1025 1998 1040 1005 19330 3771 2395 1999 5772 1010 2315 2044 1037 2152 6458 1998 2028 1997 1996 8759 1997 1996 2924 1997 3163 1012 1037 2413 2277 1999 3417 2906 18722 5246 2067 2000 18582 2575 1010 1998 2001 2328 2000 3710 1996 3278 2047 8549 24997 4140 2451 1999 1996 2237 1012 2012 1996 2051 1010 2027 11846 1996 3484 1997 1996 27938 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000001\n",
            "INFO:tensorflow:example_index: 1\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] harold winston ' s supported which country ' s during its six day war ? [SEP] the em ##bar ##go was not uniform across europe . of the nine members of the european economic community ( ee ##c ) , the netherlands faced a complete em ##bar ##go , the uk and france received almost un ##int ##er ##rup ##ted supplies ( having refused to allow america to use their airfields and em ##bar ##go ##ed arms and supplies to both the arabs and the israelis ) , while the other six faced partial cut ##backs . the uk had traditionally been an ally of israel , and harold wilson ' s government supported the israelis during the six - day war . his successor , ted heath , reversed this policy in 1970 , calling for israel to withdraw to its pre - 1967 borders . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 17:0 18:1 19:1 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:11 32:12 33:13 34:14 35:15 36:16 37:16 38:16 39:16 40:16 41:17 42:18 43:19 44:20 45:21 46:22 47:22 48:22 49:22 50:23 51:24 52:25 53:26 54:27 55:28 56:29 57:29 58:29 59:29 60:29 61:30 62:31 63:31 64:32 65:33 66:34 67:35 68:36 69:37 70:38 71:39 72:40 73:41 74:41 75:41 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:48 84:49 85:50 86:51 87:51 88:51 89:52 90:53 91:54 92:55 93:56 94:57 95:58 96:58 97:58 98:59 99:60 100:61 101:62 102:63 103:64 104:65 105:66 106:67 107:67 108:68 109:69 110:70 111:70 112:70 113:71 114:72 115:73 116:74 117:75 118:76 119:77 120:77 121:77 122:78 123:78 124:79 125:80 126:80 127:81 128:82 129:82 130:83 131:84 132:85 133:86 134:87 135:87 136:88 137:89 138:90 139:91 140:92 141:93 142:94 143:95 144:95 145:95 146:96 147:96\n",
            "INFO:tensorflow:token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True\n",
            "INFO:tensorflow:input_ids: 101 7157 10180 1005 1055 3569 2029 2406 1005 1055 2076 2049 2416 2154 2162 1029 102 1996 7861 8237 3995 2001 2025 6375 2408 2885 1012 1997 1996 3157 2372 1997 1996 2647 3171 2451 1006 25212 2278 1007 1010 1996 4549 4320 1037 3143 7861 8237 3995 1010 1996 2866 1998 2605 2363 2471 4895 18447 2121 21531 3064 6067 1006 2383 4188 2000 3499 2637 2000 2224 2037 25278 1998 7861 8237 3995 2098 2608 1998 6067 2000 2119 1996 14560 1998 1996 28363 1007 1010 2096 1996 2060 2416 4320 7704 3013 12221 1012 1996 2866 2018 6964 2042 2019 9698 1997 3956 1010 1998 7157 4267 1005 1055 2231 3569 1996 28363 2076 1996 2416 1011 2154 2162 1012 2010 6332 1010 6945 9895 1010 11674 2023 3343 1999 3359 1010 4214 2005 3956 2000 10632 2000 2049 3653 1011 3476 6645 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000002\n",
            "INFO:tensorflow:example_index: 2\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is one thing measuring in crystal lattice ##s shows in ash layers ? [SEP] for many geologic applications , isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature , the point at which different radio ##metric isotope ##s stop di ##ff ##using into and out of the crystal lattice . these are used in geo ##ch ##ron ##olo ##gic and the ##rm ##och ##ron ##olo ##gic studies . common methods include uranium - lead dating , potassium - ar ##gon dating , ar ##gon - ar ##gon dating and uranium - thor ##ium dating . these methods are used for a variety of applications . dating of lava and volcanic ash layers found within a st ##rat ##ig ##raphic sequence can provide absolute age data for sedimentary rock units which do not contain radioactive isotope ##s and cal ##ib ##rate relative dating techniques . these methods can also be used to determine ages of pluto ##n em ##pl ##ace ##ment . the ##rm ##oche ##mic ##al techniques can be used to determine temperature profiles within the crust , the up ##lift of mountain ranges , and pale ##oto ##po ##graphy . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 16:0 17:1 18:2 19:3 20:3 21:4 22:5 23:6 24:7 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:16 34:17 35:18 36:19 37:20 38:21 39:22 40:23 41:24 42:25 43:26 44:27 45:28 46:29 47:30 48:30 49:31 50:32 51:33 52:34 53:35 54:36 55:36 56:37 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:51 76:51 77:51 78:51 79:52 80:53 81:53 82:53 83:53 84:53 85:53 86:54 87:54 88:55 89:56 90:57 91:58 92:58 93:58 94:59 95:59 96:60 97:60 98:60 99:60 100:61 101:61 102:62 103:62 104:62 105:62 106:62 107:63 108:64 109:65 110:65 111:65 112:65 113:66 114:66 115:67 116:68 117:69 118:70 119:71 120:72 121:73 122:74 123:75 124:75 125:76 126:77 127:78 128:79 129:80 130:81 131:82 132:83 133:84 134:85 135:86 136:86 137:86 138:86 139:87 140:88 141:89 142:90 143:91 144:92 145:93 146:94 147:95 148:96 149:97 150:98 151:99 152:100 153:101 154:102 155:102 156:103 157:104 158:104 159:104 160:105 161:106 162:107 163:107 164:108 165:109 166:110 167:111 168:112 169:113 170:114 171:115 172:116 173:117 174:118 175:118 176:119 177:119 178:119 179:119 180:119 181:120 182:120 183:120 184:120 185:120 186:121 187:122 188:123 189:124 190:125 191:126 192:127 193:128 194:129 195:130 196:131 197:131 198:132 199:133 200:133 201:134 202:135 203:136 204:136 205:137 206:138 207:138 208:138 209:138 210:138\n",
            "INFO:tensorflow:token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 2028 2518 9854 1999 6121 17779 2015 3065 1999 6683 9014 1029 102 2005 2116 22125 5097 1010 28846 21879 1997 17669 3787 2024 7594 1999 13246 2008 2507 1996 3815 1997 2051 2008 2038 2979 2144 1037 2600 2979 2083 2049 3327 8503 4860 1010 1996 2391 2012 2029 2367 2557 12589 28846 2015 2644 4487 4246 18161 2046 1998 2041 1997 1996 6121 17779 1012 2122 2024 2109 1999 20248 2818 4948 12898 12863 1998 1996 10867 11663 4948 12898 12863 2913 1012 2691 4725 2421 14247 1011 2599 5306 1010 18044 1011 12098 7446 5306 1010 12098 7446 1011 12098 7446 5306 1998 14247 1011 15321 5007 5306 1012 2122 4725 2024 2109 2005 1037 3528 1997 5097 1012 5306 1997 13697 1998 10942 6683 9014 2179 2306 1037 2358 8609 8004 20721 5537 2064 3073 7619 2287 2951 2005 25503 2600 3197 2029 2079 2025 5383 17669 28846 2015 1998 10250 12322 11657 5816 5306 5461 1012 2122 4725 2064 2036 2022 2109 2000 5646 5535 1997 26930 2078 7861 24759 10732 3672 1012 1996 10867 23555 7712 2389 5461 2064 2022 2109 2000 5646 4860 17879 2306 1996 19116 1010 1996 2039 18412 1997 3137 8483 1010 1998 5122 11439 6873 12565 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000003\n",
            "INFO:tensorflow:example_index: 3\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] during what years was william elle ##ry chan ##ning president of harvard ? [SEP] charles w . eliot , president 1869 – 1909 , eliminated the favored position of christianity from the curriculum while opening it to student self - direction . while eliot was the most crucial figure in the secular ##ization of american higher education , he was motivated not by a desire to secular ##ize education , but by trans ##cend ##ental ##ist unitarian convictions . derived from william elle ##ry chan ##ning and ralph waldo emerson , these convictions were focused on the dignity and worth of human nature , the right and ability of each person to perceive truth , and the ind ##well ##ing god in each person . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 15:0 16:1 17:1 18:2 19:2 20:3 21:4 22:4 23:4 24:4 25:5 26:6 27:7 28:8 29:9 30:10 31:11 32:12 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:19 41:19 42:19 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:29 54:30 55:31 56:32 57:33 58:33 59:34 60:35 61:36 62:37 63:38 64:39 65:40 66:41 67:42 68:42 69:43 70:43 71:44 72:45 73:46 74:46 75:46 76:46 77:47 78:48 79:48 80:49 81:50 82:51 83:52 84:52 85:53 86:53 87:54 88:55 89:56 90:57 91:57 92:58 93:59 94:60 95:61 96:62 97:63 98:64 99:65 100:66 101:67 102:68 103:69 104:69 105:70 106:71 107:72 108:73 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:81 118:82 119:82 120:82 121:83 122:84 123:85 124:86 125:86\n",
            "INFO:tensorflow:token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True\n",
            "INFO:tensorflow:input_ids: 101 2076 2054 2086 2001 2520 15317 2854 9212 5582 2343 1997 5765 1029 102 2798 1059 1012 16292 1010 2343 7845 1516 5556 1010 5892 1996 12287 2597 1997 7988 2013 1996 8882 2096 3098 2009 2000 3076 2969 1011 3257 1012 2096 16292 2001 1996 2087 10232 3275 1999 1996 10644 3989 1997 2137 3020 2495 1010 2002 2001 12774 2025 2011 1037 4792 2000 10644 4697 2495 1010 2021 2011 9099 23865 21050 2923 25477 20488 1012 5173 2013 2520 15317 2854 9212 5582 1998 6798 28806 12628 1010 2122 20488 2020 4208 2006 1996 13372 1998 4276 1997 2529 3267 1010 1996 2157 1998 3754 1997 2169 2711 2000 23084 3606 1010 1998 1996 27427 4381 2075 2643 1999 2169 2711 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000004\n",
            "INFO:tensorflow:example_index: 4\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] does packet switching charge a fee when no data is transferred ? [SEP] packet switching contrasts with another principal networking paradigm , circuit switching , a method which pre - all ##oca ##tes dedicated network bandwidth specifically for each communication session , each having a constant bit rate and late ##ncy between nodes . in cases of bill ##able services , such as cellular communication services , circuit switching is characterized by a fee per unit of connection time , even when no data is transferred , while packet switching may be characterized by a fee per unit of information transmitted , such as characters , packets , or messages . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 14:0 15:1 16:2 17:3 18:4 19:5 20:6 21:7 22:7 23:8 24:9 25:9 26:10 27:11 28:12 29:13 30:13 31:13 32:13 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:27 49:28 50:29 51:29 52:30 53:31 54:31 55:32 56:33 57:34 58:35 59:35 60:36 61:36 62:37 63:38 64:39 65:40 66:41 67:41 68:42 69:43 70:44 71:45 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:53 81:54 82:55 83:56 84:57 85:58 86:59 87:59 88:60 89:61 90:62 91:63 92:64 93:65 94:66 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:73 103:74 104:75 105:76 106:76 107:77 108:77 109:78 110:79 111:79\n",
            "INFO:tensorflow:token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True\n",
            "INFO:tensorflow:input_ids: 101 2515 14771 11991 3715 1037 7408 2043 2053 2951 2003 4015 1029 102 14771 11991 23347 2007 2178 4054 14048 20680 1010 4984 11991 1010 1037 4118 2029 3653 1011 2035 24755 4570 4056 2897 20235 4919 2005 2169 4807 5219 1010 2169 2383 1037 5377 2978 3446 1998 2397 9407 2090 14164 1012 1999 3572 1997 3021 3085 2578 1010 2107 2004 12562 4807 2578 1010 4984 11991 2003 7356 2011 1037 7408 2566 3131 1997 4434 2051 1010 2130 2043 2053 2951 2003 4015 1010 2096 14771 11991 2089 2022 7356 2011 1037 7408 2566 3131 1997 2592 11860 1010 2107 2004 3494 1010 23730 1010 2030 7696 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000005\n",
            "INFO:tensorflow:example_index: 5\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is a one class of self molecule ? [SEP] both innate and adaptive immunity depend on the ability of the immune system to distinguish between self and non - self molecules . in im ##mun ##ology , self molecules are those components of an organism ' s body that can be distinguished from foreign substances by the immune system . conversely , non - self molecules are those recognized as foreign molecules . one class of non - self molecules are called antigen ##s ( short for antibody generators ) and are defined as substances that bind to specific immune receptors and eli ##cit an immune response . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:18 30:18 31:18 32:19 33:19 34:20 35:21 36:21 37:21 38:21 39:22 40:23 41:24 42:25 43:26 44:27 45:28 46:29 47:29 48:29 49:30 50:31 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:39 59:40 60:41 61:41 62:42 63:42 64:43 65:43 66:43 67:44 68:45 69:46 70:47 71:48 72:49 73:50 74:50 75:51 76:52 77:53 78:54 79:54 80:54 81:55 82:56 83:57 84:58 85:58 86:59 87:59 88:60 89:61 90:62 91:62 92:63 93:64 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:75 105:75 106:76 107:77 108:78 109:78\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1037 2028 2465 1997 2969 13922 1029 102 2119 25605 1998 19293 15403 12530 2006 1996 3754 1997 1996 11311 2291 2000 10782 2090 2969 1998 2512 1011 2969 10737 1012 1999 10047 23041 6779 1010 2969 10737 2024 2216 6177 1997 2019 15923 1005 1055 2303 2008 2064 2022 5182 2013 3097 13978 2011 1996 11311 2291 1012 18868 1010 2512 1011 2969 10737 2024 2216 3858 2004 3097 10737 1012 2028 2465 1997 2512 1011 2969 10737 2024 2170 28873 2015 1006 2460 2005 27781 16937 1007 1998 2024 4225 2004 13978 2008 14187 2000 3563 11311 13833 1998 12005 26243 2019 11311 3433 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000006\n",
            "INFO:tensorflow:example_index: 6\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] where have oxygen bars been since 1990 ? [SEP] oxygen , as a supposed mild eu ##ph ##oric , has a history of recreational use in oxygen bars and in sports . oxygen bars are establishments , found in japan , california , and las vegas , nevada since the late 1990s that offer higher than normal o 2 exposure for a fee . professional athletes , especially in american football , also sometimes go off field between plays to wear oxygen masks in order to get a \" boost \" in performance . the ph ##arm ##aco ##logical effect is doubtful ; a place ##bo effect is a more likely explanation . available studies support a performance boost from enriched o 2 mixture ##s only if they are breathed during aero ##bic exercise . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 10:0 11:0 12:1 13:2 14:3 15:4 16:5 17:5 18:5 19:5 20:6 21:7 22:8 23:9 24:10 25:11 26:12 27:13 28:14 29:15 30:16 31:17 32:17 33:18 34:19 35:20 36:21 37:21 38:22 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:28 48:29 49:30 50:31 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:39 59:40 60:41 61:42 62:43 63:44 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:50 73:51 74:52 75:53 76:54 77:55 78:56 79:57 80:58 81:59 82:60 83:61 84:62 85:63 86:64 87:65 88:66 89:67 90:67 91:67 92:68 93:69 94:69 95:70 96:71 97:71 98:71 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:76 107:77 108:78 109:79 110:80 111:81 112:82 113:82 114:83 115:84 116:85 117:86 118:87 119:88 120:89 121:90 122:91 123:92 124:93 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:100 134:101 135:101\n",
            "INFO:tensorflow:token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True\n",
            "INFO:tensorflow:input_ids: 101 2073 2031 7722 6963 2042 2144 2901 1029 102 7722 1010 2004 1037 4011 10256 7327 8458 29180 1010 2038 1037 2381 1997 10517 2224 1999 7722 6963 1998 1999 2998 1012 7722 6963 2024 17228 1010 2179 1999 2900 1010 2662 1010 1998 5869 7136 1010 7756 2144 1996 2397 4134 2008 3749 3020 2084 3671 1051 1016 7524 2005 1037 7408 1012 2658 7576 1010 2926 1999 2137 2374 1010 2036 2823 2175 2125 2492 2090 3248 2000 4929 7722 15806 1999 2344 2000 2131 1037 1000 12992 1000 1999 2836 1012 1996 6887 27292 22684 9966 3466 2003 21888 1025 1037 2173 5092 3466 2003 1037 2062 3497 7526 1012 2800 2913 2490 1037 2836 12992 2013 25202 1051 1016 8150 2015 2069 2065 2027 2024 8726 2076 18440 13592 6912 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000007\n",
            "INFO:tensorflow:example_index: 7\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] where is the famous rock the rhine flows around ? [SEP] the dominant economic sectors in the middle rhine area are vin ##ic ##ult ##ure and tourism . the rhine gorge between rude ##sh ##eim am r ##hein and ko ##ble ##nz is listed as a unesco world heritage site . near sank ##t goa ##rs ##hausen , the rhine flows around the famous rock lore ##lei . with its outstanding architectural monuments , the slopes full of vines , settlements crowded on the narrow river banks and scores of castles lined up along the top of the steep slopes , the middle rhine valley can be considered the ep ##ito ##me of the rhine romantic ##ism . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:10 24:10 25:10 26:11 27:12 28:12 29:13 30:14 31:15 32:16 33:17 34:17 35:17 36:18 37:19 38:19 39:20 40:21 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:27 49:28 50:29 51:29 52:30 53:31 54:31 55:32 56:32 57:32 58:32 59:33 60:34 61:35 62:36 63:37 64:38 65:39 66:40 67:40 68:40 69:41 70:42 71:43 72:44 73:45 74:45 75:46 76:47 77:48 78:49 79:50 80:50 81:51 82:52 83:53 84:54 85:55 86:56 87:57 88:58 89:59 90:60 91:61 92:62 93:63 94:64 95:65 96:66 97:67 98:68 99:69 100:70 101:70 102:71 103:72 104:73 105:74 106:75 107:76 108:77 109:78 110:79 111:79 112:79 113:80 114:81 115:82 116:83 117:83 118:83\n",
            "INFO:tensorflow:token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True\n",
            "INFO:tensorflow:input_ids: 101 2073 2003 1996 3297 2600 1996 10950 6223 2105 1029 102 1996 7444 3171 11105 1999 1996 2690 10950 2181 2024 19354 2594 11314 5397 1998 6813 1012 1996 10950 14980 2090 12726 4095 12112 2572 1054 26496 1998 12849 3468 14191 2003 3205 2004 1037 12239 2088 4348 2609 1012 2379 7569 2102 15244 2869 13062 1010 1996 10950 6223 2105 1996 3297 2600 19544 23057 1012 2007 2049 5151 6549 10490 1010 1996 10314 2440 1997 16702 1010 7617 10789 2006 1996 4867 2314 5085 1998 7644 1997 15618 7732 2039 2247 1996 2327 1997 1996 9561 10314 1010 1996 2690 10950 3028 2064 2022 2641 1996 4958 9956 4168 1997 1996 10950 6298 2964 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000008\n",
            "INFO:tensorflow:example_index: 8\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] polynomial time reductions are an example of what ? [SEP] many complexity classes are defined using the concept of a reduction . a reduction is a transformation of one problem into another problem . it captures the informal notion of a problem being at least as difficult as another problem . for instance , if a problem x can be solved using an algorithm for y , x is no more difficult than y , and we say that x reduces to y . there are many different types of reductions , based on the method of reduction , such as cook reductions , ka ##rp reductions and levin reductions , and the bound on the complexity of reductions , such as polynomial - time reductions or log - space reductions . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:36 50:37 51:37 52:38 53:39 54:39 55:40 56:41 57:42 58:43 59:44 60:45 61:46 62:47 63:48 64:49 65:50 66:51 67:51 68:52 69:53 70:54 71:55 72:56 73:57 74:58 75:58 76:59 77:60 78:61 79:62 80:63 81:64 82:65 83:66 84:66 85:67 86:68 87:69 88:70 89:71 90:72 91:73 92:73 93:74 94:75 95:76 96:77 97:78 98:79 99:79 100:80 101:81 102:82 103:83 104:83 105:84 106:84 107:85 108:86 109:87 110:88 111:88 112:89 113:90 114:91 115:92 116:93 117:94 118:95 119:96 120:96 121:97 122:98 123:99 124:99 125:99 126:100 127:101 128:102 129:102 130:102 131:103 132:103\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True\n",
            "INFO:tensorflow:input_ids: 101 17505 2051 25006 2024 2019 2742 1997 2054 1029 102 2116 11619 4280 2024 4225 2478 1996 4145 1997 1037 7312 1012 1037 7312 2003 1037 8651 1997 2028 3291 2046 2178 3291 1012 2009 19566 1996 11900 9366 1997 1037 3291 2108 2012 2560 2004 3697 2004 2178 3291 1012 2005 6013 1010 2065 1037 3291 1060 2064 2022 13332 2478 2019 9896 2005 1061 1010 1060 2003 2053 2062 3697 2084 1061 1010 1998 2057 2360 2008 1060 13416 2000 1061 1012 2045 2024 2116 2367 4127 1997 25006 1010 2241 2006 1996 4118 1997 7312 1010 2107 2004 5660 25006 1010 10556 14536 25006 1998 20206 25006 1010 1998 1996 5391 2006 1996 11619 1997 25006 1010 2107 2004 17505 1011 2051 25006 2030 8833 1011 2686 25006 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000009\n",
            "INFO:tensorflow:example_index: 9\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the weight of a bush ##el of engines in pounds ? [SEP] the historical measure of a steam engine ' s energy efficiency was its \" duty \" . the concept of duty was first introduced by watt in order to illustrate how much more efficient his engines were over the earlier new ##com ##en designs . duty is the number of foot - pounds of work delivered by burning one bush ##el ( 94 pounds ) of coal . the best examples of new ##com ##en designs had a duty of about 7 million , but most were closer to 5 million . watt ' s original low - pressure designs were able to deliver duty as high as 25 million , but averaged about 17 . this was a three - fold improvement over the average new ##com ##en design . early watt engines equipped with high - pressure steam improved this to 65 million . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:6 23:6 24:7 25:8 26:9 27:10 28:11 29:11 30:11 31:11 32:12 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:28 49:29 50:30 51:31 52:32 53:33 54:34 55:35 56:35 57:35 58:36 59:36 60:37 61:38 62:39 63:40 64:41 65:42 66:42 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:49 76:50 77:50 78:51 79:51 80:52 81:53 82:53 83:54 84:55 85:56 86:57 87:58 88:58 89:58 90:59 91:60 92:61 93:62 94:63 95:64 96:65 97:66 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:73 107:74 108:74 109:74 110:75 111:76 112:76 113:76 114:77 115:78 116:79 117:80 118:81 119:82 120:83 121:84 122:85 123:86 124:87 125:87 126:88 127:89 128:90 129:91 130:91 131:92 132:93 133:94 134:95 135:95 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:107 153:107 154:108 155:109 156:110 157:111 158:112 159:113 160:113\n",
            "INFO:tensorflow:token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 3635 1997 1037 5747 2884 1997 5209 1999 7038 1029 102 1996 3439 5468 1997 1037 5492 3194 1005 1055 2943 8122 2001 2049 1000 4611 1000 1012 1996 4145 1997 4611 2001 2034 3107 2011 15231 1999 2344 2000 19141 2129 2172 2062 8114 2010 5209 2020 2058 1996 3041 2047 9006 2368 5617 1012 4611 2003 1996 2193 1997 3329 1011 7038 1997 2147 5359 2011 5255 2028 5747 2884 1006 6365 7038 1007 1997 5317 1012 1996 2190 4973 1997 2047 9006 2368 5617 2018 1037 4611 1997 2055 1021 2454 1010 2021 2087 2020 3553 2000 1019 2454 1012 15231 1005 1055 2434 2659 1011 3778 5617 2020 2583 2000 8116 4611 2004 2152 2004 2423 2454 1010 2021 11398 2055 2459 1012 2023 2001 1037 2093 1011 10671 7620 2058 1996 2779 2047 9006 2368 2640 1012 2220 15231 5209 6055 2007 2152 1011 3778 5492 5301 2023 2000 3515 2454 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000010\n",
            "INFO:tensorflow:example_index: 10\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what term corresponds to the minimum measurement of the time across all functions of n ? [SEP] if the input size is n , the time taken can be expressed as a function of n . since the time taken on different inputs of the same size can be different , the worst - case time complexity t ( n ) is defined to be the maximum time taken over all inputs of size n . if t ( n ) is a polynomial in n , then the algorithm is said to be a polynomial time algorithm . co ##bham ' s thesis says that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:5 25:6 26:7 27:8 28:9 29:10 30:11 31:12 32:13 33:14 34:15 35:16 36:16 37:17 38:18 39:19 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:28 49:29 50:30 51:30 52:31 53:32 54:32 55:32 56:33 57:34 58:35 59:35 60:35 61:35 62:36 63:37 64:38 65:39 66:40 67:41 68:42 69:43 70:44 71:45 72:46 73:47 74:48 75:49 76:49 77:50 78:51 79:51 80:51 81:51 82:52 83:53 84:54 85:55 86:56 87:56 88:57 89:58 90:59 91:60 92:61 93:62 94:63 95:64 96:65 97:66 98:67 99:67 100:68 101:68 102:68 103:68 104:69 105:70 106:71 107:72 108:73 109:74 110:75 111:76 112:77 113:78 114:79 115:80 116:81 117:82 118:83 119:84 120:85 121:86 122:87 123:88 124:89 125:89\n",
            "INFO:tensorflow:token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2744 14788 2000 1996 6263 10903 1997 1996 2051 2408 2035 4972 1997 1050 1029 102 2065 1996 7953 2946 2003 1050 1010 1996 2051 2579 2064 2022 5228 2004 1037 3853 1997 1050 1012 2144 1996 2051 2579 2006 2367 20407 1997 1996 2168 2946 2064 2022 2367 1010 1996 5409 1011 2553 2051 11619 1056 1006 1050 1007 2003 4225 2000 2022 1996 4555 2051 2579 2058 2035 20407 1997 2946 1050 1012 2065 1056 1006 1050 1007 2003 1037 17505 1999 1050 1010 2059 1996 9896 2003 2056 2000 2022 1037 17505 2051 9896 1012 2522 25522 1005 1055 9459 2758 2008 1037 3291 2064 2022 13332 2007 1037 22945 3815 1997 4219 2065 2009 14456 1037 17505 2051 9896 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000011\n",
            "INFO:tensorflow:example_index: 11\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] how is income inequality generally viewed by workers ? [SEP] in a purely capitalist mode of production ( i . e . where professional and labor organizations cannot limit the number of workers ) the workers wages will not be controlled by these organizations , or by the employer , but rather by the market . wages work in the same way as prices for any other good . thus , wages can be considered as a function of market price of skill . and therefore , inequality is driven by this price . under the law of supply and demand , the price of skill is determined by a race between the demand for the skilled worker and the supply of the skilled worker . \" on the other hand , markets can also concentrate wealth , pass environmental costs on to society , and abuse workers and consumers . \" \" markets , by themselves , even when they are stable , often lead to high levels of inequality , outcomes that are widely viewed as unfair . \" employers who offer a below market wage will find that their business is chronic ##ally under ##sta ##ffed . their competitors will take advantage of the situation by offering a higher wage the best of their labor . for a businessman who has the profit motive as the prime interest , it is a losing proposition to offer below or above market wages to workers . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:7 20:7 21:7 22:7 23:8 24:9 25:10 26:11 27:12 28:13 29:14 30:15 31:16 32:17 33:18 34:18 35:19 36:20 37:21 38:22 39:23 40:24 41:25 42:26 43:27 44:28 45:28 46:29 47:30 48:31 49:32 50:32 51:33 52:34 53:35 54:36 55:37 56:37 57:38 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:47 67:48 68:49 69:49 70:50 71:50 72:51 73:52 74:53 75:54 76:55 77:56 78:57 79:58 80:59 81:60 82:61 83:62 84:62 85:63 86:64 87:64 88:65 89:66 90:67 91:68 92:69 93:70 94:70 95:71 96:72 97:73 98:74 99:75 100:76 101:77 102:77 103:78 104:79 105:80 106:81 107:82 108:83 109:84 110:85 111:86 112:87 113:88 114:89 115:90 116:91 117:92 118:93 119:94 120:95 121:96 122:97 123:98 124:99 125:100 126:100 127:101 128:101 129:102 130:103 131:104 132:104 133:105 134:106 135:107 136:108 137:109 138:109 139:110 140:111 141:112 142:113 143:114 144:115 145:115 146:116 147:117 148:118 149:119 150:120 151:120 152:120 153:121 154:121 155:121 156:122 157:123 158:123 159:124 160:125 161:126 162:127 163:128 164:128 165:129 166:130 167:131 168:132 169:133 170:134 171:135 172:135 173:136 174:137 175:138 176:139 177:140 178:141 179:142 180:142 181:142 182:143 183:144 184:145 185:146 186:147 187:148 188:149 189:150 190:151 191:152 192:153 193:154 194:155 195:156 196:156 197:157 198:157 199:157 200:157 201:158 202:159 203:160 204:161 205:162 206:163 207:164 208:165 209:166 210:167 211:168 212:169 213:170 214:171 215:172 216:173 217:174 218:175 219:175 220:176 221:177 222:178 223:179 224:180 225:181 226:182 227:183 228:184 229:185 230:186 231:187 232:187 233:188 234:189 235:190 236:191 237:192 238:193 239:194 240:195 241:196 242:197 243:198 244:199 245:200 246:201 247:201\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True\n",
            "INFO:tensorflow:input_ids: 101 2129 2003 3318 16440 3227 7021 2011 3667 1029 102 1999 1037 11850 19640 5549 1997 2537 1006 1045 1012 1041 1012 2073 2658 1998 4450 4411 3685 5787 1996 2193 1997 3667 1007 1996 3667 12678 2097 2025 2022 4758 2011 2122 4411 1010 2030 2011 1996 11194 1010 2021 2738 2011 1996 3006 1012 12678 2147 1999 1996 2168 2126 2004 7597 2005 2151 2060 2204 1012 2947 1010 12678 2064 2022 2641 2004 1037 3853 1997 3006 3976 1997 8066 1012 1998 3568 1010 16440 2003 5533 2011 2023 3976 1012 2104 1996 2375 1997 4425 1998 5157 1010 1996 3976 1997 8066 2003 4340 2011 1037 2679 2090 1996 5157 2005 1996 10571 7309 1998 1996 4425 1997 1996 10571 7309 1012 1000 2006 1996 2060 2192 1010 6089 2064 2036 10152 7177 1010 3413 4483 5366 2006 2000 2554 1010 1998 6905 3667 1998 10390 1012 1000 1000 6089 1010 2011 3209 1010 2130 2043 2027 2024 6540 1010 2411 2599 2000 2152 3798 1997 16440 1010 13105 2008 2024 4235 7021 2004 15571 1012 1000 12433 2040 3749 1037 2917 3006 11897 2097 2424 2008 2037 2449 2003 11888 3973 2104 9153 15388 1012 2037 10159 2097 2202 5056 1997 1996 3663 2011 5378 1037 3020 11897 1996 2190 1997 2037 4450 1012 2005 1037 6883 2040 2038 1996 5618 15793 2004 1996 3539 3037 1010 2009 2003 1037 3974 14848 2000 3749 2917 2030 2682 3006 12678 2000 3667 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000012\n",
            "INFO:tensorflow:example_index: 12\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what theatre was the best example of \" polish monumental theatre \" ? [SEP] nearby , in og ##rod sas ##ki ( the saxon garden ) , the summer theatre was in operation from 1870 to 1939 , and in the inter - war period , the theatre complex also included mom ##us , warsaw ' s first literary cabaret , and leon sc ##hill ##er ' s musical theatre mel ##od ##ram . the wo ##j ##cie ##ch bog ##us ##ław ##ski theatre ( 1922 – 26 ) , was the best example of \" polish monumental theatre \" . from the mid - 1930s , the great theatre building housed the up ##ati institute of dramatic arts – the first state - run academy of dramatic art , with an acting department and a stage directing department . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 15:0 16:0 17:1 18:2 19:2 20:3 21:3 22:4 23:4 24:5 25:6 26:6 27:6 28:7 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:16 39:17 40:18 41:19 42:20 43:20 44:20 45:21 46:21 47:22 48:23 49:24 50:25 51:26 52:27 53:27 54:27 55:28 56:28 57:28 58:29 59:30 60:31 61:31 62:32 63:33 64:34 65:34 66:34 67:34 68:34 69:35 70:36 71:37 72:37 73:37 74:37 75:38 76:39 77:39 78:39 79:39 80:40 81:40 82:40 83:40 84:41 85:42 86:42 87:42 88:42 89:42 90:42 91:43 92:44 93:45 94:46 95:47 96:48 97:48 98:49 99:50 100:50 101:50 102:51 103:52 104:53 105:53 106:53 107:53 108:54 109:55 110:56 111:57 112:58 113:59 114:60 115:60 116:61 117:62 118:63 119:64 120:65 121:66 122:67 123:68 124:68 125:68 126:69 127:70 128:71 129:72 130:72 131:73 132:74 133:75 134:76 135:77 136:78 137:79 138:80 139:81 140:81\n",
            "INFO:tensorflow:token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True\n",
            "INFO:tensorflow:input_ids: 101 2054 3004 2001 1996 2190 2742 1997 1000 3907 15447 3004 1000 1029 102 3518 1010 1999 13958 14127 21871 3211 1006 1996 10038 3871 1007 1010 1996 2621 3004 2001 1999 3169 2013 6940 2000 3912 1010 1998 1999 1996 6970 1011 2162 2558 1010 1996 3004 3375 2036 2443 3566 2271 1010 8199 1005 1055 2034 4706 19685 1010 1998 6506 8040 7100 2121 1005 1055 3315 3004 11463 7716 6444 1012 1996 24185 3501 23402 2818 22132 2271 19704 5488 3004 1006 4798 1516 2656 1007 1010 2001 1996 2190 2742 1997 1000 3907 15447 3004 1000 1012 2013 1996 3054 1011 5687 1010 1996 2307 3004 2311 7431 1996 2039 10450 2820 1997 6918 2840 1516 1996 2034 2110 1011 2448 2914 1997 6918 2396 1010 2007 2019 3772 2533 1998 1037 2754 9855 2533 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000013\n",
            "INFO:tensorflow:example_index: 13\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] who refined robert mayo ##w ' s work ? [SEP] in the late 17th century , robert boyle proved that air is necessary for combustion . english chemist john mayo ##w ( 1641 – 1679 ) refined this work by showing that fire requires only a part of air that he called spirit ##us ni ##tro ##aer ##eus or just ni ##tro ##aer ##eus . in one experiment he found that placing either a mouse or a lit candle in a closed container over water caused the water to rise and replace one - fourteenth of the air ' s volume before ex ##ting ##uis ##hing the subjects . from this he sur ##mise ##d that ni ##tro ##aer ##eus is consumed in both res ##piration and combustion . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:13 27:14 28:15 29:16 30:17 31:17 32:18 33:18 34:18 35:18 36:18 37:19 38:20 39:21 40:22 41:23 42:24 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:33 52:34 53:35 54:35 55:36 56:36 57:36 58:36 59:37 60:38 61:39 62:39 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:44 71:45 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:54 81:55 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:67 94:67 95:67 96:68 97:69 98:70 99:70 100:70 101:71 102:72 103:73 104:73 105:73 106:73 107:74 108:75 109:75 110:76 111:77 112:78 113:79 114:79 115:79 116:80 117:81 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:86 127:87 128:88 129:88\n",
            "INFO:tensorflow:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True\n",
            "INFO:tensorflow:input_ids: 101 2040 15514 2728 14415 2860 1005 1055 2147 1029 102 1999 1996 2397 5550 2301 1010 2728 16694 4928 2008 2250 2003 4072 2005 16513 1012 2394 15535 2198 14415 2860 1006 25702 1516 27924 1007 15514 2023 2147 2011 4760 2008 2543 5942 2069 1037 2112 1997 2250 2008 2002 2170 4382 2271 9152 13181 27867 10600 2030 2074 9152 13181 27867 10600 1012 1999 2028 7551 2002 2179 2008 6885 2593 1037 8000 2030 1037 5507 13541 1999 1037 2701 11661 2058 2300 3303 1996 2300 2000 4125 1998 5672 2028 1011 15276 1997 1996 2250 1005 1055 3872 2077 4654 3436 27020 12053 1996 5739 1012 2013 2023 2002 7505 28732 2094 2008 9152 13181 27867 10600 2003 10202 1999 2119 24501 16781 1998 16513 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000014\n",
            "INFO:tensorflow:example_index: 14\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] extended networking benefits helped those that could not connect to what platform ? [SEP] the computer science network ( cs ##net ) was a computer network funded by the u . s . national science foundation ( ns ##f ) that began operation in 1981 . its purpose was to extend networking benefits , for computer science departments at academic and research institutions that could not be directly connected to ar ##pan ##et , due to funding or authorization limitations . it played a significant role in spreading awareness of , and access to , national networking and was a major milestone on the path to development of the global internet . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:4 21:4 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:12 32:12 33:12 34:13 35:14 36:15 37:16 38:16 39:16 40:16 41:17 42:18 43:19 44:20 45:21 46:21 47:22 48:23 49:24 50:25 51:26 52:27 53:28 54:28 55:29 56:30 57:31 58:32 59:33 60:34 61:35 62:36 63:37 64:38 65:39 66:40 67:41 68:42 69:43 70:44 71:45 72:45 73:45 74:45 75:46 76:47 77:48 78:49 79:50 80:51 81:51 82:52 83:53 84:54 85:55 86:56 87:57 88:58 89:59 90:60 91:60 92:61 93:62 94:63 95:63 96:64 97:65 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:74 107:75 108:76 109:77 110:78 111:79 112:79\n",
            "INFO:tensorflow:token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True\n",
            "INFO:tensorflow:input_ids: 101 3668 14048 6666 3271 2216 2008 2071 2025 7532 2000 2054 4132 1029 102 1996 3274 2671 2897 1006 20116 7159 1007 2001 1037 3274 2897 6787 2011 1996 1057 1012 1055 1012 2120 2671 3192 1006 24978 2546 1007 2008 2211 3169 1999 3261 1012 2049 3800 2001 2000 7949 14048 6666 1010 2005 3274 2671 7640 2012 3834 1998 2470 4896 2008 2071 2025 2022 3495 4198 2000 12098 9739 3388 1010 2349 2000 4804 2030 20104 12546 1012 2009 2209 1037 3278 2535 1999 9359 7073 1997 1010 1998 3229 2000 1010 2120 14048 1998 2001 1037 2350 19199 2006 1996 4130 2000 2458 1997 1996 3795 4274 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000015\n",
            "INFO:tensorflow:example_index: 15\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] where is the ho ##lo ##cene further expanded since it is very short with short epoch ##s ? [SEP] the following four timeline ##s show the geologic time scale . the first shows the entire time from the formation of the earth to the present , but this com ##press ##es the most recent e ##on . therefore , the second scale shows the most recent e ##on with an expanded scale . the second scale com ##press ##es the most recent era , so the most recent era is expanded in the third scale . since the qu ##ater ##nary is a very short period with short epoch ##s , it is further expanded in the fourth scale . the second , third , and fourth timeline ##s are therefore each sub ##section ##s of their preceding timeline as indicated by as ##ter ##isk ##s . the ho ##lo ##cene ( the latest epoch ) is too small to be shown clearly on the third timeline on the right , another reason for expanding the fourth scale . the pleistocene ( p ) epoch . q stands for the qu ##ater ##nary period . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 20:0 21:1 22:2 23:3 24:3 25:4 26:5 27:6 28:7 29:8 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:23 46:23 47:24 48:25 49:26 50:26 51:26 52:27 53:28 54:29 55:30 56:30 57:30 58:31 59:31 60:32 61:33 62:34 63:35 64:36 65:37 66:38 67:39 68:39 69:40 70:41 71:42 72:43 73:43 74:44 75:45 76:46 77:47 78:47 79:47 80:48 81:49 82:50 83:51 84:51 85:52 86:53 87:54 88:55 89:56 90:57 91:58 92:59 93:60 94:61 95:62 96:62 97:63 98:64 99:65 100:65 101:65 102:66 103:67 104:68 105:69 106:70 107:71 108:72 109:73 110:73 111:73 112:74 113:75 114:76 115:77 116:78 117:79 118:80 119:81 120:81 121:82 122:83 123:83 124:84 125:84 126:85 127:86 128:87 129:87 130:88 131:89 132:90 133:91 134:91 135:91 136:92 137:93 138:94 139:95 140:96 141:97 142:98 143:99 144:99 145:99 146:99 147:99 148:100 149:101 150:101 151:101 152:102 153:102 154:103 155:104 156:104 157:105 158:106 159:107 160:108 161:109 162:110 163:111 164:112 165:113 166:114 167:115 168:116 169:117 170:118 171:118 172:119 173:120 174:121 175:122 176:123 177:124 178:125 179:125 180:126 181:127 182:128 183:128 184:128 185:129 186:129 187:130 188:131 189:132 190:133 191:134 192:134 193:134 194:135 195:135\n",
            "INFO:tensorflow:token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True\n",
            "INFO:tensorflow:input_ids: 101 2073 2003 1996 7570 4135 17968 2582 4423 2144 2009 2003 2200 2460 2007 2460 25492 2015 1029 102 1996 2206 2176 17060 2015 2265 1996 22125 2051 4094 1012 1996 2034 3065 1996 2972 2051 2013 1996 4195 1997 1996 3011 2000 1996 2556 1010 2021 2023 4012 20110 2229 1996 2087 3522 1041 2239 1012 3568 1010 1996 2117 4094 3065 1996 2087 3522 1041 2239 2007 2019 4423 4094 1012 1996 2117 4094 4012 20110 2229 1996 2087 3522 3690 1010 2061 1996 2087 3522 3690 2003 4423 1999 1996 2353 4094 1012 2144 1996 24209 24932 24041 2003 1037 2200 2460 2558 2007 2460 25492 2015 1010 2009 2003 2582 4423 1999 1996 2959 4094 1012 1996 2117 1010 2353 1010 1998 2959 17060 2015 2024 3568 2169 4942 29015 2015 1997 2037 11003 17060 2004 5393 2011 2004 3334 20573 2015 1012 1996 7570 4135 17968 1006 1996 6745 25492 1007 2003 2205 2235 2000 2022 3491 4415 2006 1996 2353 17060 2006 1996 2157 1010 2178 3114 2005 9186 1996 2959 4094 1012 1996 25080 1006 1052 1007 25492 1012 1053 4832 2005 1996 24209 24932 24041 2558 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000016\n",
            "INFO:tensorflow:example_index: 16\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] how much area does the university library garden cover ? [SEP] another important library – the university library , founded in 1816 , is home to over two million items . the building was designed by architects marek bud ##zyn ##ski and z ##bi ##gni ##ew bad ##owski and opened on 15 december 1999 . it is surrounded by green . the university library garden , designed by ir ##ena ba ##jer ##ska , was opened on 12 june 2002 . it is one of the largest and most beautiful roof gardens in europe with an area of more than 10 , 000 m2 ( 107 , 63 ##9 . 10 sq ft ) , and plants covering 5 , 111 m2 ( 55 , 01 ##4 . 35 sq ft ) . as the university garden it is open to the public every day . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:6 20:7 21:8 22:9 23:9 24:10 25:11 26:12 27:13 28:14 29:15 30:16 31:16 32:17 33:18 34:19 35:20 36:21 37:22 38:23 39:24 40:24 41:24 42:25 43:26 44:26 45:26 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:33 55:33 56:34 57:35 58:36 59:37 60:38 61:38 62:39 63:40 64:41 65:42 66:42 67:43 68:44 69:45 70:45 71:46 72:46 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:59 89:60 90:61 91:62 92:63 93:64 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:72 103:72 104:73 105:74 106:74 107:74 108:74 109:74 110:74 111:74 112:75 113:76 114:76 115:76 116:77 117:78 118:79 119:80 120:80 121:80 122:81 123:82 124:82 125:82 126:82 127:82 128:82 129:82 130:83 131:84 132:84 133:84 134:85 135:86 136:87 137:88 138:89 139:90 140:91 141:92 142:93 143:94 144:95 145:96 146:96\n",
            "INFO:tensorflow:token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True\n",
            "INFO:tensorflow:input_ids: 101 2129 2172 2181 2515 1996 2118 3075 3871 3104 1029 102 2178 2590 3075 1516 1996 2118 3075 1010 2631 1999 12357 1010 2003 2188 2000 2058 2048 2454 5167 1012 1996 2311 2001 2881 2011 8160 29318 13007 23749 5488 1998 1062 5638 29076 7974 2919 15249 1998 2441 2006 2321 2285 2639 1012 2009 2003 5129 2011 2665 1012 1996 2118 3075 3871 1010 2881 2011 20868 8189 8670 20009 8337 1010 2001 2441 2006 2260 2238 2526 1012 2009 2003 2028 1997 1996 2922 1998 2087 3376 4412 5822 1999 2885 2007 2019 2181 1997 2062 2084 2184 1010 2199 25525 1006 10550 1010 6191 2683 1012 2184 5490 3027 1007 1010 1998 4264 5266 1019 1010 11118 25525 1006 4583 1010 5890 2549 1012 3486 5490 3027 1007 1012 2004 1996 2118 3871 2009 2003 2330 2000 1996 2270 2296 2154 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000017\n",
            "INFO:tensorflow:example_index: 17\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] as a result of a compromise , about how many chinese schools became national type schools ? [SEP] after malaysia ' s independence in 1957 , the government instructed all schools to surrender their properties and be ass ##imi ##lated into the national school system . this caused an up ##ro ##ar among the chinese and a compromise was achieved in that the schools would instead become \" national type \" schools . under such a system , the government is only in charge of the school curriculum and teaching personnel while the lands still belonged to the schools . while chinese primary schools were allowed to retain chinese as the medium of instruction , chinese secondary schools are required to change into english - medium schools . over 60 schools converted to become national type schools . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 19:0 20:1 21:1 22:1 23:2 24:3 25:4 26:4 27:5 28:6 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:16 40:16 41:17 42:18 43:19 44:20 45:21 46:21 47:22 48:23 49:24 50:25 51:25 52:25 53:26 54:27 55:28 56:29 57:30 58:31 59:32 60:33 61:34 62:35 63:36 64:37 65:38 66:39 67:40 68:41 69:41 70:42 71:42 72:43 73:43 74:44 75:45 76:46 77:47 78:47 79:48 80:49 81:50 82:51 83:52 84:53 85:54 86:55 87:56 88:57 89:58 90:59 91:60 92:61 93:62 94:63 95:64 96:65 97:66 98:67 99:68 100:68 101:69 102:70 103:71 104:72 105:73 106:74 107:75 108:76 109:77 110:78 111:79 112:80 113:81 114:82 115:82 116:83 117:84 118:85 119:86 120:87 121:88 122:89 123:90 124:91 125:91 126:91 127:92 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:101 138:101\n",
            "INFO:tensorflow:token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True\n",
            "INFO:tensorflow:input_ids: 101 2004 1037 2765 1997 1037 12014 1010 2055 2129 2116 2822 2816 2150 2120 2828 2816 1029 102 2044 6027 1005 1055 4336 1999 3890 1010 1996 2231 10290 2035 2816 2000 7806 2037 5144 1998 2022 4632 27605 13776 2046 1996 2120 2082 2291 1012 2023 3303 2019 2039 3217 2906 2426 1996 2822 1998 1037 12014 2001 4719 1999 2008 1996 2816 2052 2612 2468 1000 2120 2828 1000 2816 1012 2104 2107 1037 2291 1010 1996 2231 2003 2069 1999 3715 1997 1996 2082 8882 1998 4252 5073 2096 1996 4915 2145 6272 2000 1996 2816 1012 2096 2822 3078 2816 2020 3039 2000 9279 2822 2004 1996 5396 1997 7899 1010 2822 3905 2816 2024 3223 2000 2689 2046 2394 1011 5396 2816 1012 2058 3438 2816 4991 2000 2468 2120 2828 2816 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000018\n",
            "INFO:tensorflow:example_index: 18\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what magnetic character do triple ##t o ##2 have ? [SEP] in the triple ##t form , o 2 molecules are para ##ma ##gne ##tic . that is , they imp ##art magnetic character to oxygen when it is in the presence of a magnetic field , because of the spin magnetic moments of the un ##pa ##ired electrons in the molecule , and the negative exchange energy between neighboring o 2 molecules . liquid oxygen is attracted to a magnet to a sufficient extent that , in laboratory demonstrations , a bridge of liquid oxygen may be supported against its own weight between the poles of a powerful magnet . [ c ] [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:0 13:1 14:2 15:2 16:3 17:3 18:4 19:5 20:6 21:7 22:8 23:8 24:8 25:8 26:8 27:9 28:10 29:10 30:11 31:12 32:12 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:26 48:27 49:28 50:29 51:30 52:31 53:32 54:33 55:34 56:35 57:35 58:35 59:36 60:37 61:38 62:39 63:39 64:40 65:41 66:42 67:43 68:44 69:45 70:46 71:47 72:48 73:49 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:61 87:61 88:62 89:63 90:64 91:64 92:65 93:66 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:75 103:76 104:77 105:78 106:79 107:80 108:81 109:82 110:83 111:83 112:83 113:83 114:83\n",
            "INFO:tensorflow:token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True\n",
            "INFO:tensorflow:input_ids: 101 2054 8060 2839 2079 6420 2102 1051 2475 2031 1029 102 1999 1996 6420 2102 2433 1010 1051 1016 10737 2024 11498 2863 10177 4588 1012 2008 2003 1010 2027 17727 8445 8060 2839 2000 7722 2043 2009 2003 1999 1996 3739 1997 1037 8060 2492 1010 2138 1997 1996 6714 8060 5312 1997 1996 4895 4502 27559 15057 1999 1996 13922 1010 1998 1996 4997 3863 2943 2090 8581 1051 1016 10737 1012 6381 7722 2003 6296 2000 1037 16853 2000 1037 7182 6698 2008 1010 1999 5911 13616 1010 1037 2958 1997 6381 7722 2089 2022 3569 2114 2049 2219 3635 2090 1996 10567 1997 1037 3928 16853 1012 1031 1039 1033 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000019\n",
            "INFO:tensorflow:example_index: 19\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what machine ' s branching does not exactly capture many of the mathematical models we want to analyze ? [SEP] however , some computational problems are easier to analyze in terms of more unusual resources . for example , a non - deter ##mini ##stic turing machine is a computational model that is allowed to branch out to check many different possibilities at once . the non - deter ##mini ##stic turing machine has very little to do with how we physically want to compute algorithms , but its branching exactly captures many of the mathematical models we want to analyze , so that non - deter ##mini ##stic time is a very important resource in analyzing computational problems . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 21:0 22:0 23:1 24:2 25:3 26:4 27:5 28:6 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:13 37:14 38:15 39:15 40:16 41:17 42:17 43:17 44:17 45:17 46:18 47:19 48:20 49:21 50:22 51:23 52:24 53:25 54:26 55:27 56:28 57:29 58:30 59:31 60:32 61:33 62:34 63:35 64:36 65:36 66:37 67:38 68:38 69:38 70:38 71:38 72:39 73:40 74:41 75:42 76:43 77:44 78:45 79:46 80:47 81:48 82:49 83:50 84:51 85:52 86:53 87:53 88:54 89:55 90:56 91:57 92:58 93:59 94:60 95:61 96:62 97:63 98:64 99:65 100:66 101:67 102:67 103:68 104:69 105:70 106:70 107:70 108:70 109:70 110:71 111:72 112:73 113:74 114:75 115:76 116:77 117:78 118:79 119:80 120:80\n",
            "INFO:tensorflow:token_is_max_context: 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True\n",
            "INFO:tensorflow:input_ids: 101 2054 3698 1005 1055 23346 2515 2025 3599 5425 2116 1997 1996 8045 4275 2057 2215 2000 17908 1029 102 2174 1010 2070 15078 3471 2024 6082 2000 17908 1999 3408 1997 2062 5866 4219 1012 2005 2742 1010 1037 2512 1011 28283 25300 10074 28639 3698 2003 1037 15078 2944 2008 2003 3039 2000 3589 2041 2000 4638 2116 2367 12020 2012 2320 1012 1996 2512 1011 28283 25300 10074 28639 3698 2038 2200 2210 2000 2079 2007 2129 2057 8186 2215 2000 24134 13792 1010 2021 2049 23346 3599 19566 2116 1997 1996 8045 4275 2057 2215 2000 17908 1010 2061 2008 2512 1011 28283 25300 10074 2051 2003 1037 2200 2590 7692 1999 20253 15078 3471 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "od4syU8Mv8PM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_input_fn = run_squad.input_fn_builder(\n",
        "        input_file=eval_writer.filename,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-YAwHWiwR4l",
        "colab_type": "code",
        "outputId": "828298e1-f660-4816-e636-847104bc08aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3672
        }
      },
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "for result in estimator.predict(\n",
        "  predict_input_fn, yield_single_examples=True):\n",
        "  if len(all_results) % 1000 == 0:\n",
        "    tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
        "  unique_id = int(result[\"unique_ids\"])\n",
        "  start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "  end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "  all_results.append(\n",
        "      run_squad.RawResult(\n",
        "          unique_id=unique_id,\n",
        "          start_logits=start_logits,\n",
        "          end_logits=end_logits))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 384)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 384)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 384)\n",
            "INFO:tensorflow:  name = unique_ids, shape = (?,)\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = cls/squad/output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = cls/squad/output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from output_model/model.ckpt-1500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Processing example: 0\n",
            "INFO:tensorflow:Processing example: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wui12loS9YWR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
        "                      max_answer_length, do_lower_case):\n",
        "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
        "  #tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "  #tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "  example_index_to_features = collections.defaultdict(list)\n",
        "  for feature in all_features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "  unique_id_to_result = {}\n",
        "  for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "      \"PrelimPrediction\",\n",
        "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "  all_predictions = collections.OrderedDict()\n",
        "  all_nbest_json = collections.OrderedDict()\n",
        "  scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "  for (example_index, example) in enumerate(all_examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "    # keep track of the minimum score of null start+end of position 0\n",
        "    score_null = 1000000  # large and positive\n",
        "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
        "    null_start_logit = 0  # the start logit at the slice with min null score\n",
        "    null_end_logit = 0  # the end logit at the slice with min null score\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "      result = unique_id_to_result[feature.unique_id]\n",
        "      start_indexes = run_squad._get_best_indexes(result.start_logits, n_best_size)\n",
        "      end_indexes = run_squad._get_best_indexes(result.end_logits, n_best_size)\n",
        "      # if we could have irrelevant answers, get the min score of irrelevant\n",
        "      if version_2_with_negative:\n",
        "        feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
        "        if feature_null_score < score_null:\n",
        "          score_null = feature_null_score\n",
        "          min_null_feature_index = feature_index\n",
        "          null_start_logit = result.start_logits[0]\n",
        "          null_end_logit = result.end_logits[0]\n",
        "      for start_index in start_indexes:\n",
        "        for end_index in end_indexes:\n",
        "          # We could hypothetically create invalid predictions, e.g., predict\n",
        "          # that the start of the span is in the question. We throw out all\n",
        "          # invalid predictions.\n",
        "          if start_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if end_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if start_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if end_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if not feature.token_is_max_context.get(start_index, False):\n",
        "            continue\n",
        "          if end_index < start_index:\n",
        "            continue\n",
        "          length = end_index - start_index + 1\n",
        "          if length > max_answer_length:\n",
        "            continue\n",
        "          prelim_predictions.append(\n",
        "              _PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_logit=result.start_logits[start_index],\n",
        "                  end_logit=result.end_logits[end_index]))\n",
        "\n",
        "    if version_2_with_negative:\n",
        "      prelim_predictions.append(\n",
        "          _PrelimPrediction(\n",
        "              feature_index=min_null_feature_index,\n",
        "              start_index=0,\n",
        "              end_index=0,\n",
        "              start_logit=null_start_logit,\n",
        "              end_logit=null_end_logit))\n",
        "    prelim_predictions = sorted(\n",
        "        prelim_predictions,\n",
        "        key=lambda x: (x.start_logit + x.end_logit),\n",
        "        reverse=True)\n",
        "\n",
        "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "      if len(nbest) >= n_best_size:\n",
        "        break\n",
        "      feature = features[pred.feature_index]\n",
        "      if pred.start_index > 0:  # this is a non-null prediction\n",
        "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "        tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "        # De-tokenize WordPieces that have been split off.\n",
        "        tok_text = tok_text.replace(\" ##\", \"\")\n",
        "        tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "        # Clean whitespace\n",
        "        tok_text = tok_text.strip()\n",
        "        tok_text = \" \".join(tok_text.split())\n",
        "        orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
        "        if final_text in seen_predictions:\n",
        "          continue\n",
        "\n",
        "        seen_predictions[final_text] = True\n",
        "      else:\n",
        "        final_text = \"\"\n",
        "        seen_predictions[final_text] = True\n",
        "\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=final_text,\n",
        "              start_logit=pred.start_logit,\n",
        "              end_logit=pred.end_logit))\n",
        "\n",
        "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
        "    if version_2_with_negative:\n",
        "      if \"\" not in seen_predictions:\n",
        "        nbest.append(\n",
        "            _NbestPrediction(\n",
        "                text=\"\", start_logit=null_start_logit,\n",
        "                end_logit=null_end_logit))\n",
        "    # In very rare edge cases we could have no valid predictions. So we\n",
        "    # just create a nonce prediction in this case to avoid failure.\n",
        "    if not nbest:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "    assert len(nbest) >= 1\n",
        "\n",
        "    total_scores = []\n",
        "    best_non_null_entry = None\n",
        "    for entry in nbest:\n",
        "      total_scores.append(entry.start_logit + entry.end_logit)\n",
        "      if not best_non_null_entry:\n",
        "        if entry.text:\n",
        "          best_non_null_entry = entry\n",
        "\n",
        "    probs = run_squad._compute_softmax(total_scores)\n",
        "\n",
        "    nbest_json = []\n",
        "    for (i, entry) in enumerate(nbest):\n",
        "      output = collections.OrderedDict()\n",
        "      output[\"text\"] = entry.text\n",
        "      output[\"probability\"] = probs[i]\n",
        "      output[\"start_logit\"] = entry.start_logit\n",
        "      output[\"end_logit\"] = entry.end_logit\n",
        "      nbest_json.append(output)\n",
        "\n",
        "    assert len(nbest_json) >= 1\n",
        "\n",
        "    if not version_2_with_negative:\n",
        "      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
        "    else:\n",
        "      # predict \"\" iff the null score - the score of best non-null > threshold\n",
        "      score_diff = score_null - best_non_null_entry.start_logit - (\n",
        "          best_non_null_entry.end_logit)\n",
        "      scores_diff_json[example.qas_id] = score_diff\n",
        "      if score_diff > null_score_diff_threshold:\n",
        "        all_predictions[example.qas_id] = \"\"\n",
        "      else:\n",
        "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "\n",
        "    all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "  return all_predictions,all_nbest_json\n",
        " \n",
        "def get_final_text(pred_text, orig_text, do_lower_case):\n",
        "  \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "  # When we created the data, we kept track of the alignment between original\n",
        "  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
        "  # now `orig_text` contains the span of our original text corresponding to the\n",
        "  # span that we predicted.\n",
        "  #\n",
        "  # However, `orig_text` may contain extra characters that we don't want in\n",
        "  # our prediction.\n",
        "  #\n",
        "  # For example, let's say:\n",
        "  #   pred_text = steve smith\n",
        "  #   orig_text = Steve Smith's\n",
        "  #\n",
        "  # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
        "  #\n",
        "  # We don't want to return `pred_text` because it's already been normalized\n",
        "  # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
        "  # our tokenizer does additional normalization like stripping accent\n",
        "  # characters).\n",
        "  #\n",
        "  # What we really want to return is \"Steve Smith\".\n",
        "  #\n",
        "  # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
        "  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
        "  # can fail in certain cases in which case we just return `orig_text`.\n",
        "\n",
        "  def _strip_spaces(text):\n",
        "    ns_chars = []\n",
        "    ns_to_s_map = collections.OrderedDict()\n",
        "    for (i, c) in enumerate(text):\n",
        "      if c == \" \":\n",
        "        continue\n",
        "      ns_to_s_map[len(ns_chars)] = i\n",
        "      ns_chars.append(c)\n",
        "    ns_text = \"\".join(ns_chars)\n",
        "    return (ns_text, ns_to_s_map)\n",
        "\n",
        "  # We first tokenize `orig_text`, strip whitespace from the result\n",
        "  # and `pred_text`, and check if they are the same length. If they are\n",
        "  # NOT the same length, the heuristic has failed. If they are the same\n",
        "  # length, we assume the characters are one-to-one aligned.\n",
        "  tokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "  tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "  start_position = tok_text.find(pred_text)\n",
        "  if start_position == -1:\n",
        "    if verbose_logging:\n",
        "      tf.logging.info(\n",
        "          \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "    return orig_text\n",
        "  end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "  if len(orig_ns_text) != len(tok_ns_text):\n",
        "    if verbose_logging:\n",
        "      tf.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
        "                      orig_ns_text, tok_ns_text)\n",
        "    return orig_text\n",
        "\n",
        "  # We then project the characters in `pred_text` back to `orig_text` using\n",
        "  # the character-to-character alignment.\n",
        "  tok_s_to_ns_map = {}\n",
        "  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
        "    tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "  orig_start_position = None\n",
        "  if start_position in tok_s_to_ns_map:\n",
        "    ns_start_position = tok_s_to_ns_map[start_position]\n",
        "    if ns_start_position in orig_ns_to_s_map:\n",
        "      orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "  if orig_start_position is None:\n",
        "    if verbose_logging:\n",
        "      tf.logging.info(\"Couldn't map start position\")\n",
        "    return orig_text\n",
        "\n",
        "  orig_end_position = None\n",
        "  if end_position in tok_s_to_ns_map:\n",
        "    ns_end_position = tok_s_to_ns_map[end_position]\n",
        "    if ns_end_position in orig_ns_to_s_map:\n",
        "      orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "  if orig_end_position is None:\n",
        "    if verbose_logging:\n",
        "      tf.logging.info(\"Couldn't map end position\")\n",
        "    return orig_text\n",
        "\n",
        "  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "  return output_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6zsuzgPUCDvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "n_best_size=20\n",
        "do_lower_case=True\n",
        "verbose_logging=False\n",
        "version_2_with_negative=True\n",
        "null_score_diff_threshold=0.0\n",
        "all_predictions,all_nbest_json = write_predictions(eval_examples, eval_features, all_results,\n",
        "                  n_best_size, max_answer_length,\n",
        "                  do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "az8d8RMeDMyF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(test_file) as f:\n",
        "    eval_answers = json.load(f)\n",
        "#eval_answers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uM9JZ32C0ZwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "master_DF=pd.DataFrame()\n",
        "for data in eval_answers['data']:\n",
        "  paragraphs = data['paragraphs']\n",
        "  for paragraph in paragraphs:\n",
        "    qas = paragraph['qas']\n",
        "    for qa in qas:\n",
        "      this_id = qa['id']\n",
        "      answers = qa['answers']\n",
        "      for answer in answers:\n",
        "        this_text = answer['text']\n",
        "        this_df = pd.DataFrame(data = {'text':this_text,'id':this_id},index=[0])\n",
        "        # Add to master DF\n",
        "        master_DF = master_DF.append(this_df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-G6CHni1_U0",
        "colab_type": "code",
        "outputId": "1b5b39a9-b47c-4692-cb19-f5e6929abc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3403
        }
      },
      "cell_type": "code",
      "source": [
        "actual_DF = master_DF.drop_duplicates()\n",
        "predict_text = [x[1] for x in list(all_predictions.items())]\n",
        "ids = list(all_predictions)\n",
        "counting_vector = 0\n",
        "ids_correct = []\n",
        "import re\n",
        "\n",
        "for i in range(len(all_predictions)):\n",
        "  #print(i)\n",
        "  actual_text = (actual_DF[actual_DF['id']==ids[i]]['text'])\n",
        "  this_predict_text = str(predict_text[i])\n",
        "  for this_text in actual_text:\n",
        "    #print(this_text)\n",
        "    #print(this_predict_text)\n",
        "    \n",
        "    # Do some cleaning up\n",
        "    this_text = re.sub('[^a-zA-Z0-9 ]', '', this_text)\n",
        "    this_predict_text = re.sub('[^a-zA-Z0-9 ]', '', this_predict_text)\n",
        "\n",
        "    if this_text==this_predict_text:\n",
        "      counting_vector=counting_vector+1\n",
        "      ids_correct.append(ids[i])\n",
        "    elif re.search(this_text, this_predict_text):\n",
        "      counting_vector=counting_vector+1\n",
        "      ids_correct.append(ids[i])\n",
        "    elif re.search(this_predict_text, this_text):\n",
        "      counting_vector=counting_vector+1\n",
        "      ids_correct.append(ids[i])\n",
        "    else:\n",
        "      print(this_text + \" \" + str(1))\n",
        "      print(this_predict_text + \" \" + str(2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sankt Goarshausen 1\n",
            "Lorelei 2\n",
            "Near Sankt Goarshausen 1\n",
            "Lorelei 2\n",
            "10000 m2 1\n",
            "5111 m2 5501435 sq ft 2\n",
            "more than 10000 m2 1\n",
            "5111 m2 5501435 sq ft 2\n",
            "all 1\n",
            "60 2\n",
            "negative effect 1\n",
            "High and persistent unemployment in which inequality increases has a negative 2\n",
            "better 1\n",
            "much higher school fees 2\n",
            "produce better academic results 1\n",
            "much higher school fees 2\n",
            "among states in the US with larger income inequalities 1\n",
            "countries with bigger income inequalities 2\n",
            "using sickles to deflate one of the large domes covering two satellite dishes 1\n",
            "GCSB Waihopai 2\n",
            "using sickles to deflate one of the large domes 1\n",
            "GCSB Waihopai 2\n",
            "algebraic 1\n",
            "identity 2\n",
            "modern algebraic number theory 1\n",
            "identity 2\n",
            "infrequent rain 1\n",
            "Mediterranean 2\n",
            "easier credit 1\n",
            "unsustainable monetary stimulation 2\n",
            "to distract Montcalm 1\n",
            "a massacre 2\n",
            "distract Montcalm 1\n",
            "a massacre 2\n",
            "late 19th 1\n",
            "19th century 2\n",
            "A or  1\n",
            "n  p  2n  2 2\n",
            "deterministic 1\n",
            "general numbers n 2\n",
            "deterministic algorithms 1\n",
            "general numbers n 2\n",
            "Complexity measures 1\n",
            "circuit complexity and decision tree complexity 2\n",
            "complexity measures 1\n",
            "circuit complexity and decision tree complexity 2\n",
            "tropical 1\n",
            "moist broadleaf 2\n",
            "oxides of silicon 1\n",
            "oxygen 2\n",
            "silicon 1\n",
            "oxygen 2\n",
            "1996 1\n",
            "2000 2\n",
            "Old Rhine 1\n",
            "sluice 2\n",
            "Oude Rijn 1\n",
            "sluice 2\n",
            "Alpha Phi Omega 1\n",
            "Four 2\n",
            "mild euphoric 1\n",
            "placebo effect 2\n",
            "a supposed mild euphoric 1\n",
            "placebo effect 2\n",
            "euphoric 1\n",
            "placebo effect 2\n",
            "increased 1\n",
            "the ground water level fell 2\n",
            "rate of flow was increased 1\n",
            "the ground water level fell 2\n",
            "74000 BP 1\n",
            "Pleistocene 2\n",
            "74000 BP  Before Present 1\n",
            "Pleistocene 2\n",
            "a mans presence 1\n",
            "establish if a woman in receipt of benefits as a single mother is wrongly claiming to be living alone 2\n",
            "betray a mans presence in the household 1\n",
            "establish if a woman in receipt of benefits as a single mother is wrongly claiming to be living alone 2\n",
            "carbon related emissions 1\n",
            "tree growth stages 2\n",
            "carbon related 1\n",
            "tree growth stages 2\n",
            "northern 1\n",
            "Tehachapi Mountains 2\n",
            "The citys residents fled to the north 1\n",
            "north most of them dying during the journey but the infection had been spread to the people of Asia Minor 2\n",
            "the north 1\n",
            "north most of them dying during the journey but the infection had been spread to the people of Asia Minor 2\n",
            "the opening of hostilities 1\n",
            "Albany Congress 2\n",
            "Mainau 1\n",
            "Lindau 2\n",
            "worstcase 1\n",
            "On2 2\n",
            "worst 1\n",
            "On2 2\n",
            "1999 1\n",
            "Imperial 2\n",
            "05 1\n",
            "111 2\n",
            "05 1\n",
            "111 2\n",
            "61 1\n",
            "111 2\n",
            "elected 1\n",
            "either house of parliament 2\n",
            "water 1\n",
            "sunlight 2\n",
            "melts 1\n",
            "such that the temperature of the firebox crown increases significantly 2\n",
            "the lead melts 1\n",
            "such that the temperature of the firebox crown increases significantly 2\n",
            "a finite value 1\n",
            "1 2\n",
            "finite 1\n",
            "1 2\n",
            "northwest across Europe 1\n",
            "Italy 2\n",
            "northwest 1\n",
            "Italy 2\n",
            "Afghanistan 1\n",
            "USSR 2\n",
            "the Conservatives 1\n",
            "David McLetchie 2\n",
            "Conservatives 1\n",
            "David McLetchie 2\n",
            "President Agnew 1\n",
            "Marshall Cohen 2\n",
            "military roads to the area by Braddock and Forbes 1\n",
            "legal and illegal settlement 2\n",
            "construction of military roads to the area by Braddock and Forbes 1\n",
            "legal and illegal settlement 2\n",
            "the construction of military roads to the area 1\n",
            "legal and illegal settlement 2\n",
            "military roads to the area 1\n",
            "legal and illegal settlement 2\n",
            "the construction of military roads 1\n",
            "legal and illegal settlement 2\n",
            "the problem 1\n",
            "a problem 2\n",
            "annexing outlying communities 1\n",
            "funding education sanitation and traffic control 2\n",
            "Leonhard Euler 1\n",
            "Christian Goldbach 2\n",
            "private finance initiatives PFIs 1\n",
            "PublicPrivate Partnering 2\n",
            "15244 1\n",
            "5792 2\n",
            "populationwide satisfaction and happiness 1\n",
            "higher aggregate utility 2\n",
            "satisfaction and happiness 1\n",
            "higher aggregate utility 2\n",
            "OpenTV 1\n",
            "MPEG4 2\n",
            "a pharmacy practice residency 1\n",
            "hospitals 2\n",
            "pharmacy practice residency 1\n",
            "hospitals 2\n",
            "Thailand 1\n",
            "ThaiJapanese 2\n",
            "British Empire 1\n",
            "pseudosciences 2\n",
            "plants and factories 1\n",
            "70 m3s 2500 cu fts 2\n",
            "industry 1\n",
            "70 m3s 2500 cu fts 2\n",
            "spiritual cures 1\n",
            "herbal remedies 2\n",
            "spiritual 1\n",
            "herbal remedies 2\n",
            "between 14 and 58 C above 1990 levels 1\n",
            "90 certain that temperatures will continue to rise with average global surface temperature projected to increase by between 14 and 58 C 2\n",
            "Noord River 1\n",
            "Wijk bij Duurstede 2\n",
            "Nieuwe Maas 1\n",
            "Wijk bij Duurstede 2\n",
            "an equality 1\n",
            "fixed 2\n",
            "unclear 1\n",
            "fixed 2\n",
            "Migration period 1\n",
            "5th century 2\n",
            "V8 and six cylinder engines 1\n",
            "four cylinder engines that were more fuel efficient than the typical American V8 and six cylinder 2\n",
            "2001 1\n",
            "2007 2\n",
            "elite politicians 1\n",
            "a postman or tax collector 2\n",
            "individuals 1\n",
            "a postman or tax collector 2\n",
            "1340s onwards 1\n",
            "late 1340s 2\n",
            "mosaics 1\n",
            "stonework or metalwork 2\n",
            "roughly 260 kilometres 1\n",
            "260 kilometres 160 mi 2\n",
            "beta decay of neutrons in atomic nuclei 1\n",
            "The weak force is due to the exchange of the heavy W and Z bosons Its most familiar effect is beta decay 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SfGhftXM5uDm",
        "colab_type": "code",
        "outputId": "43d677b7-094f-41c4-93c5-2f1b06c47a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "accuracy = counting_vector/len(all_predictions)\n",
        "print('Accuracy is ' + str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is 0.7021943573667712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ALKUaqgX0A75",
        "colab_type": "code",
        "outputId": "956307ef-05a8-4225-960f-00d61f5b3918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2686
        }
      },
      "cell_type": "code",
      "source": [
        "ids_incorrect = [i for i in ids if i not in ids_correct]\n",
        "ids_incorrect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5727f05b4b864d190016406b',\n",
              " '5acff3b377cf76001a6865fc',\n",
              " '57332b66d058e614000b5758',\n",
              " '5730042804bcaa1900d77011',\n",
              " '5ad26dbcd7d075001a4293d5',\n",
              " '571a49de4faf5e1900b8a94c',\n",
              " '5728349dff5b5019007d9f01',\n",
              " '5a6ce4fa4eec6b001a80a68b',\n",
              " '5ad26797d7d075001a429236',\n",
              " '571127a5a58dae1900cd6cc6',\n",
              " '5ad507d25b96ef001a10aa4c',\n",
              " '5ad02e5d77cf76001a686d3c',\n",
              " '5ad26dbcd7d075001a4293d6',\n",
              " '5ad21e1ad7d075001a428471',\n",
              " '5a63772268151a001a9222f4',\n",
              " '5a1c850fb4fb5d001871464e',\n",
              " '5ad24057d7d075001a428930',\n",
              " '57300f8504bcaa1900d770d1',\n",
              " '5a25d89aef59cd001a623cf8',\n",
              " '57309bfb8ab72b1400f9c5e9',\n",
              " '5a605197eae51e001ab14d0f',\n",
              " '5ad3ba81604f3c001a3fee9e',\n",
              " '572faec7b2c2fd1400568333',\n",
              " '572fffb404bcaa1900d76ff0',\n",
              " '573735e8c3c5551400e51e72',\n",
              " '5a2c07d5bfd06b001a5ae983',\n",
              " '57268882f1498d1400e8e307',\n",
              " '5a8940ba3b2508001a72a55e',\n",
              " '5ad568d35b96ef001a10ae1b',\n",
              " '5a3e42de378766001a00252a',\n",
              " '5a81f0f031013a001a334fa9',\n",
              " '5727db85ff5b5019007d96fc',\n",
              " '5ad3a27c604f3c001a3fea31',\n",
              " '5a89473c3b2508001a72a59c',\n",
              " '57273a465951b619008f8702',\n",
              " '571c3e8cdd7acb1400e4c0aa',\n",
              " '5ad14d8e645df0001a2d16b9',\n",
              " '5ad251d6d7d075001a428ceb',\n",
              " '5710f4b8b654c5140001fa45',\n",
              " '5ad405f0604f3c001a3ffe31',\n",
              " '5a8945a43b2508001a72a588',\n",
              " '5ad4d5e15b96ef001a10a29b',\n",
              " '56e1c2eee3433e1400423135',\n",
              " '5ad0383677cf76001a686e3b',\n",
              " '5ad4d10e5b96ef001a10a1a4',\n",
              " '5a2ed246a83784001a7d24e4',\n",
              " '5ad0312677cf76001a686dca',\n",
              " '5725b7f389a1e219009abd5d',\n",
              " '5ad02bef77cf76001a686ca4',\n",
              " '5733638fd058e614000b59e9',\n",
              " '5a590de93e1742001a15cf57',\n",
              " '57108ee6a58dae1900cd6a1a',\n",
              " '5a89357c3b2508001a72a522',\n",
              " '5ad3ffb3604f3c001a3ffc95',\n",
              " '57267ebfdd62a815002e872d',\n",
              " '5acfec5577cf76001a6864fe',\n",
              " '5725f5b1271a42140099d373',\n",
              " '5a581597770dc0001aeeffe5',\n",
              " '5a8235a931013a001a3352da',\n",
              " '5a0c7b88f5590b0018dab41f',\n",
              " '5a82294a31013a001a335273',\n",
              " '572f6c85947a6a140053c940',\n",
              " '5ad547945b96ef001a10ac1b',\n",
              " '5a0c817df5590b0018dab45b',\n",
              " '5ad40432604f3c001a3ffdbf',\n",
              " '571077ecb654c5140001f90c',\n",
              " '5726400589a1e219009ac5f2',\n",
              " '5ad3a16d604f3c001a3fe9e3',\n",
              " '5acf7fdf77cf76001a684fd6',\n",
              " '5ad4c94f5b96ef001a10a098',\n",
              " '5a67e29c8476ee001a58a756',\n",
              " '5ad3f13e604f3c001a3ff866',\n",
              " '5ad24fb7d7d075001a428c92',\n",
              " '5a25f38dc93d92001a40035f',\n",
              " '5a638e4468151a001a922378',\n",
              " '5ad2442bd7d075001a428a3f',\n",
              " '5ad50fe55b96ef001a10ab50',\n",
              " '5710f114a58dae1900cd6b60',\n",
              " '5ad15676645df0001a2d17fa',\n",
              " '5705eb3375f01819005e7765',\n",
              " '5a2c0fb8bfd06b001a5ae9b9',\n",
              " '5ad5071f5b96ef001a10aa38',\n",
              " '5727e9523acd2414000def96',\n",
              " '5a66a55af038b7001ab0c07f',\n",
              " '5ad041c477cf76001a686f0d',\n",
              " '5a5909b13e1742001a15cf4e',\n",
              " '5ad0421177cf76001a686f15',\n",
              " '5a58f69e3e1742001a15cedf',\n",
              " '57285213ff5b5019007da181',\n",
              " '5a57f717770dc0001aeeff08',\n",
              " '572647d0708984140094c14d',\n",
              " '5a2c3a5cbfd06b001a5aea79',\n",
              " '5a6ce6b54eec6b001a80a6a7',\n",
              " '57274a1edd62a815002e9a9e',\n",
              " '5a668b1df038b7001ab0bf88',\n",
              " '56ddde6b9a695914005b9628',\n",
              " '5ad40c62604f3c001a400065',\n",
              " '5726449f1125e71900ae1929',\n",
              " '5726ddf6f1498d1400e8ee07',\n",
              " '5727526cdd62a815002e9b10',\n",
              " '5ad3af4e604f3c001a3fec76',\n",
              " '5ad3f8d2604f3c001a3ffa8e',\n",
              " '570d3468b3d812140066d544',\n",
              " '572fe60fb2c2fd140056858c',\n",
              " '57284142ff5b5019007da00c',\n",
              " '570602fa52bb89140068979e',\n",
              " '5acff73377cf76001a68668b',\n",
              " '5a0c7fb2f5590b0018dab444',\n",
              " '571cbe35dd7acb1400e4c13d',\n",
              " '5ad40ec6604f3c001a400133',\n",
              " '5ad3fd68604f3c001a3ffbe8',\n",
              " '5a3e3752378766001a0024f5',\n",
              " '5a590efb3e1742001a15cf60',\n",
              " '5ad28237d7d075001a429822',\n",
              " '572872dd2ca10214002da37e',\n",
              " '5ad3ff1b604f3c001a3ffc74',\n",
              " '5729e2b76aef0514001550cf',\n",
              " '5ad139b2645df0001a2d1295',\n",
              " '5ad262f7d7d075001a4290d6',\n",
              " '5a75054b97ca42001a521dcd',\n",
              " '5a38c911a4b263001a8c191f',\n",
              " '5ad56c6b5b96ef001a10ae6f',\n",
              " '5a5808ef770dc0001aeeff6a',\n",
              " '570d44abb3d812140066d5ff',\n",
              " '572a07c11d046914007796d9',\n",
              " '5ad2467ad7d075001a428af6',\n",
              " '5a668fc9f038b7001ab0bfe6',\n",
              " '5a1c8bb4b4fb5d00187146aa',\n",
              " '572a0ecb1d0469140077971a',\n",
              " '5a5915cd3e1742001a15cf76',\n",
              " '5725b92e38643c19005acbd6',\n",
              " '57284e9fff5b5019007da150',\n",
              " '5ad2741ed7d075001a4294d5',\n",
              " '5733d7cbd058e614000b63ae',\n",
              " '572867212ca10214002da2f2',\n",
              " '5a2eca47a83784001a7d2481',\n",
              " '57300f8504bcaa1900d770d4',\n",
              " '5acf9bcf77cf76001a68547d',\n",
              " '572943ab1d0469140077921a',\n",
              " '5a5812f9770dc0001aeeffd0',\n",
              " '5a63787868151a001a9222fa',\n",
              " '5726e313f1498d1400e8eeb4',\n",
              " '5733140a4776f419006606e2',\n",
              " '572908c13f37b31900477fc0',\n",
              " '5725f8f5ec44d21400f3d7b1',\n",
              " '57115ac550c2381900b54a78',\n",
              " '572ff430a23a5019007fcba9',\n",
              " '5ad40cdb604f3c001a400093',\n",
              " '572973f76aef051400154f0e',\n",
              " '57281ab63acd2414000df493',\n",
              " '571166352419e314009555f5',\n",
              " '5726472bdd62a815002e8042',\n",
              " '5acfefcc77cf76001a6865ae',\n",
              " '5a5808ef770dc0001aeeff69',\n",
              " '5729feaf6aef051400155188',\n",
              " '5a60556eeae51e001ab14d17',\n",
              " '57286ead2ca10214002da347']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "metadata": {
        "id": "GlJYbApT_Gjv",
        "colab_type": "code",
        "outputId": "f7cf0370-8c8c-4c99-d2cd-efbea065a6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3553
        }
      },
      "cell_type": "code",
      "source": [
        "all_predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('5727f05b4b864d190016406b', 'the poor'),\n",
              "             ('5acff3b377cf76001a6865fc', ''),\n",
              "             ('57332b66d058e614000b5758', '1915 until November 1918'),\n",
              "             ('5729f39a6aef051400155150', 'granulysin'),\n",
              "             ('5730042804bcaa1900d77011', ''),\n",
              "             ('57106644b654c5140001f8e9', 'Paris'),\n",
              "             ('5ad26dbcd7d075001a4293d5', '1724'),\n",
              "             ('57264cac708984140094c1b7', 'rationing'),\n",
              "             ('572fdc34a23a5019007fca94', 'Edinburgh Pentlands'),\n",
              "             ('571a49de4faf5e1900b8a94c', ''),\n",
              "             ('5728349dff5b5019007d9f01', ''),\n",
              "             ('5a6ce4fa4eec6b001a80a68b', 'assess patient drug allergies'),\n",
              "             ('5ad26797d7d075001a429236', 'conceptual definition of force'),\n",
              "             ('57285ed5ff5b5019007da1b9', 'Kublai Khan'),\n",
              "             ('571127a5a58dae1900cd6cc6', ''),\n",
              "             ('573362b94776f41900660974', '17th century'),\n",
              "             ('5ad507d25b96ef001a10aa4c', ''),\n",
              "             ('5ad02e5d77cf76001a686d3c', 'John B. Weller'),\n",
              "             ('5ad26dbcd7d075001a4293d6', ''),\n",
              "             ('5ad21e1ad7d075001a428471', ''),\n",
              "             ('5a63772268151a001a9222f4', 'the French'),\n",
              "             ('5a1c850fb4fb5d001871464e', ''),\n",
              "             ('5ad24057d7d075001a428930', ''),\n",
              "             ('57300f8504bcaa1900d770d1', ''),\n",
              "             ('572940246aef051400154bec',\n",
              "              'the Data Distribution Centre and the National Greenhouse Gas Inventories Programme'),\n",
              "             ('5a25d89aef59cd001a623cf8', ''),\n",
              "             ('57309bfb8ab72b1400f9c5e9', ''),\n",
              "             ('572f58d9a23a5019007fc57d', 'three'),\n",
              "             ('5a605197eae51e001ab14d0f', ''),\n",
              "             ('5ad3ba81604f3c001a3fee9e', 'saved'),\n",
              "             ('572faec7b2c2fd1400568333',\n",
              "              'a new Scottish Parliament Building, in the Holyrood area of Edinburgh'),\n",
              "             ('572fffb404bcaa1900d76ff0', ''),\n",
              "             ('573735e8c3c5551400e51e72', ''),\n",
              "             ('5725e45689a1e219009ac04a', 'Tower District'),\n",
              "             ('5a2c07d5bfd06b001a5ae983', 'Isleworth'),\n",
              "             ('57268882f1498d1400e8e307', 'ism'),\n",
              "             ('57264684708984140094c125', '30–60%'),\n",
              "             ('572fca7eb2c2fd1400568474', 'Queen Elizabeth II'),\n",
              "             ('5a8940ba3b2508001a72a55e', ''),\n",
              "             ('5ad568d35b96ef001a10ae1b', ''),\n",
              "             ('5a3e42de378766001a00252a', ''),\n",
              "             ('5a81f0f031013a001a334fa9', ''),\n",
              "             ('5727db85ff5b5019007d96fc', ''),\n",
              "             ('5ad3a27c604f3c001a3fea31', ''),\n",
              "             ('5a89473c3b2508001a72a59c', 'primary ideals'),\n",
              "             ('57273a465951b619008f8702', 'on location for a known client'),\n",
              "             ('571c3e8cdd7acb1400e4c0aa', ''),\n",
              "             ('57286ab72ca10214002da31e', 'Saul Alinsky'),\n",
              "             ('5ad14d8e645df0001a2d16b9', 'The UK'),\n",
              "             ('5ad251d6d7d075001a428ceb', 'photosynthesis'),\n",
              "             ('57114f0050c2381900b54a65', 'Steam engines'),\n",
              "             ('5710f4b8b654c5140001fa45',\n",
              "              'Count Ludwig von Nassau-Saarbrücken'),\n",
              "             ('5ad405f0604f3c001a3ffe31', ''),\n",
              "             ('5733ea04d058e614000b6596', 'Fort Le Boeuf'),\n",
              "             ('571a4ead10f8ca1400304fdd', 'Leonardo da Vinci'),\n",
              "             ('5a8945a43b2508001a72a588', ''),\n",
              "             ('5ad4d5e15b96ef001a10a29b', 'one or two'),\n",
              "             ('56e1c2eee3433e1400423135', 'quadratic time'),\n",
              "             ('5ad0383677cf76001a686e3b', 'Hamburg merchants and traders'),\n",
              "             ('5ad4d10e5b96ef001a10a1a4', ''),\n",
              "             ('5a2ed246a83784001a7d24e4', '2 million'),\n",
              "             ('5ad0312677cf76001a686dca', 'guidance and intervention'),\n",
              "             ('5725b7f389a1e219009abd5d', ''),\n",
              "             ('5ad02bef77cf76001a686ca4', ''),\n",
              "             ('5733638fd058e614000b59e9', ''),\n",
              "             ('5a590de93e1742001a15cf57', ''),\n",
              "             ('57108ee6a58dae1900cd6a1a', 'Huguon'),\n",
              "             ('5a89357c3b2508001a72a522', '1/6'),\n",
              "             ('5ad3ffb3604f3c001a3ffc95', ''),\n",
              "             ('57267ebfdd62a815002e872d', ''),\n",
              "             ('5acfec5577cf76001a6864fe',\n",
              "              'apostate\" leaders of Muslim states'),\n",
              "             ('57377862c3c5551400e51ef4', 'kinetic friction force'),\n",
              "             ('5728d4c03acd2414000dffa1', 'unfair laws'),\n",
              "             ('5725f5b1271a42140099d373', '81%'),\n",
              "             ('5733266d4776f41900660712', 'General Sejm'),\n",
              "             ('5a581597770dc0001aeeffe5', ''),\n",
              "             ('57287ee3ff5b5019007da275', '1291'),\n",
              "             ('5a8235a931013a001a3352da', ''),\n",
              "             ('5a0c7b88f5590b0018dab41f', ''),\n",
              "             ('5a82294a31013a001a335273',\n",
              "              'disadvantage low-income and under-represented minority'),\n",
              "             ('572f6c85947a6a140053c940', ''),\n",
              "             ('572ffd75b2c2fd14005686e7', 'west'),\n",
              "             ('5ad547945b96ef001a10ac1b', ''),\n",
              "             ('5a0c817df5590b0018dab45b', ''),\n",
              "             ('5ad40432604f3c001a3ffdbf', 'drowned'),\n",
              "             ('571077ecb654c5140001f90c', ''),\n",
              "             ('5726baf2dd62a815002e8e75', 'the European Community'),\n",
              "             ('5726400589a1e219009ac5f2', ''),\n",
              "             ('5ad3a16d604f3c001a3fe9e3', ''),\n",
              "             ('5acf7fdf77cf76001a684fd6', ''),\n",
              "             ('5ad4c94f5b96ef001a10a098', ''),\n",
              "             ('5a67e29c8476ee001a58a756', ''),\n",
              "             ('5ad3f13e604f3c001a3ff866', '\"kick back'),\n",
              "             ('5ad24fb7d7d075001a428c92', ''),\n",
              "             ('5a25f38dc93d92001a40035f', '828,000'),\n",
              "             ('5a638e4468151a001a922378', 'Victoria'),\n",
              "             ('5ad2442bd7d075001a428a3f', ''),\n",
              "             ('5ad50fe55b96ef001a10ab50', 'Kabaty Forest'),\n",
              "             ('5710f114a58dae1900cd6b60', ''),\n",
              "             ('5ad15676645df0001a2d17fa', 'stay'),\n",
              "             ('5705eb3375f01819005e7765', ''),\n",
              "             ('5a2c0fb8bfd06b001a5ae9b9', '1998'),\n",
              "             ('5ad5071f5b96ef001a10aa38', ''),\n",
              "             ('57293ca73f37b3190047815b', '1989'),\n",
              "             ('5727e9523acd2414000def96', ''),\n",
              "             ('5a66a55af038b7001ab0c07f', ''),\n",
              "             ('5ad041c477cf76001a686f0d', 'statements'),\n",
              "             ('5a5909b13e1742001a15cf4e', ''),\n",
              "             ('5ad0421177cf76001a686f15', '10.0%'),\n",
              "             ('5a58f69e3e1742001a15cedf', ''),\n",
              "             ('57285213ff5b5019007da181', '50 academic majors and 28'),\n",
              "             ('5a57f717770dc0001aeeff08', ''),\n",
              "             ('5726a09f708984140094cc3a', 'Article 5'),\n",
              "             ('572856beff5b5019007da192', 'Regenstein Library'),\n",
              "             ('572647d0708984140094c14d', ''),\n",
              "             ('5a2c3a5cbfd06b001a5aea79', 'VideoGuard'),\n",
              "             ('5a6ce6b54eec6b001a80a6a7', ''),\n",
              "             ('57274a1edd62a815002e9a9e', '11.1%'),\n",
              "             ('5a668b1df038b7001ab0bf88', ''),\n",
              "             ('56ddde6b9a695914005b9628', ''),\n",
              "             ('5ad40c62604f3c001a400065', 'foreigners'),\n",
              "             ('57288428ff5b5019007da291', 'thanks'),\n",
              "             ('5726449f1125e71900ae1929', ''),\n",
              "             ('5726ddf6f1498d1400e8ee07', ''),\n",
              "             ('57265746dd62a815002e8218', 'sea gooseberry'),\n",
              "             ('5727526cdd62a815002e9b10', ''),\n",
              "             ('57265642f1498d1400e8dc68', '40,000'),\n",
              "             ('5ad3af4e604f3c001a3fec76', ''),\n",
              "             ('5ad3f8d2604f3c001a3ffa8e', ''),\n",
              "             ('570d3468b3d812140066d544', ''),\n",
              "             ('572fe60fb2c2fd140056858c', ''),\n",
              "             ('57284142ff5b5019007da00c', ''),\n",
              "             ('570602fa52bb89140068979e', ''),\n",
              "             ('5acff73377cf76001a68668b', 'the temperate zone'),\n",
              "             ('5a0c7fb2f5590b0018dab444', 'electric eels'),\n",
              "             ('571cbe35dd7acb1400e4c13d', ''),\n",
              "             ('5ad40ec6604f3c001a400133', 'little effect'),\n",
              "             ('5ad3fd68604f3c001a3ffbe8',\n",
              "              'La Trinité-du-Mont became a centre of musical composition'),\n",
              "             ('5a3e3752378766001a0024f5', 'SPM'),\n",
              "             ('5a590efb3e1742001a15cf60', ''),\n",
              "             ('5ad28237d7d075001a429822', 'QED'),\n",
              "             ('57264a8cdd62a815002e808c', 'The European Commission'),\n",
              "             ('572699db5951b619008f779b', '1972'),\n",
              "             ('572872dd2ca10214002da37e', ''),\n",
              "             ('5728202c4b864d19001644ee', 'Ferenc Deák'),\n",
              "             ('5ad3ff1b604f3c001a3ffc74', ''),\n",
              "             ('5729e2b76aef0514001550cf', ''),\n",
              "             ('5ad139b2645df0001a2d1295', ''),\n",
              "             ('5ad262f7d7d075001a4290d6', 'Albert Einstein'),\n",
              "             ('572fce13a23a5019007fca15', 'Stage 2'),\n",
              "             ('5a75054b97ca42001a521dcd', ''),\n",
              "             ('57377c98c3c5551400e51efb', 'the \"spin'),\n",
              "             ('5a38c911a4b263001a8c191f', ''),\n",
              "             ('56de49a8cffd8e1900b4b7a7', 'Bethencourt'),\n",
              "             ('5ad56c6b5b96ef001a10ae6f', ''),\n",
              "             ('5a5808ef770dc0001aeeff6a', ''),\n",
              "             ('570d44abb3d812140066d5ff',\n",
              "              '1,548 public schools, 489 Catholic schools and 214 independent schools. Just under 540,800'),\n",
              "             ('572a07c11d046914007796d9', ''),\n",
              "             ('572a0bafaf94a219006aa766', 'distributive efficiency'),\n",
              "             ('5ad2467ad7d075001a428af6', ''),\n",
              "             ('5a668fc9f038b7001ab0bfe6', ''),\n",
              "             ('5a1c8bb4b4fb5d00187146aa', ''),\n",
              "             ('571c7d55dd7acb1400e4c0c4', 'monatomic'),\n",
              "             ('572a0ecb1d0469140077971a', ''),\n",
              "             ('5a5915cd3e1742001a15cf76', ''),\n",
              "             ('5725d42a89a1e219009abf59', 'Bob Gallion'),\n",
              "             ('571c83f3dd7acb1400e4c0da', 'Aufbau'),\n",
              "             ('5725b92e38643c19005acbd6', ''),\n",
              "             ('57284e9fff5b5019007da150', ''),\n",
              "             ('5ad2741ed7d075001a4294d5', ''),\n",
              "             ('5733d7cbd058e614000b63ae', 'Iroquois rule'),\n",
              "             ('572867212ca10214002da2f2', ''),\n",
              "             ('5a2eca47a83784001a7d2481', '2010'),\n",
              "             ('57300f8504bcaa1900d770d4', 'Saudi monarchy'),\n",
              "             ('5acf9bcf77cf76001a68547d',\n",
              "              'general education core includes a “dramatic, music, and visual arts” requirement'),\n",
              "             ('572943ab1d0469140077921a', ''),\n",
              "             ('5a5812f9770dc0001aeeffd0', 'Numerical models'),\n",
              "             ('5a63787868151a001a9222fa', '1 July 1851'),\n",
              "             ('5726e313f1498d1400e8eeb4', ''),\n",
              "             ('5733140a4776f419006606e2', '1313'),\n",
              "             ('572908c13f37b31900477fc0', ''),\n",
              "             ('5725f8f5ec44d21400f3d7b1', '69,284'),\n",
              "             ('57115ac550c2381900b54a78', 'turbine'),\n",
              "             ('572ff430a23a5019007fcba9', ''),\n",
              "             ('57096b66200fba1400367fa8', 'March'),\n",
              "             ('5ad40cdb604f3c001a400093', 'rich and well socially standing'),\n",
              "             ('572973f76aef051400154f0e', ''),\n",
              "             ('57281ab63acd2414000df493', ''),\n",
              "             ('571166352419e314009555f5', 'Sweden'),\n",
              "             ('5726472bdd62a815002e8042', ''),\n",
              "             ('5acfefcc77cf76001a6865ae', 'incitement to terrorism'),\n",
              "             ('5a5808ef770dc0001aeeff69', 'stratigraphy'),\n",
              "             ('573028fa04bcaa1900d7728a', '1992'),\n",
              "             ('571099b2b654c5140001f9b4', 'one-fifth'),\n",
              "             ('5729feaf6aef051400155188', ''),\n",
              "             ('5726baf2dd62a815002e8e76', 'citizenship'),\n",
              "             ('5a60556eeae51e001ab14d17', ''),\n",
              "             ('571c3a685efbb31900334db6', 'dioxygen'),\n",
              "             ('572a0c541d046914007796f5', '2001'),\n",
              "             ('57286ead2ca10214002da347', '')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "metadata": {
        "id": "aK7jNwWdyISZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_predictions=='5727f05b4b864d190016406b'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4GauqPH2HIDy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Try to run for our own examples"
      ]
    },
    {
      "metadata": {
        "id": "5JDZoCK5MNi-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_question = [\"When is the deal due\",\"Who is the prime minister\",\"Who is Jean Claude Juncker\"]\n",
        "input_context = \"The British government has won what it says are crucial improvements to the Brexit divorce deal, due to be voted on by Parliament on Tuesday (UK time). That vote is the UK's last chance to secure a Brexit that avoids either a delayed or a damaging 'no deal' exit from the EU, due on March 29. Government Minister David Lidlington said new legally binding assurances from the EU, agreed in a last-gasp negotiation on Monday, ensured the UK would not end up trapped subject to EU rules and courts indefinitely – as some Brexiteers had feared. Prime Minister Theresa May said she had secured the legal changes to the divorce deal that Parliament had wanted.”Now is the time to come together to back this improved Brexit deal and deliver on the instruction of the British people,” May said, in a late-night press conference after her unscheduled trip to Strasbourg to conclude negotiations with EU Commission President Jean-Claude Juncker. Juncker warned that the new \\”clarifications and guarantees” he had given on the divorce deal, in a legally binding instrument to accompany the official Withdrawal Agreement, were the final concessions the EU would allow on Brexit. ”In politics, sometimes you get a second chance,” he said. ”It’s what you do with the second chance that counts. Because there will be no third chance. No further interpretations or reassurances. If [the deal] fails, it fails tomorrow.” But within minutes of the new documents being revealed, some MPs and commentators dismissed them as minor footnotes to a deal essentially unchanged from the one agreed in November and overwhelmingly rejected by the House of Commons in January.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cb4iRhQ-PBel",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_question = [\"What is the trend of Sales by Household_Type\"]\n",
        "input_context = \"The top Department for Quantity between Jul 2015 and Jun 2017 was this. The top Brand for Quantity between Jul 2015 and Jun 2017 was this. The top Commodity for Quantity between Jul 2015 and Jun 2017 was this. The top Age_Band for Quantity between Jul 2015 and Jun 2017 was this. The top Loyalty for Quantity between Jul 2015 and Jun 2017 was this. The top Household_Type for Quantity between Jul 2015 and Jun 2017 was this. The top Household_Type for Quantity between Jul 2015 and Jun 2017 was this.. The bottom Department for Quantity between Jul 2015 and Jun 2017 was this. The bottom Brand for Quantity between Jul 2015 and Jun 2017 was this. The bottom Commodity for Quantity between Jul 2015 and Jun 2017 was this. The bottom Age_Band for Quantity between Jul 2015 and Jun 2017 was this. The bottom Loyalty for Quantity between Jul 2015 and Jun 2017 was this. The bottom Household_Type for Quantity between Jul 2015 and Jun 2017 was this. The bottom Household_Type for Quantity between Jul 2015 and Jun 2017 was this.. The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Department. The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Brand. The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Commodity. The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Age_Band. The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Loyalty. The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Household_Type. The breakdown of Quantity between Jul 2015 and Jun 2017 was this. Between Jul 2015 and Jun 2017, the total Quantity trend was this for Department. Between Jul 2015 and Jun 2017, the total Quantity trend was this for Brand. Between Jul 2015 and Jun 2017, the total Quantity trend was this for Commodity. Between Jul 2015 and Jun 2017, the total Quantity trend was this for Age_Band. Between Jul 2015 and Jun 2017, the total Quantity trend was this for Loyalty. Between Jul 2015 and Jun 2017, the total Quantity trend was this for Household_Type. Between Jul 2015 and Jun 2017, the total Quantity trend was this. The top Department for Sales between Jul 2015 and Jun 2017 was this. The top Brand for Sales between Jul 2015 and Jun 2017 was this. The top Commodity for Sales between Jul 2015 and Jun 2017 was this. The top Age_Band for Sales between Jul 2015 and Jun 2017 was this. The top Loyalty for Sales between Jul 2015 and Jun 2017 was this. The top Household_Type for Sales between Jul 2015 and Jun 2017 was this. The top Household_Type for Sales between Jul 2015 and Jun 2017 was this.. The bottom Department for Sales between Jul 2015 and Jun 2017 was this. The bottom Brand for Sales between Jul 2015 and Jun 2017 was this. The bottom Commodity for Sales between Jul 2015 and Jun 2017 was this. The bottom Age_Band for Sales between Jul 2015 and Jun 2017 was this. The bottom Loyalty for Sales between Jul 2015 and Jun 2017 was this. The bottom Household_Type for Sales between Jul 2015 and Jun 2017 was this. The bottom Household_Type for Sales between Jul 2015 and Jun 2017 was this.. The breakdown of Sales between Jul 2015 and Jun 2017 was this for Department. The breakdown of Sales between Jul 2015 and Jun 2017 was this for Brand. The breakdown of Sales between Jul 2015 and Jun 2017 was this for Commodity. The breakdown of Sales between Jul 2015 and Jun 2017 was this for Age_Band. The breakdown of Sales between Jul 2015 and Jun 2017 was this for Loyalty. The breakdown of Sales between Jul 2015 and Jun 2017 was this for Household_Type. The breakdown of Sales between Jul 2015 and Jun 2017 was this. Between Jul 2015 and Jun 2017, the total Sales trend was this for Department. Between Jul 2015 and Jun 2017, the total Sales trend was this for Brand. Between Jul 2015 and Jun 2017, the total Sales trend was this for Commodity. Between Jul 2015 and Jun 2017, the total Sales trend was this for Age_Band. Between Jul 2015 and Jun 2017, the total Sales trend was this for Loyalty. Between Jul 2015 and Jun 2017, the total Sales trend was this for Household_Type. Between Jul 2015 and Jun 2017, the total Sales trend was this.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkgjZ-6HHtfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Create the answers JSON first\n",
        "answers = {}\n",
        "answers['text'] = ''\n",
        "answers['answer_start'] = 0\n",
        "\n",
        "# Create the qas json second\n",
        "qas=[]\n",
        "for i in range(len(input_question)):\n",
        "  this_qas = {\"question\": str(input_question[i]),\n",
        "              \"id\": str(i),\n",
        "              \"is_impossible\": False,\n",
        "              \"answers\":[answers]}\n",
        "  qas.append(this_qas)\n",
        "\n",
        "# Now create the paragraphs json and add it all to it\n",
        "paragraphs = {}\n",
        "paragraphs['qas'] = qas\n",
        "paragraphs['context'] =  input_context\n",
        "\n",
        "# Now create the data json\n",
        "title = 'test'\n",
        "data = {}\n",
        "data['title'] = title\n",
        "data['paragraphs'] = [paragraphs]\n",
        "\n",
        "# Now finalise it\n",
        "test_json = {}\n",
        "test_json['version'] = 'v2.0'\n",
        "test_json['data'] = [data]\n",
        "\n",
        "json_test = json.dumps(test_json)\n",
        "\n",
        "# Write out the json so we can use it properly\n",
        "with open('json_test.json', 'w') as outfile:\n",
        "    json.dump(test_json, outfile)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nI-u-25cyd0_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_file = 'json_test.json'\n",
        "eval_examples = read_squad_examples(\n",
        "        input_file=eval_file, is_training=False,version_2_with_negative=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hfgC6f3sHUbl",
        "colab_type": "code",
        "outputId": "5cfac0b4-703f-47bf-a33b-904a082cdedc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "cell_type": "code",
      "source": [
        "eval_writer = run_squad.FeatureWriter(\n",
        "        filename=os.path.join(output_dir, \"eval.tf_record\"),\n",
        "        is_training=False)\n",
        "eval_features = []\n",
        "\n",
        "def append_feature(feature):\n",
        "  eval_features.append(feature)\n",
        "  eval_writer.process_feature(feature)\n",
        "\n",
        "run_squad.convert_examples_to_features(\n",
        "        examples=eval_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=max_seq_length,\n",
        "        doc_stride=doc_stride,\n",
        "        max_query_length=max_query_length,\n",
        "        is_training=False,\n",
        "        output_fn=append_feature)\n",
        "eval_writer.close()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000000\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 0\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] the top department for quantity between jul 2015 and jun 2017 was this . the top brand for quantity between jul 2015 and jun 2017 was this . the top commodity for quantity between jul 2015 and jun 2017 was this . the top age _ band for quantity between jul 2015 and jun 2017 was this . the top loyalty for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . the top household _ type for quantity between jul 2015 and jun 2017 was this . . the bottom department for quantity between jul 2015 and jun 2017 was this . the bottom brand for quantity between jul 2015 and jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:12 26:13 27:14 28:15 29:16 30:17 31:18 32:19 33:20 34:21 35:22 36:23 37:24 38:25 39:25 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:33 48:34 49:35 50:36 51:37 52:38 53:38 54:39 55:40 56:41 57:41 58:41 59:42 60:43 61:44 62:45 63:46 64:47 65:48 66:49 67:50 68:51 69:51 70:52 71:53 72:54 73:55 74:56 75:57 76:58 77:59 78:60 79:61 80:62 81:63 82:64 83:64 84:65 85:66 86:67 87:67 88:67 89:68 90:69 91:70 92:71 93:72 94:73 95:74 96:75 97:76 98:77 99:77 100:78 101:79 102:80 103:80 104:80 105:81 106:82 107:83 108:84 109:85 110:86 111:87 112:88 113:89 114:90 115:90 116:90 117:91 118:92 119:93 120:94 121:95 122:96 123:97 124:98 125:99 126:100 127:101 128:102 129:103 130:103 131:104 132:105 133:106 134:107 135:108 136:109 137:110 138:111 139:112 140:113 141:114 142:115 143:116 144:116 145:117 146:118 147:119 148:120 149:121 150:122 151:123 152:124 153:125 154:126 155:127 156:128 157:129 158:129 159:130 160:131 161:132 162:132 163:132 164:133 165:134 166:135 167:136 168:137 169:138 170:139 171:140 172:141 173:142 174:142 175:143 176:144 177:145 178:146 179:147 180:148 181:149 182:150 183:151 184:152 185:153 186:154 187:155 188:155 189:156 190:157 191:158 192:158 193:158 194:159 195:160 196:161 197:162 198:163 199:164 200:165 201:166 202:167 203:168 204:168 205:169 206:170 207:171 208:171 209:171 210:172 211:173 212:174 213:175 214:176 215:177 216:178 217:179 218:180 219:181 220:181 221:181 222:182 223:183 224:184 225:185 226:186 227:187 228:188 229:189 230:190 231:191 232:192 233:193 234:194 235:195 236:195 237:196 238:197 239:198 240:199 241:200 242:201 243:202 244:203 245:204 246:205 247:206 248:207 249:208 250:209 251:209 252:210 253:211 254:212 255:213 256:214 257:215 258:216 259:217 260:218 261:219 262:220 263:221 264:222 265:223 266:223 267:224 268:225 269:226 270:227 271:228 272:229 273:230 274:231 275:232 276:233 277:234 278:235 279:236 280:237 281:237 282:237 283:237 284:238 285:239 286:240 287:241 288:242 289:243 290:244 291:245 292:246 293:247 294:248 295:249 296:250 297:251 298:251 299:252 300:253 301:254 302:255 303:256 304:257 305:258 306:259 307:260 308:261 309:262 310:263 311:264 312:265 313:265 314:265 315:265 316:266 317:267 318:268 319:269 320:270 321:271 322:272 323:273 324:274 325:275 326:276 327:277 328:277 329:278 330:279 331:280 332:281 333:282 334:283 335:283 336:284 337:285 338:286 339:287 340:288 341:289 342:290 343:291 344:291 345:292 346:293 347:294 348:295 349:296 350:297 351:297 352:298 353:299 354:300 355:301 356:302 357:303 358:304 359:305 360:305 361:306 362:307 363:308 364:309 365:310 366:311 367:311 368:312 369:313 370:314 371:315 372:316 373:317 374:318 375:319 376:319 377:320 378:321 379:322 380:323 381:324 382:325\n",
            "INFO:tensorflow:token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 1996 2327 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000001\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 1\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] jun 2017 was this . the bottom commodity for quantity between jul 2015 and jun 2017 was this . the bottom age _ band for quantity between jul 2015 and jun 2017 was this . the bottom loyalty for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . the bottom household _ type for quantity between jul 2015 and jun 2017 was this . . the breakdown of quantity between jul 2015 and jun 2017 was this for department . the breakdown of quantity between jul 2015 and jun 2017 was this for brand . the breakdown of quantity between jul 2015 and jun 2017 was this for commodity . the breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:113 13:114 14:115 15:116 16:116 17:117 18:118 19:119 20:120 21:121 22:122 23:123 24:124 25:125 26:126 27:127 28:128 29:129 30:129 31:130 32:131 33:132 34:132 35:132 36:133 37:134 38:135 39:136 40:137 41:138 42:139 43:140 44:141 45:142 46:142 47:143 48:144 49:145 50:146 51:147 52:148 53:149 54:150 55:151 56:152 57:153 58:154 59:155 60:155 61:156 62:157 63:158 64:158 65:158 66:159 67:160 68:161 69:162 70:163 71:164 72:165 73:166 74:167 75:168 76:168 77:169 78:170 79:171 80:171 81:171 82:172 83:173 84:174 85:175 86:176 87:177 88:178 89:179 90:180 91:181 92:181 93:181 94:182 95:183 96:184 97:185 98:186 99:187 100:188 101:189 102:190 103:191 104:192 105:193 106:194 107:195 108:195 109:196 110:197 111:198 112:199 113:200 114:201 115:202 116:203 117:204 118:205 119:206 120:207 121:208 122:209 123:209 124:210 125:211 126:212 127:213 128:214 129:215 130:216 131:217 132:218 133:219 134:220 135:221 136:222 137:223 138:223 139:224 140:225 141:226 142:227 143:228 144:229 145:230 146:231 147:232 148:233 149:234 150:235 151:236 152:237 153:237 154:237 155:237 156:238 157:239 158:240 159:241 160:242 161:243 162:244 163:245 164:246 165:247 166:248 167:249 168:250 169:251 170:251 171:252 172:253 173:254 174:255 175:256 176:257 177:258 178:259 179:260 180:261 181:262 182:263 183:264 184:265 185:265 186:265 187:265 188:266 189:267 190:268 191:269 192:270 193:271 194:272 195:273 196:274 197:275 198:276 199:277 200:277 201:278 202:279 203:280 204:281 205:282 206:283 207:283 208:284 209:285 210:286 211:287 212:288 213:289 214:290 215:291 216:291 217:292 218:293 219:294 220:295 221:296 222:297 223:297 224:298 225:299 226:300 227:301 228:302 229:303 230:304 231:305 232:305 233:306 234:307 235:308 236:309 237:310 238:311 239:311 240:312 241:313 242:314 243:315 244:316 245:317 246:318 247:319 248:319 249:320 250:321 251:322 252:323 253:324 254:325 255:325 256:326 257:327 258:328 259:329 260:330 261:331 262:332 263:333 264:333 265:333 266:333 267:334 268:335 269:336 270:337 271:338 272:339 273:339 274:340 275:341 276:342 277:343 278:344 279:345 280:346 281:347 282:347 283:348 284:349 285:350 286:351 287:352 288:353 289:353 290:354 291:355 292:356 293:357 294:358 295:359 296:360 297:361 298:361 299:361 300:361 301:362 302:363 303:364 304:365 305:366 306:367 307:367 308:368 309:369 310:370 311:371 312:372 313:373 314:373 315:374 316:375 317:376 318:377 319:378 320:379 321:380 322:381 323:382 324:383 325:384 326:385 327:386 328:386 329:387 330:388 331:389 332:390 333:391 334:392 335:393 336:394 337:395 338:396 339:397 340:398 341:399 342:399 343:400 344:401 345:402 346:403 347:404 348:405 349:406 350:407 351:408 352:409 353:410 354:411 355:412 356:412 357:413 358:414 359:415 360:415 361:415 362:416 363:417 364:418 365:419 366:420 367:421 368:422 369:423 370:424 371:425 372:425 373:426 374:427 375:428 376:429 377:430 378:431 379:432 380:433 381:434 382:435\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 12022 2418 2001 2023 1012 1996 3953 19502 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000002\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 2\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] breakdown of quantity between jul 2015 and jun 2017 was this for age _ band . the breakdown of quantity between jul 2015 and jun 2017 was this for loyalty . the breakdown of quantity between jul 2015 and jun 2017 was this for household _ type . the breakdown of quantity between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total quantity trend was this for department . between jul 2015 and jun 2017 , the total quantity trend was this for brand . between jul 2015 and jun 2017 , the total quantity trend was this for commodity . between jul 2015 and jun 2017 , the total quantity trend was this for age _ band . between jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:225 13:226 14:227 15:228 16:229 17:230 18:231 19:232 20:233 21:234 22:235 23:236 24:237 25:237 26:237 27:237 28:238 29:239 30:240 31:241 32:242 33:243 34:244 35:245 36:246 37:247 38:248 39:249 40:250 41:251 42:251 43:252 44:253 45:254 46:255 47:256 48:257 49:258 50:259 51:260 52:261 53:262 54:263 55:264 56:265 57:265 58:265 59:265 60:266 61:267 62:268 63:269 64:270 65:271 66:272 67:273 68:274 69:275 70:276 71:277 72:277 73:278 74:279 75:280 76:281 77:282 78:283 79:283 80:284 81:285 82:286 83:287 84:288 85:289 86:290 87:291 88:291 89:292 90:293 91:294 92:295 93:296 94:297 95:297 96:298 97:299 98:300 99:301 100:302 101:303 102:304 103:305 104:305 105:306 106:307 107:308 108:309 109:310 110:311 111:311 112:312 113:313 114:314 115:315 116:316 117:317 118:318 119:319 120:319 121:320 122:321 123:322 124:323 125:324 126:325 127:325 128:326 129:327 130:328 131:329 132:330 133:331 134:332 135:333 136:333 137:333 138:333 139:334 140:335 141:336 142:337 143:338 144:339 145:339 146:340 147:341 148:342 149:343 150:344 151:345 152:346 153:347 154:347 155:348 156:349 157:350 158:351 159:352 160:353 161:353 162:354 163:355 164:356 165:357 166:358 167:359 168:360 169:361 170:361 171:361 172:361 173:362 174:363 175:364 176:365 177:366 178:367 179:367 180:368 181:369 182:370 183:371 184:372 185:373 186:373 187:374 188:375 189:376 190:377 191:378 192:379 193:380 194:381 195:382 196:383 197:384 198:385 199:386 200:386 201:387 202:388 203:389 204:390 205:391 206:392 207:393 208:394 209:395 210:396 211:397 212:398 213:399 214:399 215:400 216:401 217:402 218:403 219:404 220:405 221:406 222:407 223:408 224:409 225:410 226:411 227:412 228:412 229:413 230:414 231:415 232:415 233:415 234:416 235:417 236:418 237:419 238:420 239:421 240:422 241:423 242:424 243:425 244:425 245:426 246:427 247:428 248:429 249:430 250:431 251:432 252:433 253:434 254:435 255:436 256:437 257:438 258:438 259:439 260:440 261:441 262:441 263:441 264:442 265:443 266:444 267:445 268:446 269:447 270:448 271:449 272:450 273:451 274:451 275:452 276:453 277:454 278:454 279:454 280:455 281:456 282:457 283:458 284:459 285:460 286:461 287:462 288:463 289:464 290:464 291:464 292:465 293:466 294:467 295:468 296:469 297:470 298:471 299:472 300:473 301:474 302:475 303:476 304:477 305:477 306:478 307:479 308:480 309:481 310:482 311:483 312:484 313:485 314:486 315:487 316:488 317:489 318:490 319:490 320:491 321:492 322:493 323:494 324:495 325:496 326:497 327:498 328:499 329:500 330:501 331:502 332:503 333:503 334:504 335:505 336:506 337:506 338:506 339:507 340:508 341:509 342:510 343:511 344:512 345:513 346:514 347:515 348:516 349:516 350:517 351:518 352:519 353:520 354:521 355:522 356:523 357:524 358:525 359:526 360:527 361:528 362:529 363:529 364:530 365:531 366:532 367:532 368:532 369:533 370:534 371:535 372:536 373:537 374:538 375:539 376:540 377:541 378:542 379:542 380:543 381:544 382:545\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 11712 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000003\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 3\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] jul 2015 and jun 2017 , the total quantity trend was this for loyalty . between jul 2015 and jun 2017 , the total quantity trend was this for household _ type . between jul 2015 and jun 2017 , the total quantity trend was this . the top department for sales between jul 2015 and jun 2017 was this . the top brand for sales between jul 2015 and jun 2017 was this . the top commodity for sales between jul 2015 and jun 2017 was this . the top age _ band for sales between jul 2015 and jun 2017 was this . the top loyalty for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:335 13:336 14:337 15:338 16:339 17:339 18:340 19:341 20:342 21:343 22:344 23:345 24:346 25:347 26:347 27:348 28:349 29:350 30:351 31:352 32:353 33:353 34:354 35:355 36:356 37:357 38:358 39:359 40:360 41:361 42:361 43:361 44:361 45:362 46:363 47:364 48:365 49:366 50:367 51:367 52:368 53:369 54:370 55:371 56:372 57:373 58:373 59:374 60:375 61:376 62:377 63:378 64:379 65:380 66:381 67:382 68:383 69:384 70:385 71:386 72:386 73:387 74:388 75:389 76:390 77:391 78:392 79:393 80:394 81:395 82:396 83:397 84:398 85:399 86:399 87:400 88:401 89:402 90:403 91:404 92:405 93:406 94:407 95:408 96:409 97:410 98:411 99:412 100:412 101:413 102:414 103:415 104:415 105:415 106:416 107:417 108:418 109:419 110:420 111:421 112:422 113:423 114:424 115:425 116:425 117:426 118:427 119:428 120:429 121:430 122:431 123:432 124:433 125:434 126:435 127:436 128:437 129:438 130:438 131:439 132:440 133:441 134:441 135:441 136:442 137:443 138:444 139:445 140:446 141:447 142:448 143:449 144:450 145:451 146:451 147:452 148:453 149:454 150:454 151:454 152:455 153:456 154:457 155:458 156:459 157:460 158:461 159:462 160:463 161:464 162:464 163:464 164:465 165:466 166:467 167:468 168:469 169:470 170:471 171:472 172:473 173:474 174:475 175:476 176:477 177:477 178:478 179:479 180:480 181:481 182:482 183:483 184:484 185:485 186:486 187:487 188:488 189:489 190:490 191:490 192:491 193:492 194:493 195:494 196:495 197:496 198:497 199:498 200:499 201:500 202:501 203:502 204:503 205:503 206:504 207:505 208:506 209:506 210:506 211:507 212:508 213:509 214:510 215:511 216:512 217:513 218:514 219:515 220:516 221:516 222:517 223:518 224:519 225:520 226:521 227:522 228:523 229:524 230:525 231:526 232:527 233:528 234:529 235:529 236:530 237:531 238:532 239:532 240:532 241:533 242:534 243:535 244:536 245:537 246:538 247:539 248:540 249:541 250:542 251:542 252:543 253:544 254:545 255:545 256:545 257:546 258:547 259:548 260:549 261:550 262:551 263:552 264:553 265:554 266:555 267:555 268:555 269:556 270:557 271:558 272:559 273:560 274:561 275:562 276:563 277:564 278:565 279:566 280:567 281:568 282:569 283:569 284:570 285:571 286:572 287:573 288:574 289:575 290:576 291:577 292:578 293:579 294:580 295:581 296:582 297:583 298:583 299:584 300:585 301:586 302:587 303:588 304:589 305:590 306:591 307:592 308:593 309:594 310:595 311:596 312:597 313:597 314:598 315:599 316:600 317:601 318:602 319:603 320:604 321:605 322:606 323:607 324:608 325:609 326:610 327:611 328:611 329:611 330:611 331:612 332:613 333:614 334:615 335:616 336:617 337:618 338:619 339:620 340:621 341:622 342:623 343:624 344:625 345:625 346:626 347:627 348:628 349:629 350:630 351:631 352:632 353:633 354:634 355:635 356:636 357:637 358:638 359:639 360:639 361:639 362:639 363:640 364:641 365:642 366:643 367:644 368:645 369:646 370:647 371:648 372:649 373:650 374:651 375:651 376:652 377:653 378:654 379:655 380:656 381:657 382:657\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:False 263:False 264:False 265:False 266:False 267:False 268:False 269:False 270:False 271:False 272:False 273:False 274:False 275:False 276:False 277:False 278:False 279:False 280:False 281:False 282:False 283:False 284:False 285:False 286:False 287:False 288:False 289:False 290:False 291:False 292:False 293:False 294:False 295:False 296:False 297:False 298:False 299:False 300:False 301:False 302:False 303:False 304:False 305:False 306:False 307:False 308:False 309:False 310:False 311:False 312:False 313:False 314:False 315:False 316:False 317:False 318:False 319:False 320:False 321:False 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 11712 9874 2001 2023 1012 1996 2327 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 1000000004\n",
            "INFO:tensorflow:example_index: 0\n",
            "INFO:tensorflow:doc_span_index: 4\n",
            "INFO:tensorflow:tokens: [CLS] what is the trend of sales by household _ type [SEP] 2015 and jun 2017 was this . the top household _ type for sales between jul 2015 and jun 2017 was this . . the bottom department for sales between jul 2015 and jun 2017 was this . the bottom brand for sales between jul 2015 and jun 2017 was this . the bottom commodity for sales between jul 2015 and jun 2017 was this . the bottom age _ band for sales between jul 2015 and jun 2017 was this . the bottom loyalty for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . the bottom household _ type for sales between jul 2015 and jun 2017 was this . . the breakdown of sales between jul 2015 and jun 2017 was this for department . the breakdown of sales between jul 2015 and jun 2017 was this for brand . the breakdown of sales between jul 2015 and jun 2017 was this for commodity . the breakdown of sales between jul 2015 and jun 2017 was this for age _ band . the breakdown of sales between jul 2015 and jun 2017 was this for loyalty . the breakdown of sales between jul 2015 and jun 2017 was this for household _ type . the breakdown of sales between jul 2015 and jun 2017 was this . between jul 2015 and jun 2017 , the total sales trend was this for department . between jul 2015 and jun 2017 , the total sales trend was this for brand . between jul 2015 and jun 2017 , the total sales trend was this for commodity . between jul 2015 and jun 2017 , the total sales trend was this for age _ band . between jul 2015 and jun 2017 , the total sales trend was this for loyalty . between jul 2015 and jun 2017 , the total sales trend was this for household _ type . between jul 2015 and jun 2017 , the total sales trend was this . [SEP]\n",
            "INFO:tensorflow:token_to_orig_map: 12:446 13:447 14:448 15:449 16:450 17:451 18:451 19:452 20:453 21:454 22:454 23:454 24:455 25:456 26:457 27:458 28:459 29:460 30:461 31:462 32:463 33:464 34:464 35:464 36:465 37:466 38:467 39:468 40:469 41:470 42:471 43:472 44:473 45:474 46:475 47:476 48:477 49:477 50:478 51:479 52:480 53:481 54:482 55:483 56:484 57:485 58:486 59:487 60:488 61:489 62:490 63:490 64:491 65:492 66:493 67:494 68:495 69:496 70:497 71:498 72:499 73:500 74:501 75:502 76:503 77:503 78:504 79:505 80:506 81:506 82:506 83:507 84:508 85:509 86:510 87:511 88:512 89:513 90:514 91:515 92:516 93:516 94:517 95:518 96:519 97:520 98:521 99:522 100:523 101:524 102:525 103:526 104:527 105:528 106:529 107:529 108:530 109:531 110:532 111:532 112:532 113:533 114:534 115:535 116:536 117:537 118:538 119:539 120:540 121:541 122:542 123:542 124:543 125:544 126:545 127:545 128:545 129:546 130:547 131:548 132:549 133:550 134:551 135:552 136:553 137:554 138:555 139:555 140:555 141:556 142:557 143:558 144:559 145:560 146:561 147:562 148:563 149:564 150:565 151:566 152:567 153:568 154:569 155:569 156:570 157:571 158:572 159:573 160:574 161:575 162:576 163:577 164:578 165:579 166:580 167:581 168:582 169:583 170:583 171:584 172:585 173:586 174:587 175:588 176:589 177:590 178:591 179:592 180:593 181:594 182:595 183:596 184:597 185:597 186:598 187:599 188:600 189:601 190:602 191:603 192:604 193:605 194:606 195:607 196:608 197:609 198:610 199:611 200:611 201:611 202:611 203:612 204:613 205:614 206:615 207:616 208:617 209:618 210:619 211:620 212:621 213:622 214:623 215:624 216:625 217:625 218:626 219:627 220:628 221:629 222:630 223:631 224:632 225:633 226:634 227:635 228:636 229:637 230:638 231:639 232:639 233:639 234:639 235:640 236:641 237:642 238:643 239:644 240:645 241:646 242:647 243:648 244:649 245:650 246:651 247:651 248:652 249:653 250:654 251:655 252:656 253:657 254:657 255:658 256:659 257:660 258:661 259:662 260:663 261:664 262:665 263:665 264:666 265:667 266:668 267:669 268:670 269:671 270:671 271:672 272:673 273:674 274:675 275:676 276:677 277:678 278:679 279:679 280:680 281:681 282:682 283:683 284:684 285:685 286:685 287:686 288:687 289:688 290:689 291:690 292:691 293:692 294:693 295:693 296:694 297:695 298:696 299:697 300:698 301:699 302:699 303:700 304:701 305:702 306:703 307:704 308:705 309:706 310:707 311:707 312:707 313:707 314:708 315:709 316:710 317:711 318:712 319:713 320:713 321:714 322:715 323:716 324:717 325:718 326:719 327:720 328:721 329:721 330:722 331:723 332:724 333:725 334:726 335:727 336:727 337:728 338:729 339:730 340:731 341:732 342:733 343:734 344:735 345:735 346:735 347:735 348:736 349:737 350:738 351:739 352:740 353:741 354:741 355:742 356:743 357:744 358:745 359:746 360:747 361:747\n",
            "INFO:tensorflow:token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True 361:True\n",
            "INFO:tensorflow:input_ids: 101 2054 2003 1996 9874 1997 4341 2011 4398 1035 2828 102 2325 1998 12022 2418 2001 2023 1012 1996 2327 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 3953 2533 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4435 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 19502 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 2287 1035 2316 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 9721 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1996 3953 4398 1035 2828 2005 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2533 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4435 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 19502 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 2287 1035 2316 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 9721 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 2005 4398 1035 2828 1012 1996 12554 1997 4341 2090 21650 2325 1998 12022 2418 2001 2023 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2533 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4435 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 19502 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 2287 1035 2316 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 9721 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 2005 4398 1035 2828 1012 2090 21650 2325 1998 12022 2418 1010 1996 2561 4341 9874 2001 2023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OYcaFUVCdNWT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_input_fn = run_squad.input_fn_builder(\n",
        "        input_file=eval_writer.filename,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Jsjc2wddUrV",
        "colab_type": "code",
        "outputId": "8dca2858-e738-4869-91ed-12b9fd93da34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3655
        }
      },
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "for result in estimator.predict(\n",
        "  predict_input_fn, yield_single_examples=True):\n",
        "  if len(all_results) % 1000 == 0:\n",
        "    tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
        "  unique_id = int(result[\"unique_ids\"])\n",
        "  start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "  end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "  all_results.append(\n",
        "      run_squad.RawResult(\n",
        "          unique_id=unique_id,\n",
        "          start_logits=start_logits,\n",
        "          end_logits=end_logits))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 384)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 384)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 384)\n",
            "INFO:tensorflow:  name = unique_ids, shape = (?,)\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = cls/squad/output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = cls/squad/output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from output_model/model.ckpt-21\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Processing example: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YBUrRqzKdXsq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_best_size=20\n",
        "do_lower_case=True\n",
        "verbose_logging=False\n",
        "version_2_with_negative=True\n",
        "null_score_diff_threshold=0.0\n",
        "all_predictions,all_nbest_json = write_predictions(eval_examples, eval_features, all_results,\n",
        "                  n_best_size, max_answer_length,\n",
        "                  do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v_atW0FFEHoh",
        "colab_type": "code",
        "outputId": "2bd42090-6aa7-4604-da69-3f4028312912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "n=1\n",
        "print(input_question)\n",
        "print([x[n] for x in list(all_predictions.items())])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What is the trend of Sales by Household_Type']\n",
            "['']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lGHEzQ3XmDsm",
        "colab_type": "code",
        "outputId": "e7d6d1de-dd41-4b7f-b782-42321fc4c443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1737
        }
      },
      "cell_type": "code",
      "source": [
        "all_nbest_json"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0',\n",
              "              [OrderedDict([('text', ''),\n",
              "                            ('probability', 0.9898992165660715),\n",
              "                            ('start_logit', 5.0583038330078125),\n",
              "                            ('end_logit', 5.007004737854004)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Sales between Jul 2015 and Jun 2017 was this for Department.'),\n",
              "                            ('probability', 0.0005855198397399823),\n",
              "                            ('start_logit', 1.5260491371154785),\n",
              "                            ('end_logit', 1.1064010858535767)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Sales between Jul 2015 and Jun 2017 was this for Department. The breakdown of Sales between Jul 2015 and Jun 2017 was this for Brand.'),\n",
              "                            ('probability', 0.0005832171602283242),\n",
              "                            ('start_logit', 1.5260491371154785),\n",
              "                            ('end_logit', 1.1024606227874756)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Sales between Jul 2015 and Jun 2017 was this for Brand.'),\n",
              "                            ('probability', 0.0005591090948567288),\n",
              "                            ('start_logit', 1.483834147453308),\n",
              "                            ('end_logit', 1.1024606227874756)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Age_Band.'),\n",
              "                            ('probability', 0.0005582904872994763),\n",
              "                            ('start_logit', 1.47954523563385),\n",
              "                            ('end_logit', 1.105284333229065)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Commodity.'),\n",
              "                            ('probability', 0.0005347128228128479),\n",
              "                            ('start_logit', 1.4233275651931763),\n",
              "                            ('end_logit', 1.1183524131774902)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Brand. The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Commodity.'),\n",
              "                            ('probability', 0.0005307516783293092),\n",
              "                            ('start_logit', 1.4158920049667358),\n",
              "                            ('end_logit', 1.1183524131774902)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Quantity between Jul 2015 and Jun 2017 was this for Brand.'),\n",
              "                            ('probability', 0.0005307072642753163),\n",
              "                            ('start_logit', 1.4158920049667358),\n",
              "                            ('end_logit', 1.1182687282562256)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The top Department for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.000521560901346347),\n",
              "                            ('start_logit', 1.3962808847427368),\n",
              "                            ('end_logit', 1.120495319366455)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The top Age_Band for Quantity between Jul 2015 and Jun 2017 was this. The top Loyalty for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005203609403442162),\n",
              "                            ('start_logit', 1.3902500867843628),\n",
              "                            ('end_logit', 1.124222755432129)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The top Department for Quantity between Jul 2015 and Jun 2017 was this. The top Brand for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005202757775728417),\n",
              "                            ('start_logit', 1.3962808847427368),\n",
              "                            ('end_logit', 1.1180282831192017)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The top Household_Type for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005196320187398611),\n",
              "                            ('start_logit', 1.3889610767364502),\n",
              "                            ('end_logit', 1.1241099834442139)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The top Loyalty for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005191690625991673),\n",
              "                            ('start_logit', 1.387956976890564),\n",
              "                            ('end_logit', 1.124222755432129)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The top Loyalty for Quantity between Jul 2015 and Jun 2017 was this. The top Household_Type for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005191105181730617),\n",
              "                            ('start_logit', 1.387956976890564),\n",
              "                            ('end_logit', 1.1241099834442139)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The top Age_Band for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.000518566482377102),\n",
              "                            ('start_logit', 1.3902500867843628),\n",
              "                            ('end_logit', 1.1207683086395264)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Sales between Jul 2015 and Jun 2017 was this for Household_Type. The breakdown of Sales between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005183856964176052),\n",
              "                            ('start_logit', 1.3970683813095093),\n",
              "                            ('end_logit', 1.1136013269424438)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Sales between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005173302394677672),\n",
              "                            ('start_logit', 1.3950302600860596),\n",
              "                            ('end_logit', 1.1136013269424438)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Sales between Jul 2015 and Jun 2017 was this for Household_Type.'),\n",
              "                            ('probability', 0.0005160955901985142),\n",
              "                            ('start_logit', 1.3970683813095093),\n",
              "                            ('end_logit', 1.1091737747192383)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The breakdown of Sales between Jul 2015 and Jun 2017 was this. Between Jul 2015 and Jun 2017, the total Sales trend was this for Department.'),\n",
              "                            ('probability', 0.0005152080794990431),\n",
              "                            ('start_logit', 1.3950302600860596),\n",
              "                            ('end_logit', 1.1094907522201538)]),\n",
              "               OrderedDict([('text',\n",
              "                             'The bottom Brand for Quantity between Jul 2015 and Jun 2017 was this.'),\n",
              "                            ('probability', 0.0005127797796506018),\n",
              "                            ('start_logit', 1.38010573387146),\n",
              "                            ('end_logit', 1.1196908950805664)])])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}