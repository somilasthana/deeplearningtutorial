{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-with-RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somilasthana/deeplearningtutorial/blob/master/Text_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5gF9iZ211k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKIi5xFH2M-m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e380da78-ba83-44b6-d220-1713b71b9a75"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PqLJ0782fdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58311f18-3760-4c8b-c8b8-1bd358ed2a89"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print(\"Length {}\".format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LRpSJ-q2yoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "89522249-383e-4527-d040-09986c5d6fb0"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pobHp71_29T5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "848a8ad5-1674-40d4-cca4-1f7eac8edd2c"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(\"Number of Unique Character {}\".format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Character 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUT-VmSJ3KjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KyNYptA4Dhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "304c2ef7-2c45-42c4-ff3f-a7bc85bf1384"
      },
      "source": [
        "text[:13], text_as_int[:13]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('First Citizen', array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVUDNzBQ4bCg",
        "colab_type": "text"
      },
      "source": [
        "Prediction Task \n",
        "\n",
        "Given a character, or a sequence of characters, what is the most probable next character? RNN predicts the next char using the previously seen elements by maintaining an internal state.\n",
        "\n",
        "RNN is feeded input sequence for fixed length seq_length as a training data ( = X ), the target are the corresponding input sequence but one shifted (=y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH_OV_hb4dqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1Trond4Ha2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples_per_epoch = len(text)//seq_length # 11153"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "segDu2L86r3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "c2dad07b-ea23-4c99-ea7a-418bc6cd0932"
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VidpMm2O7gKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eyNJtFl73Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text  = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV3pP2ml8aIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7c146fe2-17c8-4a52-f8d2-0dbc8c32a527"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpd3v2G39XU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "c31cf352-6cfe-41a9-a964-b75259fc7947"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgQTfoKG-Ozi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a1f338cd-f05c-4b75-cd6c-0a46c8007c7d"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drkU08Cd9g6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=64\n",
        "\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "steps_per_epoch = int(steps_per_epoch)\n",
        "\n",
        "BUFFER_SIZE=10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWutZZal9p2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ddb5cbd-c7d0-43e6-f1bb-68d42f2f3100"
      },
      "source": [
        "dataset= dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset # Batch of 64 with 100 training features/chars, Batch of 64 with 100 target features/chars "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVKgmKBN-YX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjRu17yf-7Tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd387794-67d9-4d17-bd53-29b68d39386b"
      },
      "source": [
        "tf.test.is_gpu_available() # GPU is available"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O_zYiv-_Bpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = tf.keras.layers.CuDNNGRU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pVzub5A_M3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential(\n",
        "      [\n",
        "          tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "          rnn(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
        "          tf.keras.layers.Dense(vocab_size)\n",
        "      ]\n",
        "  )\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1cUz4lrAKP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOka5xw0AkWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "74bac794-c670-4131-c016-878d2d36d25a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSrj-whkA8Ab",
        "colab_type": "text"
      },
      "source": [
        "Training the Model\n",
        "\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state , and the input this time step "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efEbNvusAwFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUfjOxj5Gunl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.train.AdamOptimizer(), loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPv1p0_UHNBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZVbWUCZH9iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIkTPneyIDA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "2566c421-cf14-4e53-a466-aa5d3a845dce"
      },
      "source": [
        "history = model.fit(\n",
        "    dataset.repeat(),\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "173/174 [============================>.] - ETA: 0s - loss: 2.7134WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
            "174/174 [==============================] - 16s 95ms/step - loss: 2.7101\n",
            "Epoch 2/3\n",
            "174/174 [==============================] - 12s 68ms/step - loss: 1.9430\n",
            "Epoch 3/3\n",
            "174/174 [==============================] - 12s 68ms/step - loss: 1.6792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4m8xftRPREm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "cc89c746-6cdb-41c8-d541-5853a3b5881d"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hF6vVcyIyBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  \n",
        "  num_generate= 1000\n",
        "  \n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  \n",
        "  text_generated=[]\n",
        "  \n",
        "  temperature=1.0\n",
        "  \n",
        "  model.reset_states()\n",
        "  \n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    \n",
        "    predictions = predictions / temperature\n",
        "    \n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1, 0].numpy()\n",
        "    \n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "    \n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4XBIS1DOsJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "outputId": "79257f5e-26fd-46ce-c696-bc12fae7d6b2"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: that that?\n",
            "\n",
            "First CIOCE:\n",
            "My lize, any madraindaking thou hurds,\n",
            "That should eved here in choss'd, you she banison:\n",
            "Yearshilved will be weO:\n",
            "Thou thought to see, thou night wase the use.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Now forweds, my lord, if your courtry,\n",
            "That though the pracgoneds,\n",
            "That saught for never be in him.\n",
            "The not they shouth, daster, what she\n",
            "May comes: he crie, not in manicarair\n",
            "Ir all this hanr.\n",
            "\n",
            "PRINCE:\n",
            "Now, breaks it. peep seemed.\n",
            "\n",
            "CLARDONEN:\n",
            "I thou no,\n",
            "When you show, but imp by a car.\n",
            "\n",
            "First Mandager:\n",
            "And teengan, mysch.\n",
            "\n",
            "HARG III:\n",
            "But never go?\n",
            "the peech of years you wilt come against then truem her,\n",
            "Are all sutchis goil.\n",
            "\n",
            "VORMINS:\n",
            "What\n",
            "Thou have my brootle cousind.\n",
            "\n",
            "ROMEO:\n",
            "Why, here alone, sir,\n",
            "Years not in weep to things\n",
            "If stake my spirce we to,\n",
            "You hate me it would the beancily? what's destines in me,\n",
            "Wencelice! 'tis milt!\n",
            "\n",
            "MENCY GO:\n",
            "\n",
            "ShepI he should\n",
            "Cowite York! look; my wrife;\n",
            "There art thou heave us, truy are carn of hatin.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Cit;\n",
            "You nore, fovery from nless!\n",
            "\n",
            "Nurse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7dlCR5OYvUo",
        "colab_type": "text"
      },
      "source": [
        "Advanced: Customized Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ73ovNXO1jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Al6fhMPZT-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=tf.train.AdamOptimizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_DTUyySZg-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx7ha922ZjXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epochs in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  hidden = model.reset_states()\n",
        "  \n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions=model(inp)\n",
        "      loss=tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "      \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    \n",
        "    if batch_n% 100 == 0:\n",
        "      template= 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epochs+1, batch_n, loss))\n",
        "      \n",
        "      \n",
        "  if (epochs+1)%5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHlhMhxTbriF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "d38832db-0081-4cb3-9dc4-996ba4f473f3"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_3 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_opDt48hkWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "outputId": "31086f13-8b43-4020-f23b-ec3668283348"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: Whis\n",
            "near'd.\n",
            "You think you grace the bastle, of the air.\n",
            "Barry, made; here is my oathlines? nay, thou trustle is man instrume to sove\n",
            "Nou'r bound with noldines Kate; Lucentio is conjuity:\n",
            "And thou conser, had straked tale, the treach\n",
            "When it will even Bursh, and I am louse,\n",
            "And all the woeful to this breath of her turn.\n",
            "\n",
            "PAULINA:\n",
            "To me,\n",
            "Anielting hareful and impraining the house of Lusifer.\n",
            "Hereform and all obysire; i' a patrecion,\n",
            "It ckress of thy friends, hast thou,\n",
            "Which your consul to sister his own parties\n",
            "Did no ever from me their heless.\n",
            "Give with all arms and men, to ask glous!\n",
            "Nor I should desarp.\n",
            "\n",
            "Cirs, I\n",
            "was bound to thy message;\n",
            "Protucine look him say; but nou will, not together,\n",
            "Is beet from Petit onch with death.\n",
            "But, were no dief stee ovater:\n",
            "Trust of good scorn,\n",
            "Thy father way the rest to look, much little something breathe;\n",
            "For thy conceit or death,\n",
            "Besides the heading sleep not this\n",
            "That ever mean that knowledge him; where the time\n",
            "In name until them.\n",
            "\n",
            "MENENIUS:\n",
            "Rie'd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUlqr0xBh_x3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}